diff --color -uprN llvm-14.0.6.src/CMakeLists.txt llvm-14.0.6.src.patched/CMakeLists.txt
--- llvm-14.0.6.src/CMakeLists.txt	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/CMakeLists.txt	2024-07-04 15:24:55.527947755 +0800
@@ -1241,22 +1241,6 @@ if (MSVC AND CMAKE_HOST_SYSTEM_NAME STRE
   include(InstallRequiredSystemLibraries)
 endif()
 
-if (LLVM_INCLUDE_BENCHMARKS)
-  # Override benchmark defaults so that when the library itself is updated these
-  # modifications are not lost.
-  set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL "Disable benchmark testing" FORCE)
-  set(BENCHMARK_ENABLE_EXCEPTIONS OFF CACHE BOOL "Disable benchmark exceptions" FORCE)
-  set(BENCHMARK_ENABLE_INSTALL OFF CACHE BOOL "Don't install benchmark" FORCE)
-  set(BENCHMARK_DOWNLOAD_DEPENDENCIES OFF CACHE BOOL "Don't download dependencies" FORCE)
-  set(BENCHMARK_ENABLE_GTEST_TESTS OFF CACHE BOOL "Disable Google Test in benchmark" FORCE)
-  set(BENCHMARK_ENABLE_WERROR ${LLVM_ENABLE_WERROR} CACHE BOOL 
-    "Handle -Werror for Google Benchmark based on LLVM_ENABLE_WERROR" FORCE)
-  # Since LLVM requires C++11 it is safe to assume that std::regex is available.
-  set(HAVE_STD_REGEX ON CACHE BOOL "OK" FORCE)
-  add_subdirectory(${LLVM_THIRD_PARTY_DIR}/benchmark 
-    ${CMAKE_CURRENT_BINARY_DIR}/third-party/benchmark)
-  add_subdirectory(benchmarks)
-endif()
 
 if (LLVM_INCLUDE_UTILS AND LLVM_INCLUDE_TOOLS)
   add_subdirectory(utils/llvm-locstats)
diff --color -uprN llvm-14.0.6.src/include/llvm/Analysis/LoopInfo.h llvm-14.0.6.src.patched/include/llvm/Analysis/LoopInfo.h
--- llvm-14.0.6.src/include/llvm/Analysis/LoopInfo.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/Analysis/LoopInfo.h	2024-07-04 15:24:55.527947755 +0800
@@ -925,13 +925,13 @@ public:
   void releaseMemory() {
     BBMap.clear();
 
-    for (auto *L : TopLevelLoops)
-      L->~LoopT();
+    // for (auto *L : TopLevelLoops)
+    //   L->~LoopT();
     TopLevelLoops.clear();
-    LoopAllocator.Reset();
+    // LoopAllocator.Reset();
   }
 
-  template <typename... ArgsTy> LoopT *AllocateLoop(ArgsTy &&... Args) {
+  template <typename... ArgsTy> LoopT *AllocateLoop(ArgsTy &&...Args) {
     LoopT *Storage = LoopAllocator.Allocate<LoopT>();
     return new (Storage) LoopT(std::forward<ArgsTy>(Args)...);
   }
@@ -1213,7 +1213,6 @@ public:
   // IR is assumed to be in LCSSA form before the planned insertion.
   bool wouldBeOutOfLoopUseRequiringLCSSA(const Value *V,
                                          const BasicBlock *ExitBB) const;
-
 };
 
 /// Enable verification of loop info.
@@ -1307,13 +1306,13 @@ MDNode *findOptionMDForLoop(const Loop *
 
 Optional<bool> getOptionalBoolLoopAttribute(const Loop *TheLoop,
                                             StringRef Name);
-  
+
 /// Returns true if Name is applied to TheLoop and enabled.
 bool getBooleanLoopAttribute(const Loop *TheLoop, StringRef Name);
 
 /// Find named metadata for a loop with an integer value.
-llvm::Optional<int>
-getOptionalIntLoopAttribute(const Loop *TheLoop, StringRef Name);
+llvm::Optional<int> getOptionalIntLoopAttribute(const Loop *TheLoop,
+                                                StringRef Name);
 
 /// Find named metadata for a loop with an integer value. Return \p Default if
 /// not set.
@@ -1369,6 +1368,6 @@ makePostTransformationMetadata(llvm::LLV
                                llvm::ArrayRef<llvm::StringRef> RemovePrefixes,
                                llvm::ArrayRef<llvm::MDNode *> AddAttrs);
 
-} // End llvm namespace
+} // namespace llvm
 
 #endif
diff --color -uprN llvm-14.0.6.src/include/llvm/Analysis/ScalarEvolution.h llvm-14.0.6.src.patched/include/llvm/Analysis/ScalarEvolution.h
--- llvm-14.0.6.src/include/llvm/Analysis/ScalarEvolution.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/Analysis/ScalarEvolution.h	2024-07-04 15:24:55.527947755 +0800
@@ -140,7 +140,14 @@ public:
   explicit SCEV(const FoldingSetNodeIDRef ID, SCEVTypes SCEVTy,
                 unsigned short ExpressionSize)
       : FastID(ID), SCEVType(SCEVTy), ExpressionSize(ExpressionSize) {}
-  SCEV(const SCEV &) = delete;
+  // SCEV(const SCEV *N)
+  //     : FastID(N->FastID), SCEVType(N->getSCEVType()),
+  //       ExpressionSize(N->ExpressionSize), SubclassData(N->SubclassData) {
+  //   SetNextInBucket(N->getNextInBucket());
+  // };
+  // SCEV(SCEV &&N) noexcept {
+
+  // }
   SCEV &operator=(const SCEV &) = delete;
 
   SCEVTypes getSCEVType() const { return SCEVType; }
@@ -169,9 +176,7 @@ public:
   // SCEV and all its operands recursively. We may use it to avoid performing
   // heavy transformations on SCEVs of excessive size for sake of saving the
   // compilation time.
-  unsigned short getExpressionSize() const {
-    return ExpressionSize;
-  }
+  unsigned short getExpressionSize() const { return ExpressionSize; }
 
   /// Print out the internal representation of this scalar to the specified
   /// stream.  This should really only be used for debugging purposes.
@@ -402,9 +407,7 @@ public:
   bool isAlwaysTrue() const override;
 
   /// Methods for support type inquiry through isa, cast, and dyn_cast:
-  static bool classof(const SCEVPredicate *P) {
-    return P->getKind() == P_Wrap;
-  }
+  static bool classof(const SCEVPredicate *P) { return P->getKind() == P_Wrap; }
 };
 
 /// This class represents a composition of other SCEV predicates, and is the
@@ -412,7 +415,8 @@ public:
 /// logical "AND" of all the predicates in the union.
 ///
 /// NB! Unlike other SCEVPredicate sub-classes this class does not live in the
-/// ScalarEvolution::Preds folding set.  This is why the \c add function is sound.
+/// ScalarEvolution::Preds folding set.  This is why the \c add function is
+/// sound.
 class SCEVUnionPredicate final : public SCEVPredicate {
 private:
   using PredicateMap =
@@ -829,8 +833,7 @@ public:
   /// unknown or not guaranteed to be the multiple of a constant., Will also
   /// return 1 if the trip count is very large (>= 2^32).
   /// Note that the argument is an exit count for loop L, NOT a trip count.
-  unsigned getSmallConstantTripMultiple(const Loop *L,
-                                        const SCEV *ExitCount);
+  unsigned getSmallConstantTripMultiple(const Loop *L, const SCEV *ExitCount);
 
   /// Returns the largest constant divisor of the trip count of the
   /// loop.  Will return 1 if no trip count could be computed, or if a
@@ -847,7 +850,7 @@ public:
                                         const BasicBlock *ExitingBlock);
 
   /// The terms "backedge taken count" and "exit count" are used
-  /// interchangeably to refer to the number of times the backedge of a loop 
+  /// interchangeably to refer to the number of times the backedge of a loop
   /// has executed before the loop is exited.
   enum ExitCountKind {
     /// An expression exactly describing the number of times the backedge has
@@ -860,7 +863,7 @@ public:
   };
 
   /// Return the number of times the backedge executes before the given exit
-  /// would be taken; if not exactly computable, return SCEVCouldNotCompute. 
+  /// would be taken; if not exactly computable, return SCEVCouldNotCompute.
   /// For a single exit loop, this value is equivelent to the result of
   /// getBackedgeTakenCount.  The loop is guaranteed to exit (via *some* exit)
   /// before the backedge is executed (ExitCount + 1) times.  Note that there
@@ -1321,7 +1324,7 @@ private:
   /// ExitNotTakenInfo and BackedgeTakenInfo.
   struct ExitLimit {
     const SCEV *ExactNotTaken; // The exit is not taken exactly this many times
-    const SCEV *MaxNotTaken; // The exit is not taken at most this many times
+    const SCEV *MaxNotTaken;   // The exit is not taken at most this many times
 
     // Not taken either exactly MaxNotTaken or zero times
     bool MaxOrZero = false;
@@ -1375,8 +1378,8 @@ private:
                               const SCEV *ExactNotTaken,
                               const SCEV *MaxNotTaken,
                               std::unique_ptr<SCEVUnionPredicate> Predicate)
-      : ExitingBlock(ExitingBlock), ExactNotTaken(ExactNotTaken),
-        MaxNotTaken(ExactNotTaken), Predicate(std::move(Predicate)) {}
+        : ExitingBlock(ExitingBlock), ExactNotTaken(ExactNotTaken),
+          MaxNotTaken(ExactNotTaken), Predicate(std::move(Predicate)) {}
 
     bool hasAlwaysTruePredicate() const {
       return !Predicate || Predicate->isAlwaysTrue();
@@ -1546,7 +1549,7 @@ private:
   BlockDisposition computeBlockDisposition(const SCEV *S, const BasicBlock *BB);
 
   /// Stores all SCEV that use a given SCEV as its direct operand.
-  DenseMap<const SCEV *, SmallPtrSet<const SCEV *, 8> > SCEVUsers;
+  DenseMap<const SCEV *, SmallPtrSet<const SCEV *, 8>> SCEVUsers;
 
   /// Memoized results from getRange
   DenseMap<const SCEV *, ConstantRange> UnsignedRanges;
@@ -1610,7 +1613,7 @@ private:
 
   /// A helper function for createAddRecFromPHI to handle simple cases.
   const SCEV *createSimpleAffineAddRec(PHINode *PN, Value *BEValueV,
-                                            Value *StartValueV);
+                                       Value *StartValueV);
 
   /// Helper function called from createNodeForPHI.
   const SCEV *createNodeFromSelectLikePHI(PHINode *PN);
@@ -1700,8 +1703,7 @@ private:
 
   ExitLimit computeExitLimitFromCondCached(ExitLimitCacheTy &Cache,
                                            const Loop *L, Value *ExitCond,
-                                           bool ExitIfTrue,
-                                           bool ControlsExit,
+                                           bool ExitIfTrue, bool ControlsExit,
                                            bool AllowPredicates);
   ExitLimit computeExitLimitFromCondImpl(ExitLimitCacheTy &Cache, const Loop *L,
                                          Value *ExitCond, bool ExitIfTrue,
@@ -1718,8 +1720,7 @@ private:
   /// to use a minimal set of SCEV predicates in order to return an exact
   /// answer.
   ExitLimit computeExitLimitFromICmp(const Loop *L, ICmpInst *ExitCond,
-                                     bool ExitIfTrue,
-                                     bool IsSubExpr,
+                                     bool ExitIfTrue, bool IsSubExpr,
                                      bool AllowPredicates = false);
 
   /// Variant of previous which takes the components representing an ICmp
@@ -1835,10 +1836,9 @@ private:
   /// whenever the condition described by Pred, FoundLHS, and FoundRHS is
   /// true. Here LHS is an operation that includes FoundLHS as one of its
   /// arguments.
-  bool isImpliedViaOperations(ICmpInst::Predicate Pred,
-                              const SCEV *LHS, const SCEV *RHS,
-                              const SCEV *FoundLHS, const SCEV *FoundRHS,
-                              unsigned Depth = 0);
+  bool isImpliedViaOperations(ICmpInst::Predicate Pred, const SCEV *LHS,
+                              const SCEV *RHS, const SCEV *FoundLHS,
+                              const SCEV *FoundRHS, unsigned Depth = 0);
 
   /// Test whether the condition described by Pred, LHS, and RHS is true.
   /// Use only simple non-recursive types of checks, such as range analysis etc.
@@ -1895,10 +1895,9 @@ private:
   /// This routine tries to figure out predicate for Phis which are SCEVUnknown
   /// if it is true for every possible incoming value from their respective
   /// basic blocks.
-  bool isImpliedViaMerge(ICmpInst::Predicate Pred,
-                         const SCEV *LHS, const SCEV *RHS,
-                         const SCEV *FoundLHS, const SCEV *FoundRHS,
-                         unsigned Depth);
+  bool isImpliedViaMerge(ICmpInst::Predicate Pred, const SCEV *LHS,
+                         const SCEV *RHS, const SCEV *FoundLHS,
+                         const SCEV *FoundRHS, unsigned Depth);
 
   /// Test whether the condition described by Pred, LHS, and RHS is true
   /// whenever the condition described by Pred, FoundLHS, and FoundRHS is
@@ -2078,8 +2077,8 @@ private:
                                  SCEV::NoWrapFlags Flags);
 
   // Get addrec expr already created or create a new one.
-  const SCEV *getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops,
-                                    const Loop *L, SCEV::NoWrapFlags Flags);
+  const SCEV *getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops, const Loop *L,
+                                    SCEV::NoWrapFlags Flags);
 
   /// Return x if \p Val is f(x) where f is a 1-1 function.
   const SCEV *stripInjectiveFunctions(const SCEV *Val) const;
diff --color -uprN llvm-14.0.6.src/include/llvm/Analysis/ScalarEvolutionExpressions.h llvm-14.0.6.src.patched/include/llvm/Analysis/ScalarEvolutionExpressions.h
--- llvm-14.0.6.src/include/llvm/Analysis/ScalarEvolutionExpressions.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/Analysis/ScalarEvolutionExpressions.h	2024-07-05 17:03:20.329059313 +0800
@@ -66,6 +66,10 @@ class SCEVConstant : public SCEV {
       : SCEV(ID, scConstant, 1), V(v) {}
 
 public:
+  // SCEVConstant(const SCEVConstant *N) : SCEV(N) {
+  //   // V = new ConstantInt();
+  //   V->setValue(N->getAPInt());
+  // }
   ConstantInt *getValue() const { return V; }
   const APInt &getAPInt() const { return getValue()->getValue(); }
 
@@ -196,6 +200,8 @@ protected:
         NumOperands(N) {}
 
 public:
+  // SCEVNAryExpr(const SCEVNAryExpr *N)
+  //     : Operands(N->Operands), NumOperands(N->NumOperands), SCEV(N) {}
   size_t getNumOperands() const { return NumOperands; }
 
   const SCEV *getOperand(unsigned i) const {
@@ -304,6 +310,7 @@ class SCEVUDivExpr : public SCEV {
   }
 
 public:
+  // SCEVUDivExpr(const SCEVUDivExpr *N) : SCEV(N), Operands(N->Operands) {}
   const SCEV *getLHS() const { return Operands[0]; }
   const SCEV *getRHS() const { return Operands[1]; }
   size_t getNumOperands() const { return 2; }
@@ -589,6 +596,9 @@ class SCEVUnknown final : public SCEV, p
   void allUsesReplacedWith(Value *New) override;
 
 public:
+  // Change note BY Zeng-WCh
+  bool is_copied = false;
+  // SCEVUnknown(const SCEVUnknown *N) : SCEV(N), SE(N->SE), Next(Next) {}
   Value *getValue() const { return getValPtr(); }
 
   /// @{
diff --color -uprN llvm-14.0.6.src/include/llvm/CodeGen/MachineFunctionPass.h llvm-14.0.6.src.patched/include/llvm/CodeGen/MachineFunctionPass.h
--- llvm-14.0.6.src/include/llvm/CodeGen/MachineFunctionPass.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/CodeGen/MachineFunctionPass.h	2024-07-04 15:24:55.527947755 +0800
@@ -29,7 +29,8 @@ namespace llvm {
 /// override runOnMachineFunction.
 class MachineFunctionPass : public FunctionPass {
 public:
-  bool doInitialization(Module&) override {
+  // virtual void dump() const {}
+  bool doInitialization(Module &) override {
     // Cache the properties info at module-init time so we don't have to
     // construct them for every function.
     RequiredProperties = getRequiredProperties();
@@ -37,6 +38,7 @@ public:
     ClearedProperties = getClearedProperties();
     return false;
   }
+
 protected:
   explicit MachineFunctionPass(char &ID) : FunctionPass(ID) {}
 
@@ -75,6 +77,6 @@ private:
   bool runOnFunction(Function &F) override;
 };
 
-} // End llvm namespace
+} // namespace llvm
 
 #endif
diff --color -uprN llvm-14.0.6.src/include/llvm/IR/Constants.h llvm-14.0.6.src.patched/include/llvm/IR/Constants.h
--- llvm-14.0.6.src/include/llvm/IR/Constants.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/IR/Constants.h	2024-07-04 15:24:55.527947755 +0800
@@ -87,7 +87,7 @@ class ConstantInt final : public Constan
 
 public:
   ConstantInt(const ConstantInt &) = delete;
-
+  void setValue(APInt num) { this->Val = num; }
   static ConstantInt *getTrue(LLVMContext &Context);
   static ConstantInt *getFalse(LLVMContext &Context);
   static ConstantInt *getBool(LLVMContext &Context, bool V);
diff --color -uprN llvm-14.0.6.src/include/llvm/IR/LegacyPassManagers.h llvm-14.0.6.src.patched/include/llvm/IR/LegacyPassManagers.h
--- llvm-14.0.6.src/include/llvm/IR/LegacyPassManagers.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/IR/LegacyPassManagers.h	2024-07-04 15:24:55.527947755 +0800
@@ -12,12 +12,12 @@
 
 #ifndef LLVM_IR_LEGACYPASSMANAGERS_H
 #define LLVM_IR_LEGACYPASSMANAGERS_H
-
 #include "llvm/ADT/DenseMap.h"
 #include "llvm/ADT/FoldingSet.h"
 #include "llvm/ADT/SmallPtrSet.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/Pass.h"
+#include <iostream>
 #include <vector>
 
 //===----------------------------------------------------------------------===//
@@ -31,9 +31,9 @@
 //     information before it is consumed by another pass.
 //
 // Pass Manager Infrastructure uses multiple pass managers.  They are
-// PassManager, FunctionPassManager, MPPassManager, FPPassManager, BBPassManager.
-// This class hierarchy uses multiple inheritance but pass managers do not
-// derive from another pass manager.
+// PassManager, FunctionPassManager, MPPassManager, FPPassManager,
+// BBPassManager. This class hierarchy uses multiple inheritance but pass
+// managers do not derive from another pass manager.
 //
 // PassManager and FunctionPassManager are two top-level pass manager that
 // represents the external interface of this entire pass manager infrastucture.
@@ -94,18 +94,18 @@ class PMDataManager;
 
 // enums for debugging strings
 enum PassDebuggingString {
-  EXECUTION_MSG, // "Executing Pass '" + PassName
+  EXECUTION_MSG,    // "Executing Pass '" + PassName
   MODIFICATION_MSG, // "Made Modification '" + PassName
-  FREEING_MSG, // " Freeing Pass '" + PassName
-  ON_FUNCTION_MSG, // "' on Function '" + FunctionName + "'...\n"
-  ON_MODULE_MSG, // "' on Module '" + ModuleName + "'...\n"
-  ON_REGION_MSG, // "' on Region '" + Msg + "'...\n'"
-  ON_LOOP_MSG, // "' on Loop '" + Msg + "'...\n'"
-  ON_CG_MSG // "' on Call Graph Nodes '" + Msg + "'...\n'"
+  FREEING_MSG,      // " Freeing Pass '" + PassName
+  ON_FUNCTION_MSG,  // "' on Function '" + FunctionName + "'...\n"
+  ON_MODULE_MSG,    // "' on Module '" + ModuleName + "'...\n"
+  ON_REGION_MSG,    // "' on Region '" + Msg + "'...\n'"
+  ON_LOOP_MSG,      // "' on Loop '" + Msg + "'...\n'"
+  ON_CG_MSG         // "' on Call Graph Nodes '" + Msg + "'...\n'"
 };
 
-/// PassManagerPrettyStackEntry - This is used to print informative information
-/// about what pass is running when/if a stack trace is generated.
+/// PassManagerPrettyStackEntry - This is used to print informative
+/// information about what pass is running when/if a stack trace is generated.
 class PassManagerPrettyStackEntry : public PrettyStackTraceEntry {
   Pass *P;
   Value *V;
@@ -113,11 +113,11 @@ class PassManagerPrettyStackEntry : publ
 
 public:
   explicit PassManagerPrettyStackEntry(Pass *p)
-    : P(p), V(nullptr), M(nullptr) {}  // When P is releaseMemory'd.
+      : P(p), V(nullptr), M(nullptr) {} // When P is releaseMemory'd.
   PassManagerPrettyStackEntry(Pass *p, Value &v)
-    : P(p), V(&v), M(nullptr) {} // When P is run on V
+      : P(p), V(&v), M(nullptr) {} // When P is run on V
   PassManagerPrettyStackEntry(Pass *p, Module &m)
-    : P(p), V(nullptr), M(&m) {} // When P is run on M
+      : P(p), V(nullptr), M(&m) {} // When P is run on M
 
   /// print - Emit information about this stack frame to OS.
   void print(raw_ostream &OS) const override;
@@ -129,10 +129,10 @@ public:
 /// PMStack - This class implements a stack data structure of PMDataManager
 /// pointers.
 ///
-/// Top level pass managers (see PassManager.cpp) maintain active Pass Managers
-/// using PMStack. Each Pass implements assignPassManager() to connect itself
-/// with appropriate manager. assignPassManager() walks PMStack to find
-/// suitable manager.
+/// Top level pass managers (see PassManager.cpp) maintain active Pass
+/// Managers using PMStack. Each Pass implements assignPassManager() to
+/// connect itself with appropriate manager. assignPassManager() walks PMStack
+/// to find suitable manager.
 class PMStack {
 public:
   typedef std::vector<PMDataManager *>::const_reverse_iterator iterator;
@@ -176,7 +176,7 @@ public:
   void schedulePass(Pass *P);
 
   /// Set pass P as the last user of the given analysis passes.
-  void setLastUser(ArrayRef<Pass*> AnalysisPasses, Pass *P);
+  void setLastUser(ArrayRef<Pass *> AnalysisPasses, Pass *P);
 
   /// Collect passes whose last user is P
   void collectLastUses(SmallVectorImpl<Pass *> &LastUses, Pass *P);
@@ -197,7 +197,7 @@ public:
   /// Add immutable pass and initialize it.
   void addImmutablePass(ImmutablePass *P);
 
-  inline SmallVectorImpl<ImmutablePass *>& getImmutablePasses() {
+  inline SmallVectorImpl<ImmutablePass *> &getImmutablePasses() {
     return ImmutablePasses;
   }
 
@@ -234,7 +234,7 @@ private:
 
   // Map to keep track of passes that are last used by a pass.
   // This is kept in sync with LastUser.
-  DenseMap<Pass *, SmallPtrSet<Pass *, 8> > InversedLastUser;
+  DenseMap<Pass *, SmallPtrSet<Pass *, 8>> InversedLastUser;
 
   /// Immutable passes are managed by top level manager.
   SmallVector<ImmutablePass *, 16> ImmutablePasses;
@@ -242,22 +242,20 @@ private:
   /// Map from ID to immutable passes.
   SmallDenseMap<AnalysisID, ImmutablePass *, 8> ImmutablePassMap;
 
-
-  /// A wrapper around AnalysisUsage for the purpose of uniqueing.  The wrapper
-  /// is used to avoid needing to make AnalysisUsage itself a folding set node.
+  /// A wrapper around AnalysisUsage for the purpose of uniqueing.  The
+  /// wrapper is used to avoid needing to make AnalysisUsage itself a folding
+  /// set node.
   struct AUFoldingSetNode : public FoldingSetNode {
     AnalysisUsage AU;
     AUFoldingSetNode(const AnalysisUsage &AU) : AU(AU) {}
-    void Profile(FoldingSetNodeID &ID) const {
-      Profile(ID, AU);
-    }
+    void Profile(FoldingSetNodeID &ID) const { Profile(ID, AU); }
     static void Profile(FoldingSetNodeID &ID, const AnalysisUsage &AU) {
       // TODO: We could consider sorting the dependency arrays within the
       // AnalysisUsage (since they are conceptually unordered).
       ID.AddBoolean(AU.getPreservesAll());
-      auto ProfileVec = [&](const SmallVectorImpl<AnalysisID>& Vec) {
+      auto ProfileVec = [&](const SmallVectorImpl<AnalysisID> &Vec) {
         ID.AddInteger(Vec.size());
-        for(AnalysisID AID : Vec)
+        for (AnalysisID AID : Vec)
           ID.AddPointer(AID);
       };
       ProfileVec(AU.getRequiredSet());
@@ -267,18 +265,18 @@ private:
     }
   };
 
-  // Contains all of the unique combinations of AnalysisUsage.  This is helpful
-  // when we have multiple instances of the same pass since they'll usually
-  // have the same analysis usage and can share storage.
+  // Contains all of the unique combinations of AnalysisUsage.  This is
+  // helpful when we have multiple instances of the same pass since they'll
+  // usually have the same analysis usage and can share storage.
   FoldingSet<AUFoldingSetNode> UniqueAnalysisUsages;
 
-  // Allocator used for allocating UAFoldingSetNodes.  This handles deletion of
-  // all allocated nodes in one fell swoop.
+  // Allocator used for allocating UAFoldingSetNodes.  This handles deletion
+  // of all allocated nodes in one fell swoop.
   SpecificBumpPtrAllocator<AUFoldingSetNode> AUFoldingSetNodeAllocator;
 
   // Maps from a pass to it's associated entry in UniqueAnalysisUsages.  Does
   // not own the storage associated with either key or value..
-  DenseMap<Pass *, AnalysisUsage*> AnUsageMap;
+  DenseMap<Pass *, AnalysisUsage *> AnUsageMap;
 
   /// Collection of PassInfo objects found via analysis IDs and in this top
   /// level manager. This is used to memoize queries to the pass registry.
@@ -312,12 +310,10 @@ public:
   void removeNotPreservedAnalysis(Pass *P);
 
   /// Remove dead passes used by P.
-  void removeDeadPasses(Pass *P, StringRef Msg,
-                        enum PassDebuggingString);
+  void removeDeadPasses(Pass *P, StringRef Msg, enum PassDebuggingString);
 
   /// Remove P.
-  void freePass(Pass *P, StringRef Msg,
-                enum PassDebuggingString);
+  void freePass(Pass *P, StringRef Msg, enum PassDebuggingString);
 
   /// Add pass P into the PassVector. Update
   /// AvailableAnalysis appropriately if ProcessAnalysis is true.
@@ -343,8 +339,8 @@ public:
   bool preserveHigherLevelAnalysis(Pass *P);
 
   /// Populate UsedPasses with analysis pass that are used or required by pass
-  /// P and are available. Populate ReqPassNotAvailable with analysis pass that
-  /// are required by pass P but are not available.
+  /// P and are available. Populate ReqPassNotAvailable with analysis pass
+  /// that are required by pass P but are not available.
   void collectRequiredAndUsedAnalyses(
       SmallVectorImpl<Pass *> &UsedPasses,
       SmallVectorImpl<AnalysisID> &ReqPassNotAvailable, Pass *P);
@@ -375,16 +371,14 @@ public:
   void dumpPreservedSet(const Pass *P) const;
   void dumpUsedSet(const Pass *P) const;
 
-  unsigned getNumContainedPasses() const {
-    return (unsigned)PassVector.size();
-  }
+  unsigned getNumContainedPasses() const { return (unsigned)PassVector.size(); }
 
   virtual PassManagerType getPassManagerType() const {
-    assert ( 0 && "Invalid use of getPassManagerType");
+    assert(0 && "Invalid use of getPassManagerType");
     return PMT_Unknown;
   }
 
-  DenseMap<AnalysisID, Pass*> *getAvailableAnalysis() {
+  DenseMap<AnalysisID, Pass *> *getAvailableAnalysis() {
     return &AvailableAnalysis;
   }
 
@@ -402,15 +396,15 @@ public:
       Module &M,
       StringMap<std::pair<unsigned, unsigned>> &FunctionToInstrCount);
 
-  /// Emit a remark signifying that the number of IR instructions in the module
-  /// changed.
-  /// \p F is optionally passed by passes which run on Functions, and thus
-  /// always know whether or not a non-empty function is available.
+  /// Emit a remark signifying that the number of IR instructions in the
+  /// module changed. \p F is optionally passed by passes which run on
+  /// Functions, and thus always know whether or not a non-empty function is
+  /// available.
   ///
   /// \p FunctionToInstrCount maps the name of a \p Function to a pair. The
-  /// first member of the pair is the IR count of the \p Function before running
-  /// \p P, and the second member is the IR count of the \p Function after
-  /// running \p P.
+  /// first member of the pair is the IR count of the \p Function before
+  /// running \p P, and the second member is the IR count of the \p Function
+  /// after running \p P.
   void emitInstrCountChangedRemark(
       Pass *P, Module &M, int64_t Delta, unsigned CountBefore,
       StringMap<std::pair<unsigned, unsigned>> &FunctionToInstrCount,
@@ -440,7 +434,7 @@ private:
   // pass. If a pass requires an analysis which is not available then
   // the required analysis pass is scheduled to run before the pass itself is
   // scheduled to run.
-  DenseMap<AnalysisID, Pass*> AvailableAnalysis;
+  DenseMap<AnalysisID, Pass *> AvailableAnalysis;
 
   // Collection of higher level analysis used by the pass managed by
   // this manager.
@@ -501,7 +495,7 @@ public:
   StringRef getPassName() const override { return "Function Pass Manager"; }
 
   FunctionPass *getContainedPass(unsigned N) {
-    assert ( N < PassVector.size() && "Pass number out of range!");
+    assert(N < PassVector.size() && "Pass number out of range!");
     FunctionPass *FP = static_cast<FunctionPass *>(PassVector[N]);
     return FP;
   }
@@ -511,6 +505,6 @@ public:
   }
 };
 
-}
+} // namespace llvm
 
 #endif
diff --color -uprN llvm-14.0.6.src/include/llvm/IR/Value.h llvm-14.0.6.src.patched/include/llvm/IR/Value.h
--- llvm-14.0.6.src/include/llvm/IR/Value.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/IR/Value.h	2024-07-04 15:24:55.527947755 +0800
@@ -47,7 +47,7 @@ class MDNode;
 class Module;
 class ModuleSlotTracker;
 class raw_ostream;
-template<typename ValueTy> class StringMapEntry;
+template <typename ValueTy> class StringMapEntry;
 class Twine;
 class Type;
 class User;
@@ -80,6 +80,13 @@ class Value {
 
   const unsigned char SubclassID;   // Subclass identifier (for isa/dyn_cast)
   unsigned char HasValueHandle : 1; // Has a ValueHandle pointing to this?
+public:
+  //别改依赖谢谢
+  // Value(Value &V)
+  //     : VTy(V.VTy), UseList(V.UseList), SubclassData(V.SubclassData),
+  //       SubclassID(V.SubclassID),
+  //       SubclassOptionalData(V.SubclassOptionalData) {
+  // }
 
 protected:
   /// Hold subclass data that can be dropped.
@@ -181,7 +188,9 @@ private:
     user_iterator_impl() = default;
 
     bool operator==(const user_iterator_impl &x) const { return UI == x.UI; }
-    bool operator!=(const user_iterator_impl &x) const { return !operator==(x); }
+    bool operator!=(const user_iterator_impl &x) const {
+      return !operator==(x);
+    }
 
     /// Returns true if this iterator is equal to user_end() on the value.
     bool atEnd() const { return *this == user_iterator_impl(); }
@@ -198,9 +207,7 @@ private:
     }
 
     // Retrieve a pointer to the current User.
-    UserTy *operator*() const {
-      return UI->getUser();
-    }
+    UserTy *operator*() const { return UI->getUser(); }
 
     UserTy *operator->() const { return operator*(); }
 
@@ -346,9 +353,7 @@ public:
     return UseList == nullptr;
   }
 
-  bool materialized_use_empty() const {
-    return UseList == nullptr;
-  }
+  bool materialized_use_empty() const { return UseList == nullptr; }
 
   using use_iterator = use_iterator_impl<Use>;
   using const_use_iterator = use_iterator_impl<const Use>;
@@ -458,8 +463,8 @@ public:
     return const_cast<Value *>(this)->getSingleUndroppableUse();
   }
 
-  /// Return true if there is exactly one unique user of this value that cannot be
-  /// dropped (that user can have multiple uses of this value).
+  /// Return true if there is exactly one unique user of this value that cannot
+  /// be dropped (that user can have multiple uses of this value).
   User *getUniqueUndroppableUser();
   const User *getUniqueUndroppableUser() const {
     return const_cast<Value *>(this)->getUniqueUndroppableUser();
@@ -514,7 +519,7 @@ public:
 #define HANDLE_VALUE(Name) Name##Val,
 #include "llvm/IR/Value.def"
 
-    // Markers:
+  // Markers:
 #define HANDLE_CONSTANT_MARKER(Marker, Constant) Marker = Constant##Val,
 #include "llvm/IR/Value.def"
   };
@@ -529,21 +534,15 @@ public:
   /// # there are more possible values for the value type than in ValueTy enum.
   /// # the InstructionVal enumerator must be the highest valued enumerator in
   ///   the ValueTy enum.
-  unsigned getValueID() const {
-    return SubclassID;
-  }
+  unsigned getValueID() const { return SubclassID; }
 
   /// Return the raw optional flags value contained in this value.
   ///
   /// This should only be used when testing two Values for equivalence.
-  unsigned getRawSubclassOptionalData() const {
-    return SubclassOptionalData;
-  }
+  unsigned getRawSubclassOptionalData() const { return SubclassOptionalData; }
 
   /// Clear the optional flags contained in this value.
-  void clearSubclassOptionalData() {
-    SubclassOptionalData = 0;
-  }
+  void clearSubclassOptionalData() { SubclassOptionalData = 0; }
 
   /// Check the optional flags for equality.
   bool hasSameSubclassOptionalData(const Value *V) const {
@@ -666,8 +665,8 @@ public:
   /// Alias analysis.
   const Value *stripPointerCastsForAliasAnalysis() const;
   Value *stripPointerCastsForAliasAnalysis() {
-    return const_cast<Value *>(static_cast<const Value *>(this)
-                                   ->stripPointerCastsForAliasAnalysis());
+    return const_cast<Value *>(
+        static_cast<const Value *>(this)->stripPointerCastsForAliasAnalysis());
   }
 
   /// Strip off pointer casts and all-constant inbounds GEPs.
@@ -677,7 +676,7 @@ public:
   const Value *stripInBoundsConstantOffsets() const;
   Value *stripInBoundsConstantOffsets() {
     return const_cast<Value *>(
-              static_cast<const Value *>(this)->stripInBoundsConstantOffsets());
+        static_cast<const Value *>(this)->stripInBoundsConstantOffsets());
   }
 
   /// Accumulate the constant offset this value has compared to a base pointer.
@@ -742,7 +741,7 @@ public:
   const Value *stripInBoundsOffsets(function_ref<void(const Value *)> Func =
                                         [](const Value *) {}) const;
   inline Value *stripInBoundsOffsets(function_ref<void(const Value *)> Func =
-                                  [](const Value *) {}) {
+                                         [](const Value *) {}) {
     return const_cast<Value *>(
         static_cast<const Value *>(this)->stripInBoundsOffsets(Func));
   }
@@ -762,8 +761,7 @@ public:
   /// IF CanBeFreed is true, the pointer is known to be dereferenceable at
   /// point of definition only.  Caller must prove that allocation is not
   /// deallocated between point of definition and use.
-  uint64_t getPointerDereferenceableBytes(const DataLayout &DL,
-                                          bool &CanBeNull,
+  uint64_t getPointerDereferenceableBytes(const DataLayout &DL, bool &CanBeNull,
                                           bool &CanBeFreed) const;
 
   /// Returns an alignment of the pointer value.
@@ -782,7 +780,7 @@ public:
                                 const BasicBlock *PredBB) const;
   Value *DoPHITranslation(const BasicBlock *CurBB, const BasicBlock *PredBB) {
     return const_cast<Value *>(
-             static_cast<const Value *>(this)->DoPHITranslation(CurBB, PredBB));
+        static_cast<const Value *>(this)->DoPHITranslation(CurBB, PredBB));
   }
 
   /// The maximum alignment for instructions.
@@ -798,9 +796,7 @@ public:
   /// completely invalid IR very easily.  It is strongly recommended that you
   /// recreate IR objects with the right types instead of mutating them in
   /// place.
-  void mutateType(Type *Ty) {
-    VTy = Ty;
-  }
+  void mutateType(Type *Ty) { VTy = Ty; }
 
   /// Sort the use-list.
   ///
@@ -853,7 +849,9 @@ protected:
   void setValueSubclassData(unsigned short D) { SubclassData = D; }
 };
 
-struct ValueDeleter { void operator()(Value *V) { V->deleteValue(); } };
+struct ValueDeleter {
+  void operator()(Value *V) { V->deleteValue(); }
+};
 
 /// Use this instead of std::unique_ptr<Value> or std::unique_ptr<Instruction>.
 /// Those don't work because Value and Instruction's destructors are protected,
@@ -866,9 +864,11 @@ inline raw_ostream &operator<<(raw_ostre
 }
 
 void Use::set(Value *V) {
-  if (Val) removeFromList();
+  if (Val)
+    removeFromList();
   Val = V;
-  if (V) V->addUse(*this);
+  if (V)
+    V->addUse(*this);
 }
 
 Value *Use::operator=(Value *RHS) {
@@ -953,7 +953,8 @@ template <class Compare> void Value::sor
 //
 template <> struct isa_impl<Constant, Value> {
   static inline bool doit(const Value &Val) {
-    static_assert(Value::ConstantFirstVal == 0, "Val.getValueID() >= Value::ConstantFirstVal");
+    static_assert(Value::ConstantFirstVal == 0,
+                  "Val.getValueID() >= Value::ConstantFirstVal");
     return Val.getValueID() <= Value::ConstantLastVal;
   }
 };
@@ -973,7 +974,7 @@ template <> struct isa_impl<ConstantAggr
 };
 
 template <> struct isa_impl<Argument, Value> {
-  static inline bool doit (const Value &Val) {
+  static inline bool doit(const Value &Val) {
     return Val.getValueID() == Value::ArgumentVal;
   }
 };
@@ -1038,21 +1039,20 @@ DEFINE_ISA_CONVERSION_FUNCTIONS(Value, L
 
 // Specialized opaque value conversions.
 inline Value **unwrap(LLVMValueRef *Vals) {
-  return reinterpret_cast<Value**>(Vals);
+  return reinterpret_cast<Value **>(Vals);
 }
 
-template<typename T>
-inline T **unwrap(LLVMValueRef *Vals, unsigned Length) {
+template <typename T> inline T **unwrap(LLVMValueRef *Vals, unsigned Length) {
 #ifndef NDEBUG
   for (LLVMValueRef *I = Vals, *E = Vals + Length; I != E; ++I)
     unwrap<T>(*I); // For side effect of calling assert on invalid usage.
 #endif
   (void)Length;
-  return reinterpret_cast<T**>(Vals);
+  return reinterpret_cast<T **>(Vals);
 }
 
 inline LLVMValueRef *wrap(const Value **Vals) {
-  return reinterpret_cast<LLVMValueRef*>(const_cast<Value**>(Vals));
+  return reinterpret_cast<LLVMValueRef *>(const_cast<Value **>(Vals));
 }
 
 } // end namespace llvm
diff --color -uprN llvm-14.0.6.src/include/llvm/Pass.h llvm-14.0.6.src.patched/include/llvm/Pass.h
--- llvm-14.0.6.src/include/llvm/Pass.h	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/include/llvm/Pass.h	2024-07-04 15:24:55.527947755 +0800
@@ -89,7 +89,7 @@ enum class ThinOrFullLTOPhase {
 /// constrained passes described below.
 ///
 class Pass {
-  AnalysisResolver *Resolver = nullptr;  // Used to resolve analysis
+  AnalysisResolver *Resolver = nullptr; // Used to resolve analysis
   const void *PassID;
   PassKind Kind;
 
@@ -107,13 +107,11 @@ public:
   virtual StringRef getPassName() const;
 
   /// getPassID - Return the PassID number that corresponds to this pass.
-  AnalysisID getPassID() const {
-    return PassID;
-  }
+  AnalysisID getPassID() const { return PassID; }
 
   /// doInitialization - Virtual method overridden by subclasses to do
   /// any necessary initialization before any pass is run.
-  virtual bool doInitialization(Module &)  { return false; }
+  virtual bool doInitialization(Module &) { return false; }
 
   /// doFinalization - Virtual method overriden by subclasses to do any
   /// necessary clean up after all passes have run.
@@ -127,7 +125,7 @@ public:
   /// ignored.
   virtual void print(raw_ostream &OS, const Module *M) const;
 
-  void dump() const; // dump - Print to stderr.
+  virtual void dump() const; // dump - Print to stderr.
 
   /// createPrinterPass - Get a Pass appropriate to print the IR this
   /// pass operates on (Module, Function or MachineFunction).
@@ -136,8 +134,7 @@ public:
 
   /// Each pass is responsible for assigning a pass manager to itself.
   /// PMS is the stack of available pass manager.
-  virtual void assignPassManager(PMStack &,
-                                 PassManagerType) {}
+  virtual void assignPassManager(PMStack &, PassManagerType) {}
 
   /// Check if available pass managers are suitable for this pass or not.
   virtual void preparePassManager(PMStack &);
@@ -201,8 +198,9 @@ public:
   /// the case when the analysis is not available.  This method is often used by
   /// transformation APIs to update analysis results for a pass automatically as
   /// the transform is performed.
-  template<typename AnalysisType> AnalysisType *
-    getAnalysisIfAvailable() const; // Defined in PassAnalysisSupport.h
+  template <typename AnalysisType>
+  AnalysisType *
+  getAnalysisIfAvailable() const; // Defined in PassAnalysisSupport.h
 
   /// mustPreserveAnalysisID - This method serves the same function as
   /// getAnalysisIfAvailable, but works if you just have an AnalysisID.  This
@@ -214,7 +212,7 @@ public:
   /// getAnalysis<AnalysisType>() - This function is used by subclasses to get
   /// to the analysis information that they claim to use by overriding the
   /// getAnalysisUsage function.
-  template<typename AnalysisType>
+  template <typename AnalysisType>
   AnalysisType &getAnalysis() const; // Defined in PassAnalysisSupport.h
 
   template <typename AnalysisType>
@@ -222,7 +220,7 @@ public:
   getAnalysis(Function &F,
               bool *Changed = nullptr); // Defined in PassAnalysisSupport.h
 
-  template<typename AnalysisType>
+  template <typename AnalysisType>
   AnalysisType &getAnalysisID(AnalysisID PI) const;
 
   template <typename AnalysisType>
@@ -306,6 +304,7 @@ public:
   /// runOnFunction - Virtual method overriden by subclasses to do the
   /// per-function processing of the pass.
   virtual bool runOnFunction(Function &F) = 0;
+  // virtual void dump() const {}
 
   void assignPassManager(PMStack &PMS, PassManagerType T) override;
 
diff --color -uprN llvm-14.0.6.src/lib/Analysis/ScalarEvolution.cpp llvm-14.0.6.src.patched/lib/Analysis/ScalarEvolution.cpp
--- llvm-14.0.6.src/lib/Analysis/ScalarEvolution.cpp	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/lib/Analysis/ScalarEvolution.cpp	2024-07-05 16:54:32.389431060 +0800
@@ -147,12 +147,12 @@ STATISTIC(NumBruteForceTripCountsCompute
           "Number of loops with trip counts computed by force");
 
 static cl::opt<unsigned>
-MaxBruteForceIterations("scalar-evolution-max-iterations", cl::ReallyHidden,
-                        cl::ZeroOrMore,
-                        cl::desc("Maximum number of iterations SCEV will "
-                                 "symbolically execute a constant "
-                                 "derived loop"),
-                        cl::init(100));
+    MaxBruteForceIterations("scalar-evolution-max-iterations", cl::ReallyHidden,
+                            cl::ZeroOrMore,
+                            cl::desc("Maximum number of iterations SCEV will "
+                                     "symbolically execute a constant "
+                                     "derived loop"),
+                            cl::init(100));
 
 // FIXME: Enable this with EXPENSIVE_CHECKS when the test suite is clean.
 static cl::opt<bool> VerifySCEV(
@@ -215,15 +215,15 @@ static cl::opt<unsigned>
                   cl::desc("Max coefficients in AddRec during evolving"),
                   cl::init(8));
 
-static cl::opt<unsigned>
-    HugeExprThreshold("scalar-evolution-huge-expr-threshold", cl::Hidden,
-                  cl::desc("Size of the expression which is considered huge"),
-                  cl::init(4096));
-
-static cl::opt<bool>
-ClassifyExpressions("scalar-evolution-classify-expressions",
-    cl::Hidden, cl::init(true),
-    cl::desc("When printing analysis, include information on every instruction"));
+static cl::opt<unsigned> HugeExprThreshold(
+    "scalar-evolution-huge-expr-threshold", cl::Hidden,
+    cl::desc("Size of the expression which is considered huge"),
+    cl::init(4096));
+
+static cl::opt<bool> ClassifyExpressions(
+    "scalar-evolution-classify-expressions", cl::Hidden, cl::init(true),
+    cl::desc(
+        "When printing analysis, include information on every instruction"));
 
 static cl::opt<bool> UseExpensiveRangeSharpening(
     "scalar-evolution-use-expensive-range-sharpening", cl::Hidden,
@@ -248,9 +248,11 @@ LLVM_DUMP_METHOD void SCEV::dump() const
 
 void SCEV::print(raw_ostream &OS) const {
   switch (getSCEVType()) {
-  case scConstant:
-    cast<SCEVConstant>(this)->getValue()->printAsOperand(OS, false);
+  case scConstant: {
+    const SCEVConstant *i = cast<SCEVConstant>(this);
+    i->getValue()->printAsOperand(OS, false);
     return;
+  }
   case scPtrToInt: {
     const SCEVPtrToIntExpr *PtrToInt = cast<SCEVPtrToIntExpr>(this);
     const SCEV *Op = PtrToInt->getOperand();
@@ -268,15 +270,15 @@ void SCEV::print(raw_ostream &OS) const
   case scZeroExtend: {
     const SCEVZeroExtendExpr *ZExt = cast<SCEVZeroExtendExpr>(this);
     const SCEV *Op = ZExt->getOperand();
-    OS << "(zext " << *Op->getType() << " " << *Op << " to "
-       << *ZExt->getType() << ")";
+    OS << "(zext " << *Op->getType() << " " << *Op << " to " << *ZExt->getType()
+       << ")";
     return;
   }
   case scSignExtend: {
     const SCEVSignExtendExpr *SExt = cast<SCEVSignExtendExpr>(this);
     const SCEV *Op = SExt->getOperand();
-    OS << "(sext " << *Op->getType() << " " << *Op << " to "
-       << *SExt->getType() << ")";
+    OS << "(sext " << *Op->getType() << " " << *Op << " to " << *SExt->getType()
+       << ")";
     return;
   }
   case scAddRecExpr: {
@@ -306,10 +308,18 @@ void SCEV::print(raw_ostream &OS) const
     const SCEVNAryExpr *NAry = cast<SCEVNAryExpr>(this);
     const char *OpStr = nullptr;
     switch (NAry->getSCEVType()) {
-    case scAddExpr: OpStr = " + "; break;
-    case scMulExpr: OpStr = " * "; break;
-    case scUMaxExpr: OpStr = " umax "; break;
-    case scSMaxExpr: OpStr = " smax "; break;
+    case scAddExpr:
+      OpStr = " + ";
+      break;
+    case scMulExpr:
+      OpStr = " * ";
+      break;
+    case scUMaxExpr:
+      OpStr = " umax ";
+      break;
+    case scSMaxExpr:
+      OpStr = " smax ";
+      break;
     case scUMinExpr:
       OpStr = " umin ";
       break;
@@ -323,9 +333,9 @@ void SCEV::print(raw_ostream &OS) const
       llvm_unreachable("There are no other nary expression types.");
     }
     OS << "(";
-    ListSeparator LS(OpStr);
-    for (const SCEV *Op : NAry->operands())
-      OS << LS << *Op;
+    // ListSeparator LS(OpStr);
+    // for (const SCEV *Op : NAry->operands())
+    //   OS << LS << *Op;
     OS << ")";
     switch (NAry->getSCEVType()) {
     case scAddExpr:
@@ -430,18 +440,20 @@ bool SCEV::isAllOnesValue() const {
 
 bool SCEV::isNonConstantNegative() const {
   const SCEVMulExpr *Mul = dyn_cast<SCEVMulExpr>(this);
-  if (!Mul) return false;
+  if (!Mul)
+    return false;
 
   // If there is a constant factor, it will be first.
   const SCEVConstant *SC = dyn_cast<SCEVConstant>(Mul->getOperand(0));
-  if (!SC) return false;
+  if (!SC)
+    return false;
 
   // Return true if the value is negative, this matches things like (-42 * V).
   return SC->getAPInt().isNegative();
 }
 
-SCEVCouldNotCompute::SCEVCouldNotCompute() :
-  SCEV(FoldingSetNodeIDRef(), scCouldNotCompute, 0) {}
+SCEVCouldNotCompute::SCEVCouldNotCompute()
+    : SCEV(FoldingSetNodeIDRef(), scCouldNotCompute, 0) {}
 
 bool SCEVCouldNotCompute::classof(const SCEV *S) {
   return S->getSCEVType() == scCouldNotCompute;
@@ -452,7 +464,8 @@ const SCEV *ScalarEvolution::getConstant
   ID.AddInteger(scConstant);
   ID.AddPointer(V);
   void *IP = nullptr;
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
   SCEV *S = new (SCEVAllocator) SCEVConstant(ID.Intern(SCEVAllocator), V);
   UniqueSCEVs.InsertNode(S, IP);
   return S;
@@ -462,8 +475,7 @@ const SCEV *ScalarEvolution::getConstant
   return getConstant(ConstantInt::get(getContext(), Val));
 }
 
-const SCEV *
-ScalarEvolution::getConstant(Type *Ty, uint64_t V, bool isSigned) {
+const SCEV *ScalarEvolution::getConstant(Type *Ty, uint64_t V, bool isSigned) {
   IntegerType *ITy = cast<IntegerType>(getEffectiveSCEVType(Ty));
   return getConstant(ConstantInt::get(ITy, V, isSigned));
 }
@@ -508,6 +520,11 @@ SCEVSignExtendExpr::SCEVSignExtendExpr(c
 }
 
 void SCEVUnknown::deleted() {
+  // change by Zeng-WCh
+  if (this->is_copied) {
+    setValPtr(nullptr);
+    return;
+  }
   // Clear this SCEVUnknown from various maps.
   SE->forgetMemoizedResults(this);
 
@@ -533,8 +550,7 @@ bool SCEVUnknown::isSizeOf(Type *&AllocT
     if (VCE->getOpcode() == Instruction::PtrToInt)
       if (ConstantExpr *CE = dyn_cast<ConstantExpr>(VCE->getOperand(0)))
         if (CE->getOpcode() == Instruction::GetElementPtr &&
-            CE->getOperand(0)->isNullValue() &&
-            CE->getNumOperands() == 2)
+            CE->getOperand(0)->isNullValue() && CE->getNumOperands() == 2)
           if (ConstantInt *CI = dyn_cast<ConstantInt>(CE->getOperand(1)))
             if (CI->isOne()) {
               AllocTy = cast<GEPOperator>(CE)->getSourceElementType();
@@ -552,12 +568,10 @@ bool SCEVUnknown::isAlignOf(Type *&Alloc
             CE->getOperand(0)->isNullValue()) {
           Type *Ty = cast<GEPOperator>(CE)->getSourceElementType();
           if (StructType *STy = dyn_cast<StructType>(Ty))
-            if (!STy->isPacked() &&
-                CE->getNumOperands() == 3 &&
+            if (!STy->isPacked() && CE->getNumOperands() == 3 &&
                 CE->getOperand(1)->isNullValue()) {
               if (ConstantInt *CI = dyn_cast<ConstantInt>(CE->getOperand(2)))
-                if (CI->isOne() &&
-                    STy->getNumElements() == 2 &&
+                if (CI->isOne() && STy->getNumElements() == 2 &&
                     STy->getElementType(0)->isIntegerTy(1)) {
                   AllocTy = STy->getElementType(1);
                   return true;
@@ -573,8 +587,7 @@ bool SCEVUnknown::isOffsetOf(Type *&CTy,
     if (VCE->getOpcode() == Instruction::PtrToInt)
       if (ConstantExpr *CE = dyn_cast<ConstantExpr>(VCE->getOperand(0)))
         if (CE->getOpcode() == Instruction::GetElementPtr &&
-            CE->getNumOperands() == 3 &&
-            CE->getOperand(0)->isNullValue() &&
+            CE->getNumOperands() == 3 && CE->getOperand(0)->isNullValue() &&
             CE->getOperand(1)->isNullValue()) {
           Type *Ty = cast<GEPOperator>(CE)->getSourceElementType();
           // Ignore vector types here so that ScalarEvolutionExpander doesn't
@@ -848,9 +861,10 @@ CompareSCEVComplexity(EquivalenceClasses
 /// results from this routine.  In other words, we don't want the results of
 /// this to depend on where the addresses of various SCEV objects happened to
 /// land in memory.
-static void GroupByComplexity(SmallVectorImpl<const SCEV *> &Ops,
-                              LoopInfo *LI, DominatorTree &DT) {
-  if (Ops.size() < 2) return;  // Noop
+static void GroupByComplexity(SmallVectorImpl<const SCEV *> &Ops, LoopInfo *LI,
+                              DominatorTree &DT) {
+  if (Ops.size() < 2)
+    return; // Noop
 
   EquivalenceClasses<const SCEV *> EqCacheSCEV;
   EquivalenceClasses<const Value *> EqCacheValue;
@@ -879,18 +893,20 @@ static void GroupByComplexity(SmallVecto
   // complexity.  Note that this is, at worst, N^2, but the vector is likely to
   // be extremely short in practice.  Note that we take this approach because we
   // do not want to depend on the addresses of the objects we are grouping.
-  for (unsigned i = 0, e = Ops.size(); i != e-2; ++i) {
+  for (unsigned i = 0, e = Ops.size(); i != e - 2; ++i) {
     const SCEV *S = Ops[i];
     unsigned Complexity = S->getSCEVType();
 
     // If there are any objects of the same complexity and same value as this
     // one, group them.
-    for (unsigned j = i+1; j != e && Ops[j]->getSCEVType() == Complexity; ++j) {
+    for (unsigned j = i + 1; j != e && Ops[j]->getSCEVType() == Complexity;
+         ++j) {
       if (Ops[j] == S) { // Found a duplicate.
         // Move it to immediately after i'th element.
-        std::swap(Ops[i+1], Ops[j]);
-        ++i;   // no need to rescan it.
-        if (i == e-2) return;  // Done!
+        std::swap(Ops[i + 1], Ops[j]);
+        ++i; // no need to rescan it.
+        if (i == e - 2)
+          return; // Done!
       }
     }
   }
@@ -910,8 +926,7 @@ static bool hasHugeExpression(ArrayRef<c
 
 /// Compute BC(It, K).  The result has width W.  Assume, K > 0.
 static const SCEV *BinomialCoefficient(const SCEV *It, unsigned K,
-                                       ScalarEvolution &SE,
-                                       Type *ResultTy) {
+                                       ScalarEvolution &SE, Type *ResultTy) {
   // Handle the simplest case efficiently.
   if (K == 1)
     return SE.getTruncateOrZeroExtend(It, ResultTy);
@@ -995,19 +1010,19 @@ static const SCEV *BinomialCoefficient(c
   // Calculate the multiplicative inverse of K! / 2^T;
   // this multiplication factor will perform the exact division by
   // K! / 2^T.
-  APInt Mod = APInt::getSignedMinValue(W+1);
-  APInt MultiplyFactor = OddFactorial.zext(W+1);
+  APInt Mod = APInt::getSignedMinValue(W + 1);
+  APInt MultiplyFactor = OddFactorial.zext(W + 1);
   MultiplyFactor = MultiplyFactor.multiplicativeInverse(Mod);
   MultiplyFactor = MultiplyFactor.trunc(W);
 
   // Calculate the product, at width T+W
-  IntegerType *CalculationTy = IntegerType::get(SE.getContext(),
-                                                      CalculationBits);
+  IntegerType *CalculationTy =
+      IntegerType::get(SE.getContext(), CalculationBits);
   const SCEV *Dividend = SE.getTruncateOrZeroExtend(It, CalculationTy);
   for (unsigned i = 1; i != K; ++i) {
     const SCEV *S = SE.getMinusSCEV(It, SE.getConstant(It->getType(), i));
-    Dividend = SE.getMulExpr(Dividend,
-                             SE.getTruncateOrZeroExtend(S, CalculationTy));
+    Dividend =
+        SE.getMulExpr(Dividend, SE.getTruncateOrZeroExtend(S, CalculationTy));
   }
 
   // Divide by 2^T
@@ -1032,9 +1047,9 @@ const SCEV *SCEVAddRecExpr::evaluateAtIt
   return evaluateAtIteration(makeArrayRef(op_begin(), op_end()), It, SE);
 }
 
-const SCEV *
-SCEVAddRecExpr::evaluateAtIteration(ArrayRef<const SCEV *> Operands,
-                                    const SCEV *It, ScalarEvolution &SE) {
+const SCEV *SCEVAddRecExpr::evaluateAtIteration(ArrayRef<const SCEV *> Operands,
+                                                const SCEV *It,
+                                                ScalarEvolution &SE) {
   assert(Operands.size() > 0);
   const SCEV *Result = Operands[0];
   for (unsigned i = 1, e = Operands.size(); i != e; ++i) {
@@ -1192,8 +1207,7 @@ const SCEV *ScalarEvolution::getTruncate
                                              unsigned Depth) {
   assert(getTypeSizeInBits(Op->getType()) > getTypeSizeInBits(Ty) &&
          "This is not a truncating conversion!");
-  assert(isSCEVable(Ty) &&
-         "This is not a conversion to a SCEVable type!");
+  assert(isSCEVable(Ty) && "This is not a conversion to a SCEVable type!");
   assert(!Op->getType()->isPointerTy() && "Can't truncate pointer!");
   Ty = getEffectiveSCEVType(Ty);
 
@@ -1202,12 +1216,13 @@ const SCEV *ScalarEvolution::getTruncate
   ID.AddPointer(Op);
   ID.AddPointer(Ty);
   void *IP = nullptr;
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
 
   // Fold if the operand is constant.
   if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))
     return getConstant(
-      cast<ConstantInt>(ConstantExpr::getTrunc(SC->getValue(), Ty)));
+        cast<ConstantInt>(ConstantExpr::getTrunc(SC->getValue(), Ty)));
 
   // trunc(trunc(x)) --> trunc(x)
   if (const SCEVTruncateExpr *ST = dyn_cast<SCEVTruncateExpr>(Op))
@@ -1276,8 +1291,8 @@ const SCEV *ScalarEvolution::getTruncate
   // The cast wasn't folded; create an explicit cast node. We can reuse
   // the existing insert position since if we get here, we won't have
   // made any changes which would invalidate it.
-  SCEV *S = new (SCEVAllocator) SCEVTruncateExpr(ID.Intern(SCEVAllocator),
-                                                 Op, Ty);
+  SCEV *S =
+      new (SCEVAllocator) SCEVTruncateExpr(ID.Intern(SCEVAllocator), Op, Ty);
   UniqueSCEVs.InsertNode(S, IP);
   registerUser(S, Op);
   return S;
@@ -1349,8 +1364,9 @@ struct ExtendOpTraits<SCEVSignExtendExpr
   }
 };
 
-const ExtendOpTraitsBase::GetExtendExprTy ExtendOpTraits<
-    SCEVSignExtendExpr>::GetExtendExpr = &ScalarEvolution::getSignExtendExpr;
+const ExtendOpTraitsBase::GetExtendExprTy
+    ExtendOpTraits<SCEVSignExtendExpr>::GetExtendExpr =
+        &ScalarEvolution::getSignExtendExpr;
 
 template <>
 struct ExtendOpTraits<SCEVZeroExtendExpr> : public ExtendOpTraitsBase {
@@ -1365,8 +1381,9 @@ struct ExtendOpTraits<SCEVZeroExtendExpr
   }
 };
 
-const ExtendOpTraitsBase::GetExtendExprTy ExtendOpTraits<
-    SCEVZeroExtendExpr>::GetExtendExpr = &ScalarEvolution::getZeroExtendExpr;
+const ExtendOpTraitsBase::GetExtendExprTy
+    ExtendOpTraits<SCEVZeroExtendExpr>::GetExtendExpr =
+        &ScalarEvolution::getZeroExtendExpr;
 
 } // end anonymous namespace
 
@@ -1408,7 +1425,7 @@ static const SCEV *getPreStartForExtend(
 
   // 1. NSW/NUW flags on the step increment.
   auto PreStartFlags =
-    ScalarEvolution::maskFlags(SA->getNoWrapFlags(), SCEV::FlagNUW);
+      ScalarEvolution::maskFlags(SA->getNoWrapFlags(), SCEV::FlagNUW);
   const SCEV *PreStart = SE->getAddExpr(DiffOps, PreStartFlags);
   const SCEVAddRecExpr *PreAR = dyn_cast<SCEVAddRecExpr>(
       SE->getAddRecExpr(PreStart, Step, L, SCEV::FlagAnyWrap));
@@ -1453,17 +1470,16 @@ static const SCEV *getPreStartForExtend(
 // Get the normalized zero or sign extended expression for this AddRec's Start.
 template <typename ExtendOpTy>
 static const SCEV *getExtendAddRecStart(const SCEVAddRecExpr *AR, Type *Ty,
-                                        ScalarEvolution *SE,
-                                        unsigned Depth) {
+                                        ScalarEvolution *SE, unsigned Depth) {
   auto GetExtendExpr = ExtendOpTraits<ExtendOpTy>::GetExtendExpr;
 
   const SCEV *PreStart = getPreStartForExtend<ExtendOpTy>(AR, Ty, SE, Depth);
   if (!PreStart)
     return (SE->*GetExtendExpr)(AR->getStart(), Ty, Depth);
 
-  return SE->getAddExpr((SE->*GetExtendExpr)(AR->getStepRecurrence(*SE), Ty,
-                                             Depth),
-                        (SE->*GetExtendExpr)(PreStart, Ty, Depth));
+  return SE->getAddExpr(
+      (SE->*GetExtendExpr)(AR->getStepRecurrence(*SE), Ty, Depth),
+      (SE->*GetExtendExpr)(PreStart, Ty, Depth));
 }
 
 // Try to prove away overflow by looking at "nearby" add recurrences.  A
@@ -1524,16 +1540,16 @@ bool ScalarEvolution::proveNoWrapByVaryi
     ID.AddPointer(L);
     void *IP = nullptr;
     const auto *PreAR =
-      static_cast<SCEVAddRecExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));
+        static_cast<SCEVAddRecExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));
 
     // Give up if we don't already have the add recurrence we need because
     // actually constructing an add recurrence is relatively expensive.
-    if (PreAR && PreAR->getNoWrapFlags(WrapType)) {  // proves (2)
+    if (PreAR && PreAR->getNoWrapFlags(WrapType)) { // proves (2)
       const SCEV *DeltaS = getConstant(StartC->getType(), Delta);
       ICmpInst::Predicate Pred = ICmpInst::BAD_ICMP_PREDICATE;
       const SCEV *Limit = ExtendOpTraits<ExtendOpTy>::getOverflowLimitForStep(
           DeltaS, &Pred, this);
-      if (Limit && isKnownPredicate(Pred, PreAR, Limit))  // proves (1)
+      if (Limit && isKnownPredicate(Pred, PreAR, Limit)) // proves (1)
         return true;
     }
   }
@@ -1578,19 +1594,18 @@ static APInt extractConstantWithoutWrapp
   return APInt(BitWidth, 0);
 }
 
-const SCEV *
-ScalarEvolution::getZeroExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth) {
+const SCEV *ScalarEvolution::getZeroExtendExpr(const SCEV *Op, Type *Ty,
+                                               unsigned Depth) {
   assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&
          "This is not an extending conversion!");
-  assert(isSCEVable(Ty) &&
-         "This is not a conversion to a SCEVable type!");
+  assert(isSCEVable(Ty) && "This is not a conversion to a SCEVable type!");
   assert(!Op->getType()->isPointerTy() && "Can't extend pointer!");
   Ty = getEffectiveSCEVType(Ty);
 
   // Fold if the operand is constant.
   if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))
     return getConstant(
-      cast<ConstantInt>(ConstantExpr::getZExt(SC->getValue(), Ty)));
+        cast<ConstantInt>(ConstantExpr::getZExt(SC->getValue(), Ty)));
 
   // zext(zext(x)) --> zext(x)
   if (const SCEVZeroExtendExpr *SZ = dyn_cast<SCEVZeroExtendExpr>(Op))
@@ -1603,10 +1618,11 @@ ScalarEvolution::getZeroExtendExpr(const
   ID.AddPointer(Op);
   ID.AddPointer(Ty);
   void *IP = nullptr;
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
   if (Depth > MaxCastDepth) {
-    SCEV *S = new (SCEVAllocator) SCEVZeroExtendExpr(ID.Intern(SCEVAllocator),
-                                                     Op, Ty);
+    SCEV *S = new (SCEVAllocator)
+        SCEVZeroExtendExpr(ID.Intern(SCEVAllocator), Op, Ty);
     UniqueSCEVs.InsertNode(S, IP);
     registerUser(S, Op);
     return S;
@@ -1669,49 +1685,46 @@ ScalarEvolution::getZeroExtendExpr(const
         if (MaxBECount == RecastedMaxBECount) {
           Type *WideTy = IntegerType::get(getContext(), BitWidth * 2);
           // Check whether Start+Step*MaxBECount has no unsigned overflow.
-          const SCEV *ZMul = getMulExpr(CastedMaxBECount, Step,
-                                        SCEV::FlagAnyWrap, Depth + 1);
-          const SCEV *ZAdd = getZeroExtendExpr(getAddExpr(Start, ZMul,
-                                                          SCEV::FlagAnyWrap,
-                                                          Depth + 1),
-                                               WideTy, Depth + 1);
+          const SCEV *ZMul =
+              getMulExpr(CastedMaxBECount, Step, SCEV::FlagAnyWrap, Depth + 1);
+          const SCEV *ZAdd = getZeroExtendExpr(
+              getAddExpr(Start, ZMul, SCEV::FlagAnyWrap, Depth + 1), WideTy,
+              Depth + 1);
           const SCEV *WideStart = getZeroExtendExpr(Start, WideTy, Depth + 1);
           const SCEV *WideMaxBECount =
-            getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);
+              getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);
           const SCEV *OperandExtendedAdd =
-            getAddExpr(WideStart,
-                       getMulExpr(WideMaxBECount,
-                                  getZeroExtendExpr(Step, WideTy, Depth + 1),
-                                  SCEV::FlagAnyWrap, Depth + 1),
-                       SCEV::FlagAnyWrap, Depth + 1);
+              getAddExpr(WideStart,
+                         getMulExpr(WideMaxBECount,
+                                    getZeroExtendExpr(Step, WideTy, Depth + 1),
+                                    SCEV::FlagAnyWrap, Depth + 1),
+                         SCEV::FlagAnyWrap, Depth + 1);
           if (ZAdd == OperandExtendedAdd) {
             // Cache knowledge of AR NUW, which is propagated to this AddRec.
             setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNUW);
             // Return the expression with the addrec on the outside.
-            return getAddRecExpr(
-                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getZeroExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+            return getAddRecExpr(getExtendAddRecStart<SCEVZeroExtendExpr>(
+                                     AR, Ty, this, Depth + 1),
+                                 getZeroExtendExpr(Step, Ty, Depth + 1), L,
+                                 AR->getNoWrapFlags());
           }
           // Similar to above, only this time treat the step value as signed.
           // This covers loops that count down.
           OperandExtendedAdd =
-            getAddExpr(WideStart,
-                       getMulExpr(WideMaxBECount,
-                                  getSignExtendExpr(Step, WideTy, Depth + 1),
-                                  SCEV::FlagAnyWrap, Depth + 1),
-                       SCEV::FlagAnyWrap, Depth + 1);
+              getAddExpr(WideStart,
+                         getMulExpr(WideMaxBECount,
+                                    getSignExtendExpr(Step, WideTy, Depth + 1),
+                                    SCEV::FlagAnyWrap, Depth + 1),
+                         SCEV::FlagAnyWrap, Depth + 1);
           if (ZAdd == OperandExtendedAdd) {
             // Cache knowledge of AR NW, which is propagated to this AddRec.
             // Negative step causes unsigned wrap, but it still can't self-wrap.
             setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);
             // Return the expression with the addrec on the outside.
-            return getAddRecExpr(
-                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getSignExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+            return getAddRecExpr(getExtendAddRecStart<SCEVZeroExtendExpr>(
+                                     AR, Ty, this, Depth + 1),
+                                 getSignExtendExpr(Step, Ty, Depth + 1), L,
+                                 AR->getNoWrapFlags());
           }
         }
       }
@@ -1734,14 +1747,12 @@ ScalarEvolution::getZeroExtendExpr(const
           // it's one of two issue possible causes for a change which was
           // reverted.  Be conservative for the moment.
           return getAddRecExpr(
-                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getZeroExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+              getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this, Depth + 1),
+              getZeroExtendExpr(Step, Ty, Depth + 1), L, AR->getNoWrapFlags());
         }
-        
+
         // For a negative step, we can extend the operands iff doing so only
-        // traverses values in the range zext([0,UINT_MAX]). 
+        // traverses values in the range zext([0,UINT_MAX]).
         if (isKnownNegative(Step)) {
           const SCEV *N = getConstant(APInt::getMaxValue(BitWidth) -
                                       getSignedRangeMin(Step));
@@ -1752,11 +1763,10 @@ ScalarEvolution::getZeroExtendExpr(const
             // still can't self-wrap.
             setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);
             // Return the expression with the addrec on the outside.
-            return getAddRecExpr(
-                getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getSignExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+            return getAddRecExpr(getExtendAddRecStart<SCEVZeroExtendExpr>(
+                                     AR, Ty, this, Depth + 1),
+                                 getSignExtendExpr(Step, Ty, Depth + 1), L,
+                                 AR->getNoWrapFlags());
           }
         }
       }
@@ -1873,27 +1883,27 @@ ScalarEvolution::getZeroExtendExpr(const
 
   // The cast wasn't folded; create an explicit cast node.
   // Recompute the insert position, as it may have been invalidated.
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
-  SCEV *S = new (SCEVAllocator) SCEVZeroExtendExpr(ID.Intern(SCEVAllocator),
-                                                   Op, Ty);
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
+  SCEV *S =
+      new (SCEVAllocator) SCEVZeroExtendExpr(ID.Intern(SCEVAllocator), Op, Ty);
   UniqueSCEVs.InsertNode(S, IP);
   registerUser(S, Op);
   return S;
 }
 
-const SCEV *
-ScalarEvolution::getSignExtendExpr(const SCEV *Op, Type *Ty, unsigned Depth) {
+const SCEV *ScalarEvolution::getSignExtendExpr(const SCEV *Op, Type *Ty,
+                                               unsigned Depth) {
   assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&
          "This is not an extending conversion!");
-  assert(isSCEVable(Ty) &&
-         "This is not a conversion to a SCEVable type!");
+  assert(isSCEVable(Ty) && "This is not a conversion to a SCEVable type!");
   assert(!Op->getType()->isPointerTy() && "Can't extend pointer!");
   Ty = getEffectiveSCEVType(Ty);
 
   // Fold if the operand is constant.
   if (const SCEVConstant *SC = dyn_cast<SCEVConstant>(Op))
     return getConstant(
-      cast<ConstantInt>(ConstantExpr::getSExt(SC->getValue(), Ty)));
+        cast<ConstantInt>(ConstantExpr::getSExt(SC->getValue(), Ty)));
 
   // sext(sext(x)) --> sext(x)
   if (const SCEVSignExtendExpr *SS = dyn_cast<SCEVSignExtendExpr>(Op))
@@ -1910,11 +1920,12 @@ ScalarEvolution::getSignExtendExpr(const
   ID.AddPointer(Op);
   ID.AddPointer(Ty);
   void *IP = nullptr;
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
   // Limit recursion depth.
   if (Depth > MaxCastDepth) {
-    SCEV *S = new (SCEVAllocator) SCEVSignExtendExpr(ID.Intern(SCEVAllocator),
-                                                     Op, Ty);
+    SCEV *S = new (SCEVAllocator)
+        SCEVSignExtendExpr(ID.Intern(SCEVAllocator), Op, Ty);
     UniqueSCEVs.InsertNode(S, IP);
     registerUser(S, Op);
     return S;
@@ -2011,39 +2022,37 @@ ScalarEvolution::getSignExtendExpr(const
         if (MaxBECount == RecastedMaxBECount) {
           Type *WideTy = IntegerType::get(getContext(), BitWidth * 2);
           // Check whether Start+Step*MaxBECount has no signed overflow.
-          const SCEV *SMul = getMulExpr(CastedMaxBECount, Step,
-                                        SCEV::FlagAnyWrap, Depth + 1);
-          const SCEV *SAdd = getSignExtendExpr(getAddExpr(Start, SMul,
-                                                          SCEV::FlagAnyWrap,
-                                                          Depth + 1),
-                                               WideTy, Depth + 1);
+          const SCEV *SMul =
+              getMulExpr(CastedMaxBECount, Step, SCEV::FlagAnyWrap, Depth + 1);
+          const SCEV *SAdd = getSignExtendExpr(
+              getAddExpr(Start, SMul, SCEV::FlagAnyWrap, Depth + 1), WideTy,
+              Depth + 1);
           const SCEV *WideStart = getSignExtendExpr(Start, WideTy, Depth + 1);
           const SCEV *WideMaxBECount =
-            getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);
+              getZeroExtendExpr(CastedMaxBECount, WideTy, Depth + 1);
           const SCEV *OperandExtendedAdd =
-            getAddExpr(WideStart,
-                       getMulExpr(WideMaxBECount,
-                                  getSignExtendExpr(Step, WideTy, Depth + 1),
-                                  SCEV::FlagAnyWrap, Depth + 1),
-                       SCEV::FlagAnyWrap, Depth + 1);
+              getAddExpr(WideStart,
+                         getMulExpr(WideMaxBECount,
+                                    getSignExtendExpr(Step, WideTy, Depth + 1),
+                                    SCEV::FlagAnyWrap, Depth + 1),
+                         SCEV::FlagAnyWrap, Depth + 1);
           if (SAdd == OperandExtendedAdd) {
             // Cache knowledge of AR NSW, which is propagated to this AddRec.
             setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNSW);
             // Return the expression with the addrec on the outside.
-            return getAddRecExpr(
-                getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getSignExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+            return getAddRecExpr(getExtendAddRecStart<SCEVSignExtendExpr>(
+                                     AR, Ty, this, Depth + 1),
+                                 getSignExtendExpr(Step, Ty, Depth + 1), L,
+                                 AR->getNoWrapFlags());
           }
           // Similar to above, only this time treat the step value as unsigned.
           // This covers loops that count up with an unsigned step.
           OperandExtendedAdd =
-            getAddExpr(WideStart,
-                       getMulExpr(WideMaxBECount,
-                                  getZeroExtendExpr(Step, WideTy, Depth + 1),
-                                  SCEV::FlagAnyWrap, Depth + 1),
-                       SCEV::FlagAnyWrap, Depth + 1);
+              getAddExpr(WideStart,
+                         getMulExpr(WideMaxBECount,
+                                    getZeroExtendExpr(Step, WideTy, Depth + 1),
+                                    SCEV::FlagAnyWrap, Depth + 1),
+                         SCEV::FlagAnyWrap, Depth + 1);
           if (SAdd == OperandExtendedAdd) {
             // If AR wraps around then
             //
@@ -2056,11 +2065,10 @@ ScalarEvolution::getSignExtendExpr(const
             setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), SCEV::FlagNW);
 
             // Return the expression with the addrec on the outside.
-            return getAddRecExpr(
-                getExtendAddRecStart<SCEVSignExtendExpr>(AR, Ty, this,
-                                                         Depth + 1),
-                getZeroExtendExpr(Step, Ty, Depth + 1), L,
-                AR->getNoWrapFlags());
+            return getAddRecExpr(getExtendAddRecStart<SCEVSignExtendExpr>(
+                                     AR, Ty, this, Depth + 1),
+                                 getZeroExtendExpr(Step, Ty, Depth + 1), L,
+                                 AR->getNoWrapFlags());
           }
         }
       }
@@ -2109,11 +2117,12 @@ ScalarEvolution::getSignExtendExpr(const
 
   // The cast wasn't folded; create an explicit cast node.
   // Recompute the insert position, as it may have been invalidated.
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
-  SCEV *S = new (SCEVAllocator) SCEVSignExtendExpr(ID.Intern(SCEVAllocator),
-                                                   Op, Ty);
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
+  SCEV *S =
+      new (SCEVAllocator) SCEVSignExtendExpr(ID.Intern(SCEVAllocator), Op, Ty);
   UniqueSCEVs.InsertNode(S, IP);
-  registerUser(S, { Op });
+  registerUser(S, {Op});
   return S;
 }
 
@@ -2135,12 +2144,10 @@ const SCEV *ScalarEvolution::getCastExpr
 
 /// getAnyExtendExpr - Return a SCEV for the given operand extended with
 /// unspecified bits out to the given type.
-const SCEV *ScalarEvolution::getAnyExtendExpr(const SCEV *Op,
-                                              Type *Ty) {
+const SCEV *ScalarEvolution::getAnyExtendExpr(const SCEV *Op, Type *Ty) {
   assert(getTypeSizeInBits(Op->getType()) < getTypeSizeInBits(Ty) &&
          "This is not an extending conversion!");
-  assert(isSCEVable(Ty) &&
-         "This is not a conversion to a SCEVable type!");
+  assert(isSCEVable(Ty) && "This is not a conversion to a SCEVable type!");
   Ty = getEffectiveSCEVType(Ty);
 
   // Sign-extend negative constants.
@@ -2205,13 +2212,12 @@ const SCEV *ScalarEvolution::getAnyExten
 /// may be exposed. This helps getAddRecExpr short-circuit extra work in
 /// the common case where no interesting opportunities are present, and
 /// is also used as a check to avoid infinite recursion.
-static bool
-CollectAddOperandsWithScales(DenseMap<const SCEV *, APInt> &M,
-                             SmallVectorImpl<const SCEV *> &NewOps,
-                             APInt &AccumulatedConstant,
-                             const SCEV *const *Ops, size_t NumOperands,
-                             const APInt &Scale,
-                             ScalarEvolution &SE) {
+static bool CollectAddOperandsWithScales(DenseMap<const SCEV *, APInt> &M,
+                                         SmallVectorImpl<const SCEV *> &NewOps,
+                                         APInt &AccumulatedConstant,
+                                         const SCEV *const *Ops,
+                                         size_t NumOperands, const APInt &Scale,
+                                         ScalarEvolution &SE) {
   bool Interesting = false;
 
   // Iterate over the add operands. They are sorted, with constants first.
@@ -2234,10 +2240,9 @@ CollectAddOperandsWithScales(DenseMap<co
       if (Mul->getNumOperands() == 2 && isa<SCEVAddExpr>(Mul->getOperand(1))) {
         // A multiplication of a constant with another add; recurse.
         const SCEVAddExpr *Add = cast<SCEVAddExpr>(Mul->getOperand(1));
-        Interesting |=
-          CollectAddOperandsWithScales(M, NewOps, AccumulatedConstant,
-                                       Add->op_begin(), Add->getNumOperands(),
-                                       NewScale, SE);
+        Interesting |= CollectAddOperandsWithScales(
+            M, NewOps, AccumulatedConstant, Add->op_begin(),
+            Add->getNumOperands(), NewScale, SE);
       } else {
         // A multiplication of a constant with some other value. Update
         // the map.
@@ -2349,10 +2354,10 @@ ScalarEvolution::getStrengthenedNoWrapFl
 // We're trying to construct a SCEV of type `Type' with `Ops' as operands and
 // `OldFlags' as can't-wrap behavior.  Infer a more aggressive set of
 // can't-overflow flags for the operation if possible.
-static SCEV::NoWrapFlags
-StrengthenNoWrapFlags(ScalarEvolution *SE, SCEVTypes Type,
-                      const ArrayRef<const SCEV *> Ops,
-                      SCEV::NoWrapFlags Flags) {
+static SCEV::NoWrapFlags StrengthenNoWrapFlags(ScalarEvolution *SE,
+                                               SCEVTypes Type,
+                                               const ArrayRef<const SCEV *> Ops,
+                                               SCEV::NoWrapFlags Flags) {
   using namespace std::placeholders;
 
   using OBO = OverflowingBinaryOperator;
@@ -2443,7 +2448,8 @@ const SCEV *ScalarEvolution::getAddExpr(
   assert(!(OrigFlags & ~(SCEV::FlagNUW | SCEV::FlagNSW)) &&
          "only nuw or nsw allowed");
   assert(!Ops.empty() && "Cannot get empty add!");
-  if (Ops.size() == 1) return Ops[0];
+  if (Ops.size() == 1)
+    return Ops[0];
 #ifndef NDEBUG
   Type *ETy = getEffectiveSCEVType(Ops[0]->getType());
   for (unsigned i = 1, e = Ops.size(); i != e; ++i)
@@ -2465,8 +2471,9 @@ const SCEV *ScalarEvolution::getAddExpr(
     while (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(Ops[Idx])) {
       // We found two constants, fold them together!
       Ops[0] = getConstant(LHSC->getAPInt() + RHSC->getAPInt());
-      if (Ops.size() == 2) return Ops[0];
-      Ops.erase(Ops.begin()+1);  // Erase the folded element
+      if (Ops.size() == 2)
+        return Ops[0];
+      Ops.erase(Ops.begin() + 1); // Erase the folded element
       LHSC = cast<SCEVConstant>(Ops[0]);
     }
 
@@ -2476,7 +2483,8 @@ const SCEV *ScalarEvolution::getAddExpr(
       --Idx;
     }
 
-    if (Ops.size() == 1) return Ops[0];
+    if (Ops.size() == 1)
+      return Ops[0];
   }
 
   // Delay expensive flag strengthening until necessary.
@@ -2501,11 +2509,11 @@ const SCEV *ScalarEvolution::getAddExpr(
   // sorted the list, these values are required to be adjacent.
   Type *Ty = Ops[0]->getType();
   bool FoundMatch = false;
-  for (unsigned i = 0, e = Ops.size(); i != e-1; ++i)
-    if (Ops[i] == Ops[i+1]) {      //  X + Y + Y  -->  X + Y*2
+  for (unsigned i = 0, e = Ops.size(); i != e - 1; ++i)
+    if (Ops[i] == Ops[i + 1]) { //  X + Y + Y  -->  X + Y*2
       // Scan ahead to count how many equal operands there are.
       unsigned Count = 2;
-      while (i+Count != e && Ops[i+Count] == Ops[i])
+      while (i + Count != e && Ops[i + Count] == Ops[i])
         ++Count;
       // Merge the values into a multiply.
       const SCEV *Scale = getConstant(Ty, Count);
@@ -2513,8 +2521,9 @@ const SCEV *ScalarEvolution::getAddExpr(
       if (Ops.size() == Count)
         return Mul;
       Ops[i] = Mul;
-      Ops.erase(Ops.begin()+i+1, Ops.begin()+i+Count);
-      --i; e -= Count - 1;
+      Ops.erase(Ops.begin() + i + 1, Ops.begin() + i + Count);
+      --i;
+      e -= Count - 1;
       FoundMatch = true;
     }
   if (FoundMatch)
@@ -2556,7 +2565,7 @@ const SCEV *ScalarEvolution::getAddExpr(
         SmallVector<const SCEV *, 8> LargeMulOps;
         for (unsigned j = 0, f = M->getNumOperands(); j != f && Ok; ++j) {
           if (const SCEVTruncateExpr *T =
-                dyn_cast<SCEVTruncateExpr>(M->getOperand(j))) {
+                  dyn_cast<SCEVTruncateExpr>(M->getOperand(j))) {
             if (T->getOperand()->getType() != SrcType) {
               Ok = false;
               break;
@@ -2570,7 +2579,8 @@ const SCEV *ScalarEvolution::getAddExpr(
           }
         }
         if (Ok)
-          LargeOps.push_back(getMulExpr(LargeMulOps, SCEV::FlagAnyWrap, Depth + 1));
+          LargeOps.push_back(
+              getMulExpr(LargeMulOps, SCEV::FlagAnyWrap, Depth + 1));
       } else {
         Ok = false;
         break;
@@ -2654,7 +2664,7 @@ const SCEV *ScalarEvolution::getAddExpr(
         break;
       // If we have an add, expand the add operands onto the end of the operands
       // list.
-      Ops.erase(Ops.begin()+Idx);
+      Ops.erase(Ops.begin() + Idx);
       Ops.append(Add->op_begin(), Add->op_end());
       DeletedAdd = true;
       CommonFlags = maskFlags(CommonFlags, Add->getNoWrapFlags());
@@ -2678,9 +2688,8 @@ const SCEV *ScalarEvolution::getAddExpr(
     DenseMap<const SCEV *, APInt> M;
     SmallVector<const SCEV *, 8> NewOps;
     APInt AccumulatedConstant(BitWidth, 0);
-    if (CollectAddOperandsWithScales(M, NewOps, AccumulatedConstant,
-                                     Ops.data(), Ops.size(),
-                                     APInt(BitWidth, 1), *this)) {
+    if (CollectAddOperandsWithScales(M, NewOps, AccumulatedConstant, Ops.data(),
+                                     Ops.size(), APInt(BitWidth, 1), *this)) {
       struct APIntCompare {
         bool operator()(const APInt &LHS, const APInt &RHS) const {
           return LHS.ult(RHS);
@@ -2701,10 +2710,10 @@ const SCEV *ScalarEvolution::getAddExpr(
         if (MulOp.first == 1) {
           Ops.push_back(getAddExpr(MulOp.second, SCEV::FlagAnyWrap, Depth + 1));
         } else if (MulOp.first != 0) {
-          Ops.push_back(getMulExpr(
-              getConstant(MulOp.first),
-              getAddExpr(MulOp.second, SCEV::FlagAnyWrap, Depth + 1),
-              SCEV::FlagAnyWrap, Depth + 1));
+          Ops.push_back(
+              getMulExpr(getConstant(MulOp.first),
+                         getAddExpr(MulOp.second, SCEV::FlagAnyWrap, Depth + 1),
+                         SCEV::FlagAnyWrap, Depth + 1));
         }
       }
       if (Ops.empty())
@@ -2732,49 +2741,51 @@ const SCEV *ScalarEvolution::getAddExpr(
             // If the multiply has more than two operands, we must get the
             // Y*Z term.
             SmallVector<const SCEV *, 4> MulOps(Mul->op_begin(),
-                                                Mul->op_begin()+MulOp);
-            MulOps.append(Mul->op_begin()+MulOp+1, Mul->op_end());
+                                                Mul->op_begin() + MulOp);
+            MulOps.append(Mul->op_begin() + MulOp + 1, Mul->op_end());
             InnerMul = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);
           }
           SmallVector<const SCEV *, 2> TwoOps = {getOne(Ty), InnerMul};
           const SCEV *AddOne = getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);
-          const SCEV *OuterMul = getMulExpr(AddOne, MulOpSCEV,
-                                            SCEV::FlagAnyWrap, Depth + 1);
-          if (Ops.size() == 2) return OuterMul;
+          const SCEV *OuterMul =
+              getMulExpr(AddOne, MulOpSCEV, SCEV::FlagAnyWrap, Depth + 1);
+          if (Ops.size() == 2)
+            return OuterMul;
           if (AddOp < Idx) {
-            Ops.erase(Ops.begin()+AddOp);
-            Ops.erase(Ops.begin()+Idx-1);
+            Ops.erase(Ops.begin() + AddOp);
+            Ops.erase(Ops.begin() + Idx - 1);
           } else {
-            Ops.erase(Ops.begin()+Idx);
-            Ops.erase(Ops.begin()+AddOp-1);
+            Ops.erase(Ops.begin() + Idx);
+            Ops.erase(Ops.begin() + AddOp - 1);
           }
           Ops.push_back(OuterMul);
           return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);
         }
 
       // Check this multiply against other multiplies being added together.
-      for (unsigned OtherMulIdx = Idx+1;
+      for (unsigned OtherMulIdx = Idx + 1;
            OtherMulIdx < Ops.size() && isa<SCEVMulExpr>(Ops[OtherMulIdx]);
            ++OtherMulIdx) {
         const SCEVMulExpr *OtherMul = cast<SCEVMulExpr>(Ops[OtherMulIdx]);
         // If MulOp occurs in OtherMul, we can fold the two multiplies
         // together.
-        for (unsigned OMulOp = 0, e = OtherMul->getNumOperands();
-             OMulOp != e; ++OMulOp)
+        for (unsigned OMulOp = 0, e = OtherMul->getNumOperands(); OMulOp != e;
+             ++OMulOp)
           if (OtherMul->getOperand(OMulOp) == MulOpSCEV) {
             // Fold X + (A*B*C) + (A*D*E) --> X + (A*(B*C+D*E))
             const SCEV *InnerMul1 = Mul->getOperand(MulOp == 0);
             if (Mul->getNumOperands() != 2) {
               SmallVector<const SCEV *, 4> MulOps(Mul->op_begin(),
-                                                  Mul->op_begin()+MulOp);
-              MulOps.append(Mul->op_begin()+MulOp+1, Mul->op_end());
+                                                  Mul->op_begin() + MulOp);
+              MulOps.append(Mul->op_begin() + MulOp + 1, Mul->op_end());
               InnerMul1 = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);
             }
             const SCEV *InnerMul2 = OtherMul->getOperand(OMulOp == 0);
             if (OtherMul->getNumOperands() != 2) {
-              SmallVector<const SCEV *, 4> MulOps(OtherMul->op_begin(),
-                                                  OtherMul->op_begin()+OMulOp);
-              MulOps.append(OtherMul->op_begin()+OMulOp+1, OtherMul->op_end());
+              SmallVector<const SCEV *, 4> MulOps(
+                  OtherMul->op_begin(), OtherMul->op_begin() + OMulOp);
+              MulOps.append(OtherMul->op_begin() + OMulOp + 1,
+                            OtherMul->op_end());
               InnerMul2 = getMulExpr(MulOps, SCEV::FlagAnyWrap, Depth + 1);
             }
             SmallVector<const SCEV *, 2> TwoOps = {InnerMul1, InnerMul2};
@@ -2782,9 +2793,10 @@ const SCEV *ScalarEvolution::getAddExpr(
                 getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);
             const SCEV *OuterMul = getMulExpr(MulOpSCEV, InnerMulSum,
                                               SCEV::FlagAnyWrap, Depth + 1);
-            if (Ops.size() == 2) return OuterMul;
-            Ops.erase(Ops.begin()+Idx);
-            Ops.erase(Ops.begin()+OtherMulIdx-1);
+            if (Ops.size() == 2)
+              return OuterMul;
+            Ops.erase(Ops.begin() + Idx);
+            Ops.erase(Ops.begin() + OtherMulIdx - 1);
             Ops.push_back(OuterMul);
             return getAddExpr(Ops, SCEV::FlagAnyWrap, Depth + 1);
           }
@@ -2808,8 +2820,9 @@ const SCEV *ScalarEvolution::getAddExpr(
     for (unsigned i = 0, e = Ops.size(); i != e; ++i)
       if (isAvailableAtLoopEntry(Ops[i], AddRecLoop)) {
         LIOps.push_back(Ops[i]);
-        Ops.erase(Ops.begin()+i);
-        --i; --e;
+        Ops.erase(Ops.begin() + i);
+        --i;
+        --e;
       }
 
     // If we found some loop invariants, fold them into the recurrence.
@@ -2852,7 +2865,8 @@ const SCEV *ScalarEvolution::getAddExpr(
       const SCEV *NewRec = getAddRecExpr(AddRecOps, AddRecLoop, Flags);
 
       // If all of the other operands were loop invariant, we are done.
-      if (Ops.size() == 1) return NewRec;
+      if (Ops.size() == 1)
+        return NewRec;
 
       // Otherwise, add the folded AddRec by the non-invariant parts.
       for (unsigned i = 0;; ++i)
@@ -2866,15 +2880,15 @@ const SCEV *ScalarEvolution::getAddExpr(
     // Okay, if there weren't any loop invariants to be folded, check to see if
     // there are multiple AddRec's with the same loop induction variable being
     // added together.  If so, we can fold them.
-    for (unsigned OtherIdx = Idx+1;
+    for (unsigned OtherIdx = Idx + 1;
          OtherIdx < Ops.size() && isa<SCEVAddRecExpr>(Ops[OtherIdx]);
          ++OtherIdx) {
       // We expect the AddRecExpr's to be sorted in reverse dominance order,
       // so that the 1st found AddRecExpr is dominated by all others.
       assert(DT.dominates(
-           cast<SCEVAddRecExpr>(Ops[OtherIdx])->getLoop()->getHeader(),
-           AddRec->getLoop()->getHeader()) &&
-        "AddRecExprs are not sorted in reverse dominance order?");
+                 cast<SCEVAddRecExpr>(Ops[OtherIdx])->getLoop()->getHeader(),
+                 AddRec->getLoop()->getHeader()) &&
+             "AddRecExprs are not sorted in reverse dominance order?");
       if (AddRecLoop == cast<SCEVAddRecExpr>(Ops[OtherIdx])->getLoop()) {
         // Other + {A,+,B}<L> + {C,+,D}<L>  -->  Other + {A+C,+,B+D}<L>
         SmallVector<const SCEV *, 4> AddRecOps(AddRec->operands());
@@ -2882,10 +2896,10 @@ const SCEV *ScalarEvolution::getAddExpr(
              ++OtherIdx) {
           const auto *OtherAddRec = cast<SCEVAddRecExpr>(Ops[OtherIdx]);
           if (OtherAddRec->getLoop() == AddRecLoop) {
-            for (unsigned i = 0, e = OtherAddRec->getNumOperands();
-                 i != e; ++i) {
+            for (unsigned i = 0, e = OtherAddRec->getNumOperands(); i != e;
+                 ++i) {
               if (i >= AddRecOps.size()) {
-                AddRecOps.append(OtherAddRec->op_begin()+i,
+                AddRecOps.append(OtherAddRec->op_begin() + i,
                                  OtherAddRec->op_end());
                 break;
               }
@@ -2893,7 +2907,8 @@ const SCEV *ScalarEvolution::getAddExpr(
                   AddRecOps[i], OtherAddRec->getOperand(i)};
               AddRecOps[i] = getAddExpr(TwoOps, SCEV::FlagAnyWrap, Depth + 1);
             }
-            Ops.erase(Ops.begin() + OtherIdx); --OtherIdx;
+            Ops.erase(Ops.begin() + OtherIdx);
+            --OtherIdx;
           }
         }
         // Step size has changed, so we cannot guarantee no self-wraparound.
@@ -2911,9 +2926,8 @@ const SCEV *ScalarEvolution::getAddExpr(
   return getOrCreateAddExpr(Ops, ComputeFlags(Ops));
 }
 
-const SCEV *
-ScalarEvolution::getOrCreateAddExpr(ArrayRef<const SCEV *> Ops,
-                                    SCEV::NoWrapFlags Flags) {
+const SCEV *ScalarEvolution::getOrCreateAddExpr(ArrayRef<const SCEV *> Ops,
+                                                SCEV::NoWrapFlags Flags) {
   FoldingSetNodeID ID;
   ID.AddInteger(scAddExpr);
   for (const SCEV *Op : Ops)
@@ -2933,9 +2947,9 @@ ScalarEvolution::getOrCreateAddExpr(Arra
   return S;
 }
 
-const SCEV *
-ScalarEvolution::getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops,
-                                       const Loop *L, SCEV::NoWrapFlags Flags) {
+const SCEV *ScalarEvolution::getOrCreateAddRecExpr(ArrayRef<const SCEV *> Ops,
+                                                   const Loop *L,
+                                                   SCEV::NoWrapFlags Flags) {
   FoldingSetNodeID ID;
   ID.AddInteger(scAddRecExpr);
   for (const SCEV *Op : Ops)
@@ -2957,21 +2971,20 @@ ScalarEvolution::getOrCreateAddRecExpr(A
   return S;
 }
 
-const SCEV *
-ScalarEvolution::getOrCreateMulExpr(ArrayRef<const SCEV *> Ops,
-                                    SCEV::NoWrapFlags Flags) {
+const SCEV *ScalarEvolution::getOrCreateMulExpr(ArrayRef<const SCEV *> Ops,
+                                                SCEV::NoWrapFlags Flags) {
   FoldingSetNodeID ID;
   ID.AddInteger(scMulExpr);
   for (const SCEV *Op : Ops)
     ID.AddPointer(Op);
   void *IP = nullptr;
   SCEVMulExpr *S =
-    static_cast<SCEVMulExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));
+      static_cast<SCEVMulExpr *>(UniqueSCEVs.FindNodeOrInsertPos(ID, IP));
   if (!S) {
     const SCEV **O = SCEVAllocator.Allocate<const SCEV *>(Ops.size());
     std::uninitialized_copy(Ops.begin(), Ops.end(), O);
-    S = new (SCEVAllocator) SCEVMulExpr(ID.Intern(SCEVAllocator),
-                                        O, Ops.size());
+    S = new (SCEVAllocator)
+        SCEVMulExpr(ID.Intern(SCEVAllocator), O, Ops.size());
     UniqueSCEVs.InsertNode(S, IP);
     registerUser(S, Ops);
   }
@@ -2980,8 +2993,9 @@ ScalarEvolution::getOrCreateMulExpr(Arra
 }
 
 static uint64_t umul_ov(uint64_t i, uint64_t j, bool &Overflow) {
-  uint64_t k = i*j;
-  if (j > 1 && k / j != i) Overflow = true;
+  uint64_t k = i * j;
+  if (j > 1 && k / j != i)
+    Overflow = true;
   return k;
 }
 
@@ -2997,15 +3011,17 @@ static uint64_t Choose(uint64_t n, uint6
   // intermediate computations. However, we can still overflow even when the
   // final result would fit.
 
-  if (n == 0 || n == k) return 1;
-  if (k > n) return 0;
+  if (n == 0 || n == k)
+    return 1;
+  if (k > n)
+    return 0;
 
-  if (k > n/2)
-    k = n-k;
+  if (k > n / 2)
+    k = n - k;
 
   uint64_t r = 1;
   for (uint64_t i = 1; i <= k; ++i) {
-    r = umul_ov(r, n-(i-1), Overflow);
+    r = umul_ov(r, n - (i - 1), Overflow);
     r /= i;
   }
   return r;
@@ -3022,9 +3038,7 @@ static bool containsConstantInAddMulChai
       return isa<SCEVAddExpr>(S) || isa<SCEVMulExpr>(S);
     }
 
-    bool isDone() const {
-      return FoundConstant;
-    }
+    bool isDone() const { return FoundConstant; }
   };
 
   FindConstantInAddMulChain F;
@@ -3040,7 +3054,8 @@ const SCEV *ScalarEvolution::getMulExpr(
   assert(OrigFlags == maskFlags(OrigFlags, SCEV::FlagNUW | SCEV::FlagNSW) &&
          "only nuw or nsw allowed");
   assert(!Ops.empty() && "Cannot get empty mul!");
-  if (Ops.size() == 1) return Ops[0];
+  if (Ops.size() == 1)
+    return Ops[0];
 #ifndef NDEBUG
   Type *ETy = Ops[0]->getType();
   assert(!ETy->isPointerTy());
@@ -3060,8 +3075,9 @@ const SCEV *ScalarEvolution::getMulExpr(
     while (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(Ops[Idx])) {
       // We found two constants, fold them together!
       Ops[0] = getConstant(LHSC->getAPInt() * RHSC->getAPInt());
-      if (Ops.size() == 2) return Ops[0];
-      Ops.erase(Ops.begin()+1);  // Erase the folded element
+      if (Ops.size() == 2)
+        return Ops[0];
+      Ops.erase(Ops.begin() + 1); // Erase the folded element
       LHSC = cast<SCEVConstant>(Ops[0]);
     }
 
@@ -3120,9 +3136,10 @@ const SCEV *ScalarEvolution::getMulExpr(
           SmallVector<const SCEV *, 4> NewOps;
           bool AnyFolded = false;
           for (const SCEV *AddOp : Add->operands()) {
-            const SCEV *Mul = getMulExpr(Ops[0], AddOp, SCEV::FlagAnyWrap,
-                                         Depth + 1);
-            if (!isa<SCEVMulExpr>(Mul)) AnyFolded = true;
+            const SCEV *Mul =
+                getMulExpr(Ops[0], AddOp, SCEV::FlagAnyWrap, Depth + 1);
+            if (!isa<SCEVMulExpr>(Mul))
+              AnyFolded = true;
             NewOps.push_back(Mul);
           }
           if (AnyFolded)
@@ -3131,8 +3148,8 @@ const SCEV *ScalarEvolution::getMulExpr(
           // Negation preserves a recurrence's no self-wrap property.
           SmallVector<const SCEV *, 4> Operands;
           for (const SCEV *AddRecOp : AddRec->operands())
-            Operands.push_back(getMulExpr(Ops[0], AddRecOp, SCEV::FlagAnyWrap,
-                                          Depth + 1));
+            Operands.push_back(
+                getMulExpr(Ops[0], AddRecOp, SCEV::FlagAnyWrap, Depth + 1));
 
           return getAddRecExpr(Operands, AddRec->getLoop(),
                                AddRec->getNoWrapFlags(SCEV::FlagNW));
@@ -3153,7 +3170,7 @@ const SCEV *ScalarEvolution::getMulExpr(
         break;
       // If we have an mul, expand the mul operands onto the end of the
       // operands list.
-      Ops.erase(Ops.begin()+Idx);
+      Ops.erase(Ops.begin() + Idx);
       Ops.append(Mul->op_begin(), Mul->op_end());
       DeletedMul = true;
     }
@@ -3181,8 +3198,9 @@ const SCEV *ScalarEvolution::getMulExpr(
     for (unsigned i = 0, e = Ops.size(); i != e; ++i)
       if (isAvailableAtLoopEntry(Ops[i], AddRecLoop)) {
         LIOps.push_back(Ops[i]);
-        Ops.erase(Ops.begin()+i);
-        --i; --e;
+        Ops.erase(Ops.begin() + i);
+        --i;
+        --e;
       }
 
     // If we found some loop invariants, fold them into the recurrence.
@@ -3201,11 +3219,12 @@ const SCEV *ScalarEvolution::getMulExpr(
       // No self-wrap cannot be guaranteed after changing the step size, but
       // will be inferred if either NUW or NSW is true.
       SCEV::NoWrapFlags Flags = ComputeFlags({Scale, AddRec});
-      const SCEV *NewRec = getAddRecExpr(
-          NewOps, AddRecLoop, AddRec->getNoWrapFlags(Flags));
+      const SCEV *NewRec =
+          getAddRecExpr(NewOps, AddRecLoop, AddRec->getNoWrapFlags(Flags));
 
       // If all of the other operands were loop invariant, we are done.
-      if (Ops.size() == 1) return NewRec;
+      if (Ops.size() == 1)
+        return NewRec;
 
       // Otherwise, multiply the folded AddRec by the non-invariant parts.
       for (unsigned i = 0;; ++i)
@@ -3231,40 +3250,42 @@ const SCEV *ScalarEvolution::getMulExpr(
     // addrec's are of different length (mathematically, it's equivalent to
     // an infinite stream of zeros on the right).
     bool OpsModified = false;
-    for (unsigned OtherIdx = Idx+1;
+    for (unsigned OtherIdx = Idx + 1;
          OtherIdx != Ops.size() && isa<SCEVAddRecExpr>(Ops[OtherIdx]);
          ++OtherIdx) {
       const SCEVAddRecExpr *OtherAddRec =
-        dyn_cast<SCEVAddRecExpr>(Ops[OtherIdx]);
+          dyn_cast<SCEVAddRecExpr>(Ops[OtherIdx]);
       if (!OtherAddRec || OtherAddRec->getLoop() != AddRecLoop)
         continue;
 
       // Limit max number of arguments to avoid creation of unreasonably big
       // SCEVAddRecs with very complex operands.
       if (AddRec->getNumOperands() + OtherAddRec->getNumOperands() - 1 >
-          MaxAddRecSize || hasHugeExpression({AddRec, OtherAddRec}))
+              MaxAddRecSize ||
+          hasHugeExpression({AddRec, OtherAddRec}))
         continue;
 
       bool Overflow = false;
       Type *Ty = AddRec->getType();
       bool LargerThan64Bits = getTypeSizeInBits(Ty) > 64;
-      SmallVector<const SCEV*, 7> AddRecOps;
+      SmallVector<const SCEV *, 7> AddRecOps;
       for (int x = 0, xe = AddRec->getNumOperands() +
-             OtherAddRec->getNumOperands() - 1; x != xe && !Overflow; ++x) {
-        SmallVector <const SCEV *, 7> SumOps;
-        for (int y = x, ye = 2*x+1; y != ye && !Overflow; ++y) {
-          uint64_t Coeff1 = Choose(x, 2*x - y, Overflow);
-          for (int z = std::max(y-x, y-(int)AddRec->getNumOperands()+1),
-                 ze = std::min(x+1, (int)OtherAddRec->getNumOperands());
+                           OtherAddRec->getNumOperands() - 1;
+           x != xe && !Overflow; ++x) {
+        SmallVector<const SCEV *, 7> SumOps;
+        for (int y = x, ye = 2 * x + 1; y != ye && !Overflow; ++y) {
+          uint64_t Coeff1 = Choose(x, 2 * x - y, Overflow);
+          for (int z = std::max(y - x, y - (int)AddRec->getNumOperands() + 1),
+                   ze = std::min(x + 1, (int)OtherAddRec->getNumOperands());
                z < ze && !Overflow; ++z) {
-            uint64_t Coeff2 = Choose(2*x - y, x-z, Overflow);
+            uint64_t Coeff2 = Choose(2 * x - y, x - z, Overflow);
             uint64_t Coeff;
             if (LargerThan64Bits)
               Coeff = umul_ov(Coeff1, Coeff2, Overflow);
             else
-              Coeff = Coeff1*Coeff2;
+              Coeff = Coeff1 * Coeff2;
             const SCEV *CoeffTerm = getConstant(Ty, Coeff);
-            const SCEV *Term1 = AddRec->getOperand(y-z);
+            const SCEV *Term1 = AddRec->getOperand(y - z);
             const SCEV *Term2 = OtherAddRec->getOperand(z);
             SumOps.push_back(getMulExpr(CoeffTerm, Term1, Term2,
                                         SCEV::FlagAnyWrap, Depth + 1));
@@ -3275,11 +3296,13 @@ const SCEV *ScalarEvolution::getMulExpr(
         AddRecOps.push_back(getAddExpr(SumOps, SCEV::FlagAnyWrap, Depth + 1));
       }
       if (!Overflow) {
-        const SCEV *NewAddRec = getAddRecExpr(AddRecOps, AddRecLoop,
-                                              SCEV::FlagAnyWrap);
-        if (Ops.size() == 2) return NewAddRec;
+        const SCEV *NewAddRec =
+            getAddRecExpr(AddRecOps, AddRecLoop, SCEV::FlagAnyWrap);
+        if (Ops.size() == 2)
+          return NewAddRec;
         Ops[Idx] = NewAddRec;
-        Ops.erase(Ops.begin() + OtherIdx); --OtherIdx;
+        Ops.erase(Ops.begin() + OtherIdx);
+        --OtherIdx;
         OpsModified = true;
         AddRec = dyn_cast<SCEVAddRecExpr>(NewAddRec);
         if (!AddRec)
@@ -3299,10 +3322,9 @@ const SCEV *ScalarEvolution::getMulExpr(
 }
 
 /// Represents an unsigned remainder expression based on unsigned division.
-const SCEV *ScalarEvolution::getURemExpr(const SCEV *LHS,
-                                         const SCEV *RHS) {
+const SCEV *ScalarEvolution::getURemExpr(const SCEV *LHS, const SCEV *RHS) {
   assert(getEffectiveSCEVType(LHS->getType()) ==
-         getEffectiveSCEVType(RHS->getType()) &&
+             getEffectiveSCEVType(RHS->getType()) &&
          "SCEVURemExpr operand types don't match!");
 
   // Short-circuit easy cases
@@ -3328,8 +3350,7 @@ const SCEV *ScalarEvolution::getURemExpr
 
 /// Get a canonical unsigned division expression, or something simpler if
 /// possible.
-const SCEV *ScalarEvolution::getUDivExpr(const SCEV *LHS,
-                                         const SCEV *RHS) {
+const SCEV *ScalarEvolution::getUDivExpr(const SCEV *LHS, const SCEV *RHS) {
   assert(!LHS->getType()->isPointerTy() &&
          "SCEVUDivExpr operand can't be pointer!");
   assert(LHS->getType() == RHS->getType() &&
@@ -3350,7 +3371,7 @@ const SCEV *ScalarEvolution::getUDivExpr
 
   if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS)) {
     if (RHSC->getValue()->isOne())
-      return LHS;                               // X udiv 1 --> x
+      return LHS; // X udiv 1 --> x
     // If the denominator is zero, the result of the udiv is undefined. Don't
     // try to analyze it, because the resolution chosen here may differ from
     // the resolution chosen in other parts of the compiler.
@@ -3366,18 +3387,18 @@ const SCEV *ScalarEvolution::getUDivExpr
       if (!RHSC->getAPInt().isPowerOf2())
         ++MaxShiftAmt;
       IntegerType *ExtTy =
-        IntegerType::get(getContext(), getTypeSizeInBits(Ty) + MaxShiftAmt);
+          IntegerType::get(getContext(), getTypeSizeInBits(Ty) + MaxShiftAmt);
       if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(LHS))
         if (const SCEVConstant *Step =
-            dyn_cast<SCEVConstant>(AR->getStepRecurrence(*this))) {
+                dyn_cast<SCEVConstant>(AR->getStepRecurrence(*this))) {
           // {X,+,N}/C --> {X/C,+,N/C} if safe and N/C can be folded.
           const APInt &StepInt = Step->getAPInt();
           const APInt &DivInt = RHSC->getAPInt();
           if (!StepInt.urem(DivInt) &&
               getZeroExtendExpr(AR, ExtTy) ==
-              getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),
-                            getZeroExtendExpr(Step, ExtTy),
-                            AR->getLoop(), SCEV::FlagAnyWrap)) {
+                  getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),
+                                getZeroExtendExpr(Step, ExtTy), AR->getLoop(),
+                                SCEV::FlagAnyWrap)) {
             SmallVector<const SCEV *, 4> Operands;
             for (const SCEV *Op : AR->operands())
               Operands.push_back(getUDivExpr(Op, RHS));
@@ -3389,9 +3410,9 @@ const SCEV *ScalarEvolution::getUDivExpr
           const SCEVConstant *StartC = dyn_cast<SCEVConstant>(AR->getStart());
           if (StartC && !DivInt.urem(StepInt) &&
               getZeroExtendExpr(AR, ExtTy) ==
-              getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),
-                            getZeroExtendExpr(Step, ExtTy),
-                            AR->getLoop(), SCEV::FlagAnyWrap)) {
+                  getAddRecExpr(getZeroExtendExpr(AR->getStart(), ExtTy),
+                                getZeroExtendExpr(Step, ExtTy), AR->getLoop(),
+                                SCEV::FlagAnyWrap)) {
             const APInt &StartInt = StartC->getAPInt();
             const APInt &StartRem = StartInt.urem(StepInt);
             if (StartRem != 0) {
@@ -3469,8 +3490,8 @@ const SCEV *ScalarEvolution::getUDivExpr
       if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(LHS)) {
         Constant *LHSCV = LHSC->getValue();
         Constant *RHSCV = RHSC->getValue();
-        return getConstant(cast<ConstantInt>(ConstantExpr::getUDiv(LHSCV,
-                                                                   RHSCV)));
+        return getConstant(
+            cast<ConstantInt>(ConstantExpr::getUDiv(LHSCV, RHSCV)));
       }
     }
   }
@@ -3478,9 +3499,10 @@ const SCEV *ScalarEvolution::getUDivExpr
   // The Insertion Point (IP) might be invalid by now (due to UniqueSCEVs
   // changes). Make sure we get a new one.
   IP = nullptr;
-  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP)) return S;
-  SCEV *S = new (SCEVAllocator) SCEVUDivExpr(ID.Intern(SCEVAllocator),
-                                             LHS, RHS);
+  if (const SCEV *S = UniqueSCEVs.FindNodeOrInsertPos(ID, IP))
+    return S;
+  SCEV *S =
+      new (SCEVAllocator) SCEVUDivExpr(ID.Intern(SCEVAllocator), LHS, RHS);
   UniqueSCEVs.InsertNode(S, IP);
   registerUser(S, {LHS, RHS});
   return S;
@@ -3578,7 +3600,8 @@ const SCEV *ScalarEvolution::getAddRecEx
 const SCEV *
 ScalarEvolution::getAddRecExpr(SmallVectorImpl<const SCEV *> &Operands,
                                const Loop *L, SCEV::NoWrapFlags Flags) {
-  if (Operands.size() == 1) return Operands[0];
+  if (Operands.size() == 1)
+    return Operands[0];
 #ifndef NDEBUG
   Type *ETy = getEffectiveSCEVType(Operands[0]->getType());
   for (unsigned i = 1, e = Operands.size(); i != e; ++i) {
@@ -3596,8 +3619,8 @@ ScalarEvolution::getAddRecExpr(SmallVect
     return getAddRecExpr(Operands, L, SCEV::FlagAnyWrap); // {X,+,0}  -->  X
   }
 
-  // It's tempting to want to call getConstantMaxBackedgeTakenCount count here and
-  // use that information to infer NUW and NSW flags. However, computing a
+  // It's tempting to want to call getConstantMaxBackedgeTakenCount count here
+  // and use that information to infer NUW and NSW flags. However, computing a
   // BE count requires calling getAddRecExpr, so we may not yet have a
   // meaningful BE count at this point (and if we don't, we'd be stuck
   // with a SCEVCouldNotCompute as the cached BE count).
@@ -3625,7 +3648,7 @@ ScalarEvolution::getAddRecExpr(SmallVect
         // The outer recurrence keeps its NW flag but only keeps NUW/NSW if the
         // inner recurrence has the same property.
         SCEV::NoWrapFlags OuterFlags =
-          maskFlags(Flags, SCEV::FlagNW | NestedAR->getNoWrapFlags());
+            maskFlags(Flags, SCEV::FlagNW | NestedAR->getNoWrapFlags());
 
         NestedOperands[0] = getAddRecExpr(Operands, L, OuterFlags);
         AllInvariant = all_of(NestedOperands, [&](const SCEV *Op) {
@@ -3638,7 +3661,7 @@ ScalarEvolution::getAddRecExpr(SmallVect
           // The inner recurrence keeps its NW flag but only keeps NUW/NSW if
           // the outer recurrence has the same property.
           SCEV::NoWrapFlags InnerFlags =
-            maskFlags(NestedAR->getNoWrapFlags(), SCEV::FlagNW | Flags);
+              maskFlags(NestedAR->getNoWrapFlags(), SCEV::FlagNW | Flags);
           return getAddRecExpr(NestedOperands, NestedLoop, InnerFlags);
         }
       }
@@ -3673,7 +3696,7 @@ ScalarEvolution::getGEPExpr(GEPOperator
   }();
 
   SCEV::NoWrapFlags OffsetWrap =
-    AssumeInBoundsFlags ? SCEV::FlagNSW : SCEV::FlagAnyWrap;
+      AssumeInBoundsFlags ? SCEV::FlagNSW : SCEV::FlagAnyWrap;
 
   Type *CurTy = GEP->getType();
   bool FirstIter = true;
@@ -3720,7 +3743,8 @@ ScalarEvolution::getGEPExpr(GEPOperator
   // base address is unsigned. However, if we know that the offset is
   // non-negative, we can use nuw.
   SCEV::NoWrapFlags BaseWrap = AssumeInBoundsFlags && isKnownNonNegative(Offset)
-                                   ? SCEV::FlagNUW : SCEV::FlagAnyWrap;
+                                   ? SCEV::FlagNUW
+                                   : SCEV::FlagAnyWrap;
   auto *GEPExpr = getAddExpr(BaseExpr, Offset, BaseWrap);
   assert(BaseExpr->getType() == GEPExpr->getType() &&
          "GEP should not change type mid-flight.");
@@ -3746,7 +3770,8 @@ const SCEV *ScalarEvolution::getMinMaxEx
                                            SmallVectorImpl<const SCEV *> &Ops) {
   assert(SCEVMinMaxExpr::isMinMaxType(Kind) && "Not a SCEVMinMaxExpr!");
   assert(!Ops.empty() && "Cannot get empty (u|s)(min|max)!");
-  if (Ops.size() == 1) return Ops[0];
+  if (Ops.size() == 1)
+    return Ops[0];
 #ifndef NDEBUG
   Type *ETy = getEffectiveSCEVType(Ops[0]->getType());
   for (unsigned i = 1, e = Ops.size(); i != e; ++i) {
@@ -3791,8 +3816,9 @@ const SCEV *ScalarEvolution::getMinMaxEx
       ConstantInt *Fold = ConstantInt::get(
           getContext(), FoldOp(LHSC->getAPInt(), RHSC->getAPInt()));
       Ops[0] = getConstant(Fold);
-      Ops.erase(Ops.begin()+1);  // Erase the folded element
-      if (Ops.size() == 1) return Ops[0];
+      Ops.erase(Ops.begin() + 1); // Erase the folded element
+      if (Ops.size() == 1)
+        return Ops[0];
       LHSC = cast<SCEVConstant>(Ops[0]);
     }
 
@@ -3809,7 +3835,8 @@ const SCEV *ScalarEvolution::getMinMaxEx
       return LHSC;
     }
 
-    if (Ops.size() == 1) return Ops[0];
+    if (Ops.size() == 1)
+      return Ops[0];
   }
 
   // Find the first operation of the same kind
@@ -3822,7 +3849,7 @@ const SCEV *ScalarEvolution::getMinMaxEx
     bool DeletedAny = false;
     while (Ops[Idx]->getSCEVType() == Kind) {
       const SCEVMinMaxExpr *SMME = cast<SCEVMinMaxExpr>(Ops[Idx]);
-      Ops.erase(Ops.begin()+Idx);
+      Ops.erase(Ops.begin() + Idx);
       Ops.append(SMME->op_begin(), SMME->op_end());
       DeletedAny = true;
     }
@@ -3857,7 +3884,8 @@ const SCEV *ScalarEvolution::getMinMaxEx
     }
   }
 
-  if (Ops.size() == 1) return Ops[0];
+  if (Ops.size() == 1)
+    return Ops[0];
 
   assert(!Ops.empty() && "Reduced smax down to nothing!");
 
@@ -4102,9 +4130,8 @@ const SCEV *ScalarEvolution::getUMaxExpr
   return getMinMaxExpr(scUMaxExpr, Ops);
 }
 
-const SCEV *ScalarEvolution::getSMinExpr(const SCEV *LHS,
-                                         const SCEV *RHS) {
-  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };
+const SCEV *ScalarEvolution::getSMinExpr(const SCEV *LHS, const SCEV *RHS) {
+  SmallVector<const SCEV *, 2> Ops = {LHS, RHS};
   return getSMinExpr(Ops);
 }
 
@@ -4114,7 +4141,7 @@ const SCEV *ScalarEvolution::getSMinExpr
 
 const SCEV *ScalarEvolution::getUMinExpr(const SCEV *LHS, const SCEV *RHS,
                                          bool Sequential) {
-  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };
+  SmallVector<const SCEV *, 2> Ops = {LHS, RHS};
   return getUMinExpr(Ops, Sequential);
 }
 
@@ -4154,8 +4181,7 @@ const SCEV *ScalarEvolution::getStoreSiz
   return getConstant(IntTy, getDataLayout().getTypeStoreSize(StoreTy));
 }
 
-const SCEV *ScalarEvolution::getOffsetOfExpr(Type *IntTy,
-                                             StructType *STy,
+const SCEV *ScalarEvolution::getOffsetOfExpr(Type *IntTy, StructType *STy,
                                              unsigned FieldNo) {
   // We can bypass creating a target-independent constant expression and then
   // folding it back into a ConstantInt. This is just a compile-time
@@ -4179,8 +4205,8 @@ const SCEV *ScalarEvolution::getUnknown(
            "Stale SCEVUnknown in uniquing map!");
     return S;
   }
-  SCEV *S = new (SCEVAllocator) SCEVUnknown(ID.Intern(SCEVAllocator), V, this,
-                                            FirstUnknown);
+  SCEV *S = new (SCEVAllocator)
+      SCEVUnknown(ID.Intern(SCEVAllocator), V, this, FirstUnknown);
   FirstUnknown = cast<SCEVUnknown>(S);
   UniqueSCEVs.InsertNode(S, IP);
   return S;
@@ -4223,7 +4249,7 @@ Type *ScalarEvolution::getEffectiveSCEVT
 }
 
 Type *ScalarEvolution::getWiderType(Type *T1, Type *T2) const {
-  return  getTypeSizeInBits(T1) >= getTypeSizeInBits(T2) ? T1 : T2;
+  return getTypeSizeInBits(T1) >= getTypeSizeInBits(T2) ? T1 : T2;
 }
 
 bool ScalarEvolution::instructionCouldExistWitthOperands(const SCEV *A,
@@ -4237,10 +4263,9 @@ bool ScalarEvolution::instructionCouldEx
     // Can't tell.
     return false;
   return (ScopeA == ScopeB) || DT.dominates(ScopeA, ScopeB) ||
-    DT.dominates(ScopeB, ScopeA);
+         DT.dominates(ScopeB, ScopeA);
 }
 
-
 const SCEV *ScalarEvolution::getCouldNotCompute() {
   return CouldNotCompute.get();
 }
@@ -4385,8 +4410,7 @@ const SCEV *ScalarEvolution::getExisting
 const SCEV *ScalarEvolution::getNegativeSCEV(const SCEV *V,
                                              SCEV::NoWrapFlags Flags) {
   if (const SCEVConstant *VC = dyn_cast<SCEVConstant>(V))
-    return getConstant(
-               cast<ConstantInt>(ConstantExpr::getNeg(VC->getValue())));
+    return getConstant(cast<ConstantInt>(ConstantExpr::getNeg(VC->getValue())));
 
   Type *Ty = V->getType();
   Ty = getEffectiveSCEVType(Ty);
@@ -4413,8 +4437,7 @@ const SCEV *ScalarEvolution::getNotSCEV(
   assert(!V->getType()->isPointerTy() && "Can't negate pointer");
 
   if (const SCEVConstant *VC = dyn_cast<SCEVConstant>(V))
-    return getConstant(
-                cast<ConstantInt>(ConstantExpr::getNot(VC->getValue())));
+    return getConstant(cast<ConstantInt>(ConstantExpr::getNot(VC->getValue())));
 
   // Fold ~(u|s)(min|max)(~x, ~y) to (u|s)(max|min)(x, y)
   if (const SCEVMinMaxExpr *MME = dyn_cast<SCEVMinMaxExpr>(V)) {
@@ -4489,8 +4512,7 @@ const SCEV *ScalarEvolution::getMinusSCE
   // We represent LHS - RHS as LHS + (-1)*RHS. This transformation
   // makes it so that we cannot make much use of NUW.
   auto AddFlags = SCEV::FlagAnyWrap;
-  const bool RHSIsNotMinSigned =
-      !getSignedRangeMin(RHS).isMinSignedValue();
+  const bool RHSIsNotMinSigned = !getSignedRangeMin(RHS).isMinSignedValue();
   if (hasFlags(Flags, SCEV::FlagNSW)) {
     // Let M be the minimum representable signed value. Then (-1)*RHS
     // signed-wraps if and only if RHS is M. That can happen even for
@@ -4524,7 +4546,7 @@ const SCEV *ScalarEvolution::getTruncate
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot truncate or zero extend with non-integer arguments!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   if (getTypeSizeInBits(SrcTy) > getTypeSizeInBits(Ty))
     return getTruncateExpr(V, Ty, Depth);
   return getZeroExtendExpr(V, Ty, Depth);
@@ -4536,57 +4558,53 @@ const SCEV *ScalarEvolution::getTruncate
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot truncate or zero extend with non-integer arguments!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   if (getTypeSizeInBits(SrcTy) > getTypeSizeInBits(Ty))
     return getTruncateExpr(V, Ty, Depth);
   return getSignExtendExpr(V, Ty, Depth);
 }
 
-const SCEV *
-ScalarEvolution::getNoopOrZeroExtend(const SCEV *V, Type *Ty) {
+const SCEV *ScalarEvolution::getNoopOrZeroExtend(const SCEV *V, Type *Ty) {
   Type *SrcTy = V->getType();
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot noop or zero extend with non-integer arguments!");
   assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&
          "getNoopOrZeroExtend cannot truncate!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   return getZeroExtendExpr(V, Ty);
 }
 
-const SCEV *
-ScalarEvolution::getNoopOrSignExtend(const SCEV *V, Type *Ty) {
+const SCEV *ScalarEvolution::getNoopOrSignExtend(const SCEV *V, Type *Ty) {
   Type *SrcTy = V->getType();
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot noop or sign extend with non-integer arguments!");
   assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&
          "getNoopOrSignExtend cannot truncate!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   return getSignExtendExpr(V, Ty);
 }
 
-const SCEV *
-ScalarEvolution::getNoopOrAnyExtend(const SCEV *V, Type *Ty) {
+const SCEV *ScalarEvolution::getNoopOrAnyExtend(const SCEV *V, Type *Ty) {
   Type *SrcTy = V->getType();
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot noop or any extend with non-integer arguments!");
   assert(getTypeSizeInBits(SrcTy) <= getTypeSizeInBits(Ty) &&
          "getNoopOrAnyExtend cannot truncate!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   return getAnyExtendExpr(V, Ty);
 }
 
-const SCEV *
-ScalarEvolution::getTruncateOrNoop(const SCEV *V, Type *Ty) {
+const SCEV *ScalarEvolution::getTruncateOrNoop(const SCEV *V, Type *Ty) {
   Type *SrcTy = V->getType();
   assert(SrcTy->isIntOrPtrTy() && Ty->isIntOrPtrTy() &&
          "Cannot truncate or noop with non-integer arguments!");
   assert(getTypeSizeInBits(SrcTy) >= getTypeSizeInBits(Ty) &&
          "getTruncateOrNoop cannot extend!");
   if (getTypeSizeInBits(SrcTy) == getTypeSizeInBits(Ty))
-    return V;  // No conversion
+    return V; // No conversion
   return getTruncateExpr(V, Ty);
 }
 
@@ -4606,7 +4624,7 @@ const SCEV *ScalarEvolution::getUMaxFrom
 const SCEV *ScalarEvolution::getUMinFromMismatchedTypes(const SCEV *LHS,
                                                         const SCEV *RHS,
                                                         bool Sequential) {
-  SmallVector<const SCEV *, 2> Ops = { LHS, RHS };
+  SmallVector<const SCEV *, 2> Ops = {LHS, RHS};
   return getUMinFromMismatchedTypes(Ops, Sequential);
 }
 
@@ -4724,12 +4742,12 @@ private:
 /// If SCEV contains non-invariant unknown SCEV rewrite cannot be done.
 class SCEVPostIncRewriter : public SCEVRewriteVisitor<SCEVPostIncRewriter> {
 public:
-  static const SCEV *rewrite(const SCEV *S, const Loop *L, ScalarEvolution &SE) {
+  static const SCEV *rewrite(const SCEV *S, const Loop *L,
+                             ScalarEvolution &SE) {
     SCEVPostIncRewriter Rewriter(L, SE);
     const SCEV *Result = Rewriter.visit(S);
-    return Rewriter.hasSeenLoopVariantSCEVUnknown()
-        ? SE.getCouldNotCompute()
-        : Result;
+    return Rewriter.hasSeenLoopVariantSCEVUnknown() ? SE.getCouldNotCompute()
+                                                    : Result;
   }
 
   const SCEV *visitUnknown(const SCEVUnknown *Expr) {
@@ -4946,8 +4964,7 @@ ScalarEvolution::proveNoSignedWrapViaInd
   // start value and the backedge is guarded by a comparison with the post-inc
   // value, the addrec is safe.
   ICmpInst::Predicate Pred;
-  const SCEV *OverflowLimit =
-    getSignedOverflowLimitForStep(Step, &Pred, this);
+  const SCEV *OverflowLimit = getSignedOverflowLimitForStep(Step, &Pred, this);
   if (OverflowLimit &&
       (isLoopBackedgeGuardedByCond(L, Pred, AR, OverflowLimit) ||
        isKnownOnEveryIteration(Pred, AR, OverflowLimit))) {
@@ -4996,8 +5013,8 @@ ScalarEvolution::proveNoUnsignedWrapViaI
   // start value and the backedge is guarded by a comparison with the post-inc
   // value, the addrec is safe.
   if (isKnownPositive(Step)) {
-    const SCEV *N = getConstant(APInt::getMinValue(BitWidth) -
-                                getUnsignedRangeMax(Step));
+    const SCEV *N =
+        getConstant(APInt::getMinValue(BitWidth) - getUnsignedRangeMax(Step));
     if (isLoopBackedgeGuardedByCond(L, ICmpInst::ICMP_ULT, AR, N) ||
         isKnownOnEveryIteration(ICmpInst::ICMP_ULT, AR, N)) {
       Result = setFlags(Result, SCEV::FlagNUW);
@@ -5064,7 +5081,8 @@ static Optional<BinaryOp> MatchBinaryOp(
   case Instruction::Xor:
     if (auto *RHSC = dyn_cast<ConstantInt>(Op->getOperand(1)))
       // If the RHS of the xor is a signmask, then this is just an add.
-      // Instcombine turns add of signmask into xor as a strength reduction step.
+      // Instcombine turns add of signmask into xor as a strength reduction
+      // step.
       if (RHSC->getValue().isSignMask())
         return BinaryOp(Instruction::Add, Op->getOperand(0), Op->getOperand(1));
     return BinaryOp(Op);
@@ -5201,9 +5219,10 @@ static const Loop *isIntegerLoopHeaderPH
 //    will return the pair {NewAddRec, SmallPredsVec} where:
 //         NewAddRec = {%Start,+,%Step}
 //         SmallPredsVec = {P1, P2, P3} as follows:
-//           P1(WrapPred): AR: {trunc(%Start),+,(trunc %Step)}<nsw> Flags: <nssw>
-//           P2(EqualPred): %Start == (sext i32 (trunc i64 %Start to i32) to i64)
-//           P3(EqualPred): %Step == (sext i32 (trunc i64 %Step to i32) to i64)
+//           P1(WrapPred): AR: {trunc(%Start),+,(trunc %Step)}<nsw> Flags:
+//           <nssw> P2(EqualPred): %Start == (sext i32 (trunc i64 %Start to i32)
+//           to i64) P3(EqualPred): %Step == (sext i32 (trunc i64 %Step to i32)
+//           to i64)
 //    The returned pair means that SymbolicPHI can be rewritten into NewAddRec
 //    under the predicates {P1,P2,P3}.
 //    This predicated rewrite will be cached in PredicatedSCEVRewrites:
@@ -5232,7 +5251,8 @@ static const Loop *isIntegerLoopHeaderPH
 //
 // 3) Outline common code with createAddRecFromPHI to avoid duplication.
 Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>
-ScalarEvolution::createAddRecFromPHIWithCastsImpl(const SCEVUnknown *SymbolicPHI) {
+ScalarEvolution::createAddRecFromPHIWithCastsImpl(
+    const SCEVUnknown *SymbolicPHI) {
   SmallVector<const SCEVPredicate *, 3> Predicates;
 
   // *** Part1: Analyze if we have a phi-with-cast pattern for which we can
@@ -5465,7 +5485,7 @@ ScalarEvolution::createAddRecFromPHIWith
   }
 
   Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>
-    Rewrite = createAddRecFromPHIWithCastsImpl(SymbolicPHI);
+      Rewrite = createAddRecFromPHIWithCastsImpl(SymbolicPHI);
 
   // Record in the cache that the analysis failed
   if (!Rewrite) {
@@ -5687,8 +5707,7 @@ const SCEV *ScalarEvolution::createAddRe
     //   PHI(f(0), f({1,+,1})) --> f({0,+,1})
     const SCEV *Shifted = SCEVShiftRewriter::rewrite(BEValue, L, *this);
     const SCEV *Start = SCEVInitRewriter::rewrite(Shifted, L, *this, false);
-    if (Shifted != getCouldNotCompute() &&
-        Start != getCouldNotCompute()) {
+    if (Shifted != getCouldNotCompute() && Start != getCouldNotCompute()) {
       const SCEV *StartVal = getSCEV(StartValueV);
       if (Start == StartVal) {
         // Okay, for the entire analysis of this edge we assumed the PHI
@@ -5718,12 +5737,12 @@ static bool IsAvailableOnEntry(const Loo
     bool TraversalDone = false;
     bool Available = true;
 
-    const Loop *L = nullptr;  // The loop BB is in (can be nullptr)
+    const Loop *L = nullptr; // The loop BB is in (can be nullptr)
     BasicBlock *BB = nullptr;
     DominatorTree &DT;
 
     CheckAvailable(const Loop *L, BasicBlock *BB, DominatorTree &DT)
-      : L(L), BB(BB), DT(DT) {}
+        : L(L), BB(BB), DT(DT) {}
 
     bool setUnavailable() {
       TraversalDone = true;
@@ -5827,8 +5846,9 @@ static bool BrPHIToSelect(DominatorTree
 }
 
 const SCEV *ScalarEvolution::createNodeFromSelectLikePHI(PHINode *PN) {
-  auto IsReachable =
-      [&](BasicBlock *BB) { return DT.isReachableFromEntry(BB); };
+  auto IsReachable = [&](BasicBlock *BB) {
+    return DT.isReachableFromEntry(BB);
+  };
   if (PN->getNumIncomingValues() == 2 && all_of(PN->blocks(), IsReachable)) {
     const Loop *L = LI.getLoopFor(PN->getParent());
 
@@ -6075,7 +6095,8 @@ uint32_t ScalarEvolution::GetMinTrailing
 
   if (const SCEVUnknown *U = dyn_cast<SCEVUnknown>(S)) {
     // For a SCEVUnknown, ask ValueTracking.
-    KnownBits Known = computeKnownBits(U->getValue(), getDataLayout(), 0, &AC, nullptr, &DT);
+    KnownBits Known =
+        computeKnownBits(U->getValue(), getDataLayout(), 0, &AC, nullptr, &DT);
     return Known.countMinTrailingZeros();
   }
 
@@ -6112,8 +6133,8 @@ void ScalarEvolution::setNoWrapFlags(SCE
   }
 }
 
-ConstantRange ScalarEvolution::
-getRangeForUnknownRecurrence(const SCEVUnknown *U) {
+ConstantRange
+ScalarEvolution::getRangeForUnknownRecurrence(const SCEVUnknown *U) {
   const DataLayout &DL = getDataLayout();
 
   unsigned BitWidth = getTypeSizeInBits(U->getType());
@@ -6180,7 +6201,7 @@ getRangeForUnknownRecurrence(const SCEVU
 
   // Compute total shift amount, being careful of overflow and bitwidths.
   auto MaxShiftAmt = KnownStep.getMaxValue();
-  APInt TCAP(BitWidth, TC-1);
+  APInt TCAP(BitWidth, TC - 1);
   bool Overflow = false;
   auto TotalShift = MaxShiftAmt.umul_ov(TCAP, Overflow);
   if (Overflow)
@@ -6195,8 +6216,8 @@ getRangeForUnknownRecurrence(const SCEVU
     //   saturation => 0 or -1
     //   other => a value closer to zero (of the same sign)
     // Thus, the end value is closer to zero than the start.
-    auto KnownEnd = KnownBits::ashr(KnownStart,
-                                    KnownBits::makeConstant(TotalShift));
+    auto KnownEnd =
+        KnownBits::ashr(KnownStart, KnownBits::makeConstant(TotalShift));
     if (KnownStart.isNonNegative())
       // Analogous to lshr (simply not yet canonicalized)
       return ConstantRange::getNonEmpty(KnownEnd.getMinValue(),
@@ -6213,15 +6234,15 @@ getRangeForUnknownRecurrence(const SCEVU
     //   saturation => 0
     //   other => a smaller positive number
     // Thus, the low end of the unsigned range is the last value produced.
-    auto KnownEnd = KnownBits::lshr(KnownStart,
-                                    KnownBits::makeConstant(TotalShift));
+    auto KnownEnd =
+        KnownBits::lshr(KnownStart, KnownBits::makeConstant(TotalShift));
     return ConstantRange::getNonEmpty(KnownEnd.getMinValue(),
                                       KnownStart.getMaxValue() + 1);
   }
   case Instruction::Shl: {
     // Iff no bits are shifted out, value increases on every shift.
-    auto KnownEnd = KnownBits::shl(KnownStart,
-                                   KnownBits::makeConstant(TotalShift));
+    auto KnownEnd =
+        KnownBits::shl(KnownStart, KnownBits::makeConstant(TotalShift));
     if (TotalShift.ult(KnownStart.countMinLeadingZeros()))
       return ConstantRange(KnownStart.getMinValue(),
                            KnownEnd.getMaxValue() + 1);
@@ -6241,8 +6262,8 @@ ScalarEvolution::getRangeRef(const SCEV
       SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED ? UnsignedRanges
                                                        : SignedRanges;
   ConstantRange::PreferredRangeType RangeType =
-      SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED
-          ? ConstantRange::Unsigned : ConstantRange::Signed;
+      SignHint == ScalarEvolution::HINT_RANGE_UNSIGNED ? ConstantRange::Unsigned
+                                                       : ConstantRange::Signed;
 
   // See if we've computed this range already.
   DenseMap<const SCEV *, ConstantRange>::iterator I = Cache.find(S);
@@ -6278,8 +6299,8 @@ ScalarEvolution::getRangeRef(const SCEV
     if (Add->hasNoUnsignedWrap())
       WrapType |= OBO::NoUnsignedWrap;
     for (unsigned i = 1, e = Add->getNumOperands(); i != e; ++i)
-      X = X.addWithNoWrap(getRangeRef(Add->getOperand(i), SignHint),
-                          WrapType, RangeType);
+      X = X.addWithNoWrap(getRangeRef(Add->getOperand(i), SignHint), WrapType,
+                          RangeType);
     return setRange(Add, SignHint,
                     ConservativeResult.intersectWith(X, RangeType));
   }
@@ -6329,16 +6350,16 @@ ScalarEvolution::getRangeRef(const SCEV
 
   if (const SCEVZeroExtendExpr *ZExt = dyn_cast<SCEVZeroExtendExpr>(S)) {
     ConstantRange X = getRangeRef(ZExt->getOperand(), SignHint);
-    return setRange(ZExt, SignHint,
-                    ConservativeResult.intersectWith(X.zeroExtend(BitWidth),
-                                                     RangeType));
+    return setRange(
+        ZExt, SignHint,
+        ConservativeResult.intersectWith(X.zeroExtend(BitWidth), RangeType));
   }
 
   if (const SCEVSignExtendExpr *SExt = dyn_cast<SCEVSignExtendExpr>(S)) {
     ConstantRange X = getRangeRef(SExt->getOperand(), SignHint);
-    return setRange(SExt, SignHint,
-                    ConservativeResult.intersectWith(X.signExtend(BitWidth),
-                                                     RangeType));
+    return setRange(
+        SExt, SignHint,
+        ConservativeResult.intersectWith(X.signExtend(BitWidth), RangeType));
   }
 
   if (const SCEVPtrToIntExpr *PtrToInt = dyn_cast<SCEVPtrToIntExpr>(S)) {
@@ -6348,9 +6369,9 @@ ScalarEvolution::getRangeRef(const SCEV
 
   if (const SCEVTruncateExpr *Trunc = dyn_cast<SCEVTruncateExpr>(S)) {
     ConstantRange X = getRangeRef(Trunc->getOperand(), SignHint);
-    return setRange(Trunc, SignHint,
-                    ConservativeResult.intersectWith(X.truncate(BitWidth),
-                                                     RangeType));
+    return setRange(
+        Trunc, SignHint,
+        ConservativeResult.intersectWith(X.truncate(BitWidth), RangeType));
   }
 
   if (const SCEVAddRecExpr *AddRec = dyn_cast<SCEVAddRecExpr>(S)) {
@@ -6384,15 +6405,16 @@ ScalarEvolution::getRangeRef(const SCEV
             RangeType);
       else if (AllNonPos)
         ConservativeResult = ConservativeResult.intersectWith(
-            ConstantRange::getNonEmpty(
-                APInt::getSignedMinValue(BitWidth),
-                getSignedRangeMax(AddRec->getStart()) + 1),
+            ConstantRange::getNonEmpty(APInt::getSignedMinValue(BitWidth),
+                                       getSignedRangeMax(AddRec->getStart()) +
+                                           1),
             RangeType);
     }
 
     // TODO: non-affine addrec
     if (AddRec->isAffine()) {
-      const SCEV *MaxBECount = getConstantMaxBackedgeTakenCount(AddRec->getLoop());
+      const SCEV *MaxBECount =
+          getConstantMaxBackedgeTakenCount(AddRec->getLoop());
       if (!isa<SCEVCouldNotCompute>(MaxBECount) &&
           getTypeSizeInBits(MaxBECount->getType()) <= BitWidth) {
         auto RangeFromAffine = getRangeForAffineAR(
@@ -6431,8 +6453,8 @@ ScalarEvolution::getRangeRef(const SCEV
     // Check if the IR explicitly contains !range metadata.
     Optional<ConstantRange> MDRange = GetRangeFromMetadata(U->getValue());
     if (MDRange.hasValue())
-      ConservativeResult = ConservativeResult.intersectWith(MDRange.getValue(),
-                                                            RangeType);
+      ConservativeResult =
+          ConservativeResult.intersectWith(MDRange.getValue(), RangeType);
 
     // Use facts about recurrences in the underlying IR.  Note that add
     // recurrences are AddRecExprs and thus don't hit this path.  This
@@ -6492,7 +6514,7 @@ ScalarEvolution::getRangeRef(const SCEV
             ConservativeResult.intersectWith(RangeFromOps, RangeType);
         bool Erased = PendingPhiRanges.erase(Phi);
         assert(Erased && "Failed to erase Phi properly?");
-        (void) Erased;
+        (void)Erased;
       }
     }
 
@@ -6592,8 +6614,8 @@ ConstantRange ScalarEvolution::getRangeF
 
   // Next, consider step unsigned.
   ConstantRange UR = getRangeForAffineARHelper(
-      getUnsignedRangeMax(Step), getUnsignedRange(Start),
-      MaxBECountValue, BitWidth, /* Signed = */ false);
+      getUnsignedRangeMax(Step), getUnsignedRange(Start), MaxBECountValue,
+      BitWidth, /* Signed = */ false);
 
   // Finally, intersect signed and unsigned ranges.
   return SR.intersectWith(UR, ConstantRange::Smallest);
@@ -6653,8 +6675,8 @@ ConstantRange ScalarEvolution::getRangeF
   if (RangeBetween.isFullSet())
     return RangeBetween;
   // Only deal with ranges that do not wrap (i.e. RangeMin < RangeMax).
-  bool IsWrappedSet = IsSigned ? RangeBetween.isSignWrappedSet()
-                               : RangeBetween.isWrappedSet();
+  bool IsWrappedSet =
+      IsSigned ? RangeBetween.isSignWrappedSet() : RangeBetween.isWrappedSet();
   if (IsWrappedSet)
     return ConstantRange::getFull(BitWidth);
 
@@ -6684,8 +6706,7 @@ ConstantRange ScalarEvolution::getRangeV
       Optional<unsigned> CastOp;
       APInt Offset(BitWidth, 0);
 
-      assert(SE.getTypeSizeInBits(S->getType()) == BitWidth &&
-             "Should be!");
+      assert(SE.getTypeSizeInBits(S->getType()) == BitWidth && "Should be!");
 
       // Peel off a constant offset:
       if (auto *SA = dyn_cast<SCEVAddExpr>(S)) {
@@ -6783,7 +6804,8 @@ ConstantRange ScalarEvolution::getRangeV
 }
 
 SCEV::NoWrapFlags ScalarEvolution::getNoWrapFlagsFromUB(const Value *V) {
-  if (isa<ConstantExpr>(V)) return SCEV::FlagAnyWrap;
+  if (isa<ConstantExpr>(V))
+    return SCEV::FlagAnyWrap;
   const BinaryOperator *BinOp = cast<BinaryOperator>(V);
 
   // Return early if there are no flags to propagate to the SCEV.
@@ -6889,7 +6911,6 @@ bool ScalarEvolution::isGuaranteedToTran
   return false;
 }
 
-
 bool ScalarEvolution::isSCEVExprNeverPoison(const Instruction *I) {
   // Only proceed if we can prove that I does not yield poison.
   if (!programUndefinedIfPoison(I))
@@ -7148,8 +7169,7 @@ const SCEV *ScalarEvolution::createSCEV(
         unsigned TZ = A.countTrailingZeros();
         unsigned BitWidth = A.getBitWidth();
         KnownBits Known(BitWidth);
-        computeKnownBits(BO->LHS, Known, getDataLayout(),
-                         0, &AC, nullptr, &DT);
+        computeKnownBits(BO->LHS, Known, getDataLayout(), 0, &AC, nullptr, &DT);
 
         APInt EffectiveMask =
             APInt::getLowBitsSet(BitWidth, BitWidth - LZ - TZ).shl(TZ);
@@ -7163,7 +7183,7 @@ const SCEV *ScalarEvolution::createSCEV(
               unsigned MulZeros = OpC->getAPInt().countTrailingZeros();
               unsigned GCD = std::min(MulZeros, TZ);
               APInt DivAmt = APInt::getOneBitSet(BitWidth, TZ - GCD);
-              SmallVector<const SCEV*, 4> MulOps;
+              SmallVector<const SCEV *, 4> MulOps;
               MulOps.push_back(getConstant(OpC->getAPInt().lshr(GCD)));
               MulOps.append(LHSMul->op_begin() + 1, LHSMul->op_end());
               auto *NewMul = getMulExpr(MulOps, LHSMul->getNoWrapFlags());
@@ -7174,7 +7194,8 @@ const SCEV *ScalarEvolution::createSCEV(
             ShiftedLHS = getUDivExpr(LHS, MulCount);
           return getMulExpr(
               getZeroExtendExpr(
-                  getTruncateExpr(ShiftedLHS,
+                  getTruncateExpr(
+                      ShiftedLHS,
                       IntegerType::get(getContext(), BitWidth - LZ - TZ)),
                   BO->LHS->getType()),
               MulCount);
@@ -7303,8 +7324,8 @@ const SCEV *ScalarEvolution::createSCEV(
         if (L->getOperand(1) == BO->RHS)
           // For a two-shift sext-inreg, i.e. n = m,
           // use sext(trunc(x)) as the SCEV expression.
-          return getSignExtendExpr(
-              getTruncateExpr(ShlOp0SCEV, TruncTy), OuterTy);
+          return getSignExtendExpr(getTruncateExpr(ShlOp0SCEV, TruncTy),
+                                   OuterTy);
 
         ConstantInt *ShlAmtCI = dyn_cast<ConstantInt>(L->getOperand(1));
         if (ShlAmtCI && ShlAmtCI->getValue().ult(BitWidth)) {
@@ -7314,11 +7335,12 @@ const SCEV *ScalarEvolution::createSCEV(
             // expression. We already checked that ShlAmt < BitWidth, so
             // the multiplier, 1 << (ShlAmt - AShrAmt), fits into TruncTy as
             // ShlAmt - AShrAmt < Amt.
-            APInt Mul = APInt::getOneBitSet(BitWidth - AShrAmt,
-                                            ShlAmt - AShrAmt);
+            APInt Mul =
+                APInt::getOneBitSet(BitWidth - AShrAmt, ShlAmt - AShrAmt);
             return getSignExtendExpr(
                 getMulExpr(getTruncateExpr(ShlOp0SCEV, TruncTy),
-                getConstant(Mul)), OuterTy);
+                           getConstant(Mul)),
+                OuterTy);
           }
         }
       }
@@ -7869,7 +7891,7 @@ void ScalarEvolution::forgetLoop(const L
     auto LoopUsersItr = LoopUsers.find(CurrL);
     if (LoopUsersItr != LoopUsers.end()) {
       ToForget.insert(ToForget.end(), LoopUsersItr->second.begin(),
-                LoopUsersItr->second.end());
+                      LoopUsersItr->second.end());
       LoopUsers.erase(LoopUsersItr);
     }
 
@@ -7907,7 +7929,8 @@ void ScalarEvolution::forgetTopmostLoop(
 
 void ScalarEvolution::forgetValue(Value *V) {
   Instruction *I = dyn_cast<Instruction>(V);
-  if (!I) return;
+  if (!I)
+    return;
 
   // Drop information about expressions based on loop-header PHIs.
   SmallVector<Instruction *, 16> Worklist;
@@ -7919,7 +7942,7 @@ void ScalarEvolution::forgetValue(Value
   while (!Worklist.empty()) {
     I = Worklist.pop_back_val();
     ValueExprMapType::iterator It =
-      ValueExprMap.find_as(static_cast<Value *>(I));
+        ValueExprMap.find_as(static_cast<Value *>(I));
     if (It != ValueExprMap.end()) {
       eraseValueFromMap(It->first);
       ToForget.push_back(It->second);
@@ -8029,8 +8052,7 @@ bool ScalarEvolution::BackedgeTakenInfo:
 }
 
 ScalarEvolution::ExitLimit::ExitLimit(const SCEV *E)
-    : ExitLimit(E, E, false, None) {
-}
+    : ExitLimit(E, E, false, None) {}
 
 ScalarEvolution::ExitLimit::ExitLimit(
     const SCEV *E, const SCEV *M, bool MaxOrZero,
@@ -8060,13 +8082,11 @@ ScalarEvolution::ExitLimit::ExitLimit(
 ScalarEvolution::ExitLimit::ExitLimit(
     const SCEV *E, const SCEV *M, bool MaxOrZero,
     const SmallPtrSetImpl<const SCEVPredicate *> &PredSet)
-    : ExitLimit(E, M, MaxOrZero, {&PredSet}) {
-}
+    : ExitLimit(E, M, MaxOrZero, {&PredSet}) {}
 
 ScalarEvolution::ExitLimit::ExitLimit(const SCEV *E, const SCEV *M,
                                       bool MaxOrZero)
-    : ExitLimit(E, M, MaxOrZero, None) {
-}
+    : ExitLimit(E, M, MaxOrZero, None) {}
 
 /// Allocate memory for BackedgeTakenInfo and copy the not-taken count of each
 /// computable exit into a persistent ExitNotTakenInfo array.
@@ -8172,8 +8192,10 @@ ScalarEvolution::computeBackedgeTakenCou
       }
     }
   }
-  const SCEV *MaxBECount = MustExitMaxBECount ? MustExitMaxBECount :
-    (MayExitMaxBECount ? MayExitMaxBECount : getCouldNotCompute());
+  const SCEV *MaxBECount =
+      MustExitMaxBECount
+          ? MustExitMaxBECount
+          : (MayExitMaxBECount ? MayExitMaxBECount : getCouldNotCompute());
   // The loop backedge will be taken the maximum or zero times if there's
   // a single exit that must be taken the maximum or zero times.
   bool MaxOrZero = (MustExitMaxOrZero && ExitingBlocks.size() == 1);
@@ -8190,7 +8212,7 @@ ScalarEvolution::computeBackedgeTakenCou
 
 ScalarEvolution::ExitLimit
 ScalarEvolution::computeExitLimit(const Loop *L, BasicBlock *ExitingBlock,
-                                      bool AllowPredicates) {
+                                  bool AllowPredicates) {
   assert(L->contains(ExitingBlock) && "Exit count for non-loop block?");
   // If our exiting block does not dominate the latch, then its connection with
   // loop's exit limit may be far from trivial.
@@ -8206,9 +8228,9 @@ ScalarEvolution::computeExitLimit(const
     assert(ExitIfTrue == L->contains(BI->getSuccessor(1)) &&
            "It should have one successor in loop and one exit block!");
     // Proceed to the next level to examine the exit condition expression.
-    return computeExitLimitFromCond(
-        L, BI->getCondition(), ExitIfTrue,
-        /*ControlsExit=*/IsOnlyExit, AllowPredicates);
+    return computeExitLimitFromCond(L, BI->getCondition(), ExitIfTrue,
+                                    /*ControlsExit=*/IsOnlyExit,
+                                    AllowPredicates);
   }
 
   if (SwitchInst *SI = dyn_cast<SwitchInst>(Term)) {
@@ -8228,9 +8250,10 @@ ScalarEvolution::computeExitLimit(const
   return getCouldNotCompute();
 }
 
-ScalarEvolution::ExitLimit ScalarEvolution::computeExitLimitFromCond(
-    const Loop *L, Value *ExitCond, bool ExitIfTrue,
-    bool ControlsExit, bool AllowPredicates) {
+ScalarEvolution::ExitLimit
+ScalarEvolution::computeExitLimitFromCond(const Loop *L, Value *ExitCond,
+                                          bool ExitIfTrue, bool ControlsExit,
+                                          bool AllowPredicates) {
   ScalarEvolution::ExitLimitCacheTy Cache(L, ExitIfTrue, AllowPredicates);
   return computeExitLimitFromCondCached(Cache, L, ExitCond, ExitIfTrue,
                                         ControlsExit, AllowPredicates);
@@ -8254,8 +8277,7 @@ ScalarEvolution::ExitLimitCache::find(co
 }
 
 void ScalarEvolution::ExitLimitCache::insert(const Loop *L, Value *ExitCond,
-                                             bool ExitIfTrue,
-                                             bool ControlsExit,
+                                             bool ExitIfTrue, bool ControlsExit,
                                              bool AllowPredicates,
                                              const ExitLimit &EL) {
   assert(this->L == L && this->ExitIfTrue == ExitIfTrue &&
@@ -8323,9 +8345,8 @@ ScalarEvolution::ExitLimit ScalarEvoluti
   const APInt *C;
   if (match(ExitCond, m_ExtractValue<1>(m_WithOverflowInst(WO))) &&
       match(WO->getRHS(), m_APInt(C))) {
-    ConstantRange NWR =
-      ConstantRange::makeExactNoWrapRegion(WO->getBinaryOp(), *C,
-                                           WO->getNoWrapKind());
+    ConstantRange NWR = ConstantRange::makeExactNoWrapRegion(
+        WO->getBinaryOp(), *C, WO->getNoWrapKind());
     CmpInst::Predicate Pred;
     APInt NewRHSC, Offset;
     NWR.getEquivalentICmp(Pred, NewRHSC, Offset);
@@ -8336,7 +8357,8 @@ ScalarEvolution::ExitLimit ScalarEvoluti
       LHS = getAddExpr(LHS, getConstant(Offset));
     auto EL = computeExitLimitFromICmp(L, Pred, LHS, getConstant(NewRHSC),
                                        ControlsExit, AllowPredicates);
-    if (EL.hasAnyInfo()) return EL;
+    if (EL.hasAnyInfo())
+      return EL;
   }
 
   // If it's not an integer or pointer comparison then compute it the hard way.
@@ -8414,14 +8436,12 @@ ScalarEvolution::computeExitLimitFromCon
     MaxBECount = getConstant(getUnsignedRangeMax(BECount));
 
   return ExitLimit(BECount, MaxBECount, false,
-                   { &EL0.Predicates, &EL1.Predicates });
+                   {&EL0.Predicates, &EL1.Predicates});
 }
 
 ScalarEvolution::ExitLimit
-ScalarEvolution::computeExitLimitFromICmp(const Loop *L,
-                                          ICmpInst *ExitCond,
-                                          bool ExitIfTrue,
-                                          bool ControlsExit,
+ScalarEvolution::computeExitLimitFromICmp(const Loop *L, ICmpInst *ExitCond,
+                                          bool ExitIfTrue, bool ControlsExit,
                                           bool AllowPredicates) {
   // If the condition was exit on true, convert the condition to exit on false
   ICmpInst::Predicate Pred;
@@ -8436,10 +8456,10 @@ ScalarEvolution::computeExitLimitFromICm
 
   ExitLimit EL = computeExitLimitFromICmp(L, Pred, LHS, RHS, ControlsExit,
                                           AllowPredicates);
-  if (EL.hasAnyInfo()) return EL;
+  if (EL.hasAnyInfo())
+    return EL;
 
-  auto *ExhaustiveCount =
-      computeExitCountExhaustively(L, ExitCond, ExitIfTrue);
+  auto *ExhaustiveCount = computeExitCountExhaustively(L, ExitCond, ExitIfTrue);
 
   if (!isa<SCEVCouldNotCompute>(ExhaustiveCount))
     return ExhaustiveCount;
@@ -8447,12 +8467,9 @@ ScalarEvolution::computeExitLimitFromICm
   return computeShiftCompareExitLimit(ExitCond->getOperand(0),
                                       ExitCond->getOperand(1), L, OriginalPred);
 }
-ScalarEvolution::ExitLimit
-ScalarEvolution::computeExitLimitFromICmp(const Loop *L,
-                                          ICmpInst::Predicate Pred,
-                                          const SCEV *LHS, const SCEV *RHS,
-                                          bool ControlsExit,
-                                          bool AllowPredicates) {
+ScalarEvolution::ExitLimit ScalarEvolution::computeExitLimitFromICmp(
+    const Loop *L, ICmpInst::Predicate Pred, const SCEV *LHS, const SCEV *RHS,
+    bool ControlsExit, bool AllowPredicates) {
 
   // Try to evaluate any dependencies out of the loop.
   LHS = getSCEVAtScope(LHS, L);
@@ -8482,7 +8499,8 @@ ScalarEvolution::computeExitLimitFromICm
             ConstantRange::makeExactICmpRegion(Pred, RHSC->getAPInt());
 
         const SCEV *Ret = AddRec->getNumIterationsInRange(CompRange, *this);
-        if (!isa<SCEVCouldNotCompute>(Ret)) return Ret;
+        if (!isa<SCEVCouldNotCompute>(Ret))
+          return Ret;
       }
 
   // If this loop must exit based on this condition (or execute undefined
@@ -8498,11 +8516,11 @@ ScalarEvolution::computeExitLimitFromICm
       InnerLHS = ZExt->getOperand();
     if (const SCEVAddRecExpr *AR = dyn_cast<SCEVAddRecExpr>(InnerLHS)) {
       auto *StrideC = dyn_cast<SCEVConstant>(AR->getStepRecurrence(*this));
-      if (!AR->hasNoSelfWrap() && AR->getLoop() == L && AR->isAffine() && 
+      if (!AR->hasNoSelfWrap() && AR->getLoop() == L && AR->isAffine() &&
           StrideC && StrideC->getAPInt().isPowerOf2()) {
         auto Flags = AR->getNoWrapFlags();
         Flags = setFlags(Flags, SCEV::FlagNW);
-        SmallVector<const SCEV*> Operands{AR->operands()};
+        SmallVector<const SCEV *> Operands{AR->operands()};
         Flags = StrengthenNoWrapFlags(this, scAddRecExpr, Operands, Flags);
         setNoWrapFlags(const_cast<SCEVAddRecExpr *>(AR), Flags);
       }
@@ -8510,7 +8528,7 @@ ScalarEvolution::computeExitLimitFromICm
   }
 
   switch (Pred) {
-  case ICmpInst::ICMP_NE: {                     // while (X != Y)
+  case ICmpInst::ICMP_NE: { // while (X != Y)
     // Convert to: while (X-Y != 0)
     if (LHS->getType()->isPointerTy()) {
       LHS = getLosslessPtrToIntExpr(LHS);
@@ -8522,12 +8540,13 @@ ScalarEvolution::computeExitLimitFromICm
       if (isa<SCEVCouldNotCompute>(RHS))
         return RHS;
     }
-    ExitLimit EL = howFarToZero(getMinusSCEV(LHS, RHS), L, ControlsExit,
-                                AllowPredicates);
-    if (EL.hasAnyInfo()) return EL;
+    ExitLimit EL =
+        howFarToZero(getMinusSCEV(LHS, RHS), L, ControlsExit, AllowPredicates);
+    if (EL.hasAnyInfo())
+      return EL;
     break;
   }
-  case ICmpInst::ICMP_EQ: {                     // while (X == Y)
+  case ICmpInst::ICMP_EQ: { // while (X == Y)
     // Convert to: while (X-Y == 0)
     if (LHS->getType()->isPointerTy()) {
       LHS = getLosslessPtrToIntExpr(LHS);
@@ -8540,24 +8559,26 @@ ScalarEvolution::computeExitLimitFromICm
         return RHS;
     }
     ExitLimit EL = howFarToNonZero(getMinusSCEV(LHS, RHS), L);
-    if (EL.hasAnyInfo()) return EL;
+    if (EL.hasAnyInfo())
+      return EL;
     break;
   }
   case ICmpInst::ICMP_SLT:
-  case ICmpInst::ICMP_ULT: {                    // while (X < Y)
+  case ICmpInst::ICMP_ULT: { // while (X < Y)
     bool IsSigned = Pred == ICmpInst::ICMP_SLT;
-    ExitLimit EL = howManyLessThans(LHS, RHS, L, IsSigned, ControlsExit,
-                                    AllowPredicates);
-    if (EL.hasAnyInfo()) return EL;
+    ExitLimit EL =
+        howManyLessThans(LHS, RHS, L, IsSigned, ControlsExit, AllowPredicates);
+    if (EL.hasAnyInfo())
+      return EL;
     break;
   }
   case ICmpInst::ICMP_SGT:
-  case ICmpInst::ICMP_UGT: {                    // while (X > Y)
+  case ICmpInst::ICMP_UGT: { // while (X > Y)
     bool IsSigned = Pred == ICmpInst::ICMP_SGT;
-    ExitLimit EL =
-        howManyGreaterThans(LHS, RHS, L, IsSigned, ControlsExit,
-                            AllowPredicates);
-    if (EL.hasAnyInfo()) return EL;
+    ExitLimit EL = howManyGreaterThans(LHS, RHS, L, IsSigned, ControlsExit,
+                                       AllowPredicates);
+    if (EL.hasAnyInfo())
+      return EL;
     break;
   }
   default:
@@ -8617,9 +8638,8 @@ ScalarEvolution::ExitLimit ScalarEvoluti
 
   // Return true if V is of the form "LHS `shift_op` <positive constant>".
   // Return LHS in OutLHS and shift_opt in OutOpCode.
-  auto MatchPositiveShift =
-      [](Value *V, Value *&OutLHS, Instruction::BinaryOps &OutOpCode) {
-
+  auto MatchPositiveShift = [](Value *V, Value *&OutLHS,
+                               Instruction::BinaryOps &OutOpCode) {
     using namespace PatternMatch;
 
     ConstantInt *ShiftAmt;
@@ -8643,8 +8663,8 @@ ScalarEvolution::ExitLimit ScalarEvoluti
   //
   // Return true on a successful match.  Return the corresponding PHI node (%iv
   // above) in PNOut and the opcode of the shift operation in OpCodeOut.
-  auto MatchShiftRecurrence =
-      [&](Value *V, PHINode *&PNOut, Instruction::BinaryOps &OpCodeOut) {
+  auto MatchShiftRecurrence = [&](Value *V, PHINode *&PNOut,
+                                  Instruction::BinaryOps &OpCodeOut) {
     Optional<Instruction::BinaryOps> PostShiftOpCode;
 
     {
@@ -8747,9 +8767,9 @@ ScalarEvolution::ExitLimit ScalarEvoluti
 /// Return true if we can constant fold an instruction of the specified type,
 /// assuming that all operands were constants.
 static bool CanConstantFold(const Instruction *I) {
-  if (isa<BinaryOperator>(I) || isa<CmpInst>(I) ||
-      isa<SelectInst>(I) || isa<CastInst>(I) || isa<GetElementPtrInst>(I) ||
-      isa<LoadInst>(I) || isa<ExtractValueInst>(I))
+  if (isa<BinaryOperator>(I) || isa<CmpInst>(I) || isa<SelectInst>(I) ||
+      isa<CastInst>(I) || isa<GetElementPtrInst>(I) || isa<LoadInst>(I) ||
+      isa<ExtractValueInst>(I))
     return true;
 
   if (const CallInst *CI = dyn_cast<CallInst>(I))
@@ -8762,7 +8782,8 @@ static bool CanConstantFold(const Instru
 /// assuming its operands can all constant evolve.
 static bool canConstantEvolve(Instruction *I, const Loop *L) {
   // An instruction outside of the loop can't be derived from a loop PHI.
-  if (!L->contains(I)) return false;
+  if (!L->contains(I))
+    return false;
 
   if (isa<PHINode>(I)) {
     // We don't currently keep track of the control flow needed to evaluate
@@ -8788,10 +8809,12 @@ getConstantEvolvingPHIOperands(Instructi
   // constant or derived from a PHI node themselves.
   PHINode *PHI = nullptr;
   for (Value *Op : UseInst->operands()) {
-    if (isa<Constant>(Op)) continue;
+    if (isa<Constant>(Op))
+      continue;
 
     Instruction *OpInst = dyn_cast<Instruction>(Op);
-    if (!OpInst || !canConstantEvolve(OpInst, L)) return nullptr;
+    if (!OpInst || !canConstantEvolve(OpInst, L))
+      return nullptr;
 
     PHINode *P = dyn_cast<PHINode>(OpInst);
     if (!P)
@@ -8806,9 +8829,9 @@ getConstantEvolvingPHIOperands(Instructi
       PHIMap[OpInst] = P;
     }
     if (!P)
-      return nullptr;  // Not evolving from PHI
+      return nullptr; // Not evolving from PHI
     if (PHI && PHI != P)
-      return nullptr;  // Evolving from multiple different PHIs.
+      return nullptr; // Evolving from multiple different PHIs.
     PHI = P;
   }
   // This is a expression evolving from a constant PHI!
@@ -8822,7 +8845,8 @@ getConstantEvolvingPHIOperands(Instructi
 /// constraints, return null.
 static PHINode *getConstantEvolvingPHI(Value *V, const Loop *L) {
   Instruction *I = dyn_cast<Instruction>(V);
-  if (!I || !canConstantEvolve(I, L)) return nullptr;
+  if (!I || !canConstantEvolve(I, L))
+    return nullptr;
 
   if (PHINode *PN = dyn_cast<PHINode>(I))
     return PN;
@@ -8841,33 +8865,40 @@ static Constant *EvaluateExpression(Valu
                                     const DataLayout &DL,
                                     const TargetLibraryInfo *TLI) {
   // Convenient constant check, but redundant for recursive calls.
-  if (Constant *C = dyn_cast<Constant>(V)) return C;
+  if (Constant *C = dyn_cast<Constant>(V))
+    return C;
   Instruction *I = dyn_cast<Instruction>(V);
-  if (!I) return nullptr;
+  if (!I)
+    return nullptr;
 
-  if (Constant *C = Vals.lookup(I)) return C;
+  if (Constant *C = Vals.lookup(I))
+    return C;
 
   // An instruction inside the loop depends on a value outside the loop that we
   // weren't given a mapping for, or a value such as a call inside the loop.
-  if (!canConstantEvolve(I, L)) return nullptr;
+  if (!canConstantEvolve(I, L))
+    return nullptr;
 
   // An unmapped PHI can be due to a branch or another loop inside this loop,
   // or due to this not being the initial iteration through a loop where we
   // couldn't compute the evolution of this particular PHI last time.
-  if (isa<PHINode>(I)) return nullptr;
+  if (isa<PHINode>(I))
+    return nullptr;
 
-  std::vector<Constant*> Operands(I->getNumOperands());
+  std::vector<Constant *> Operands(I->getNumOperands());
 
   for (unsigned i = 0, e = I->getNumOperands(); i != e; ++i) {
     Instruction *Operand = dyn_cast<Instruction>(I->getOperand(i));
     if (!Operand) {
       Operands[i] = dyn_cast<Constant>(I->getOperand(i));
-      if (!Operands[i]) return nullptr;
+      if (!Operands[i])
+        return nullptr;
       continue;
     }
     Constant *C = EvaluateExpression(Operand, L, Vals, DL, TLI);
     Vals[Operand] = C;
-    if (!C) return nullptr;
+    if (!C)
+      return nullptr;
     Operands[i] = C;
   }
 
@@ -8881,7 +8912,6 @@ static Constant *EvaluateExpression(Valu
   return ConstantFoldInstOperands(I, Operands, DL, TLI);
 }
 
-
 // If every incoming value to PN except the one for BB is a specific Constant,
 // return that, else return nullptr.
 static Constant *getOtherIncomingValue(PHINode *PN, BasicBlock *BB) {
@@ -8909,16 +8939,16 @@ static Constant *getOtherIncomingValue(P
 /// in the header of its containing loop, we know the loop executes a
 /// constant number of times, and the PHI node is just a recurrence
 /// involving constants, fold it.
-Constant *
-ScalarEvolution::getConstantEvolutionLoopExitValue(PHINode *PN,
-                                                   const APInt &BEs,
-                                                   const Loop *L) {
+Constant *ScalarEvolution::getConstantEvolutionLoopExitValue(PHINode *PN,
+                                                             const APInt &BEs,
+                                                             const Loop *L) {
   auto I = ConstantEvolutionLoopExitValue.find(PN);
   if (I != ConstantEvolutionLoopExitValue.end())
     return I->second;
 
   if (BEs.ugt(MaxBruteForceIterations))
-    return ConstantEvolutionLoopExitValue[PN] = nullptr;  // Not going to evaluate it.
+    return ConstantEvolutionLoopExitValue[PN] =
+               nullptr; // Not going to evaluate it.
 
   Constant *&RetVal = ConstantEvolutionLoopExitValue[PN];
 
@@ -8946,9 +8976,9 @@ ScalarEvolution::getConstantEvolutionLoo
   unsigned NumIterations = BEs.getZExtValue(); // must be in range
   unsigned IterationNum = 0;
   const DataLayout &DL = getDataLayout();
-  for (; ; ++IterationNum) {
+  for (;; ++IterationNum) {
     if (IterationNum == NumIterations)
-      return RetVal = CurrentIterVals[PN];  // Got exit value!
+      return RetVal = CurrentIterVals[PN]; // Got exit value!
 
     // Compute the value of the PHIs for the next iteration.
     // EvaluateExpression adds non-phi values to the CurrentIterVals map.
@@ -8956,7 +8986,7 @@ ScalarEvolution::getConstantEvolutionLoo
     Constant *NextPHI =
         EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);
     if (!NextPHI)
-      return nullptr;        // Couldn't evaluate!
+      return nullptr; // Couldn't evaluate!
     NextIterVals[PN] = NextPHI;
 
     bool StoppedEvolving = NextPHI == CurrentIterVals[PN];
@@ -8967,7 +8997,8 @@ ScalarEvolution::getConstantEvolutionLoo
     SmallVector<std::pair<PHINode *, Constant *>, 8> PHIsToCompute;
     for (const auto &I : CurrentIterVals) {
       PHINode *PHI = dyn_cast<PHINode>(I.first);
-      if (!PHI || PHI == PN || PHI->getParent() != Header) continue;
+      if (!PHI || PHI == PN || PHI->getParent() != Header)
+        continue;
       PHIsToCompute.emplace_back(PHI, I.second);
     }
     // We use two distinct loops because EvaluateExpression may invalidate any
@@ -8975,7 +9006,7 @@ ScalarEvolution::getConstantEvolutionLoo
     for (const auto &I : PHIsToCompute) {
       PHINode *PHI = I.first;
       Constant *&NextPHI = NextIterVals[PHI];
-      if (!NextPHI) {   // Not already computed.
+      if (!NextPHI) { // Not already computed.
         Value *BEValue = PHI->getIncomingValueForBlock(Latch);
         NextPHI = EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);
       }
@@ -8996,11 +9027,13 @@ const SCEV *ScalarEvolution::computeExit
                                                           Value *Cond,
                                                           bool ExitWhen) {
   PHINode *PN = getConstantEvolvingPHI(Cond, L);
-  if (!PN) return getCouldNotCompute();
+  if (!PN)
+    return getCouldNotCompute();
 
   // If the loop is canonicalized, the PHI will have exactly two entries.
   // That's the only form we support here.
-  if (PN->getNumIncomingValues() != 2) return getCouldNotCompute();
+  if (PN->getNumIncomingValues() != 2)
+    return getCouldNotCompute();
 
   DenseMap<Instruction *, Constant *> CurrentIterVals;
   BasicBlock *Header = L->getHeader();
@@ -9019,14 +9052,16 @@ const SCEV *ScalarEvolution::computeExit
   // Okay, we find a PHI node that defines the trip count of this loop.  Execute
   // the loop symbolically to determine when the condition gets a value of
   // "ExitWhen".
-  unsigned MaxIterations = MaxBruteForceIterations;   // Limit analysis.
+  unsigned MaxIterations = MaxBruteForceIterations; // Limit analysis.
   const DataLayout &DL = getDataLayout();
-  for (unsigned IterationNum = 0; IterationNum != MaxIterations;++IterationNum){
+  for (unsigned IterationNum = 0; IterationNum != MaxIterations;
+       ++IterationNum) {
     auto *CondVal = dyn_cast_or_null<ConstantInt>(
         EvaluateExpression(Cond, L, CurrentIterVals, DL, &TLI));
 
     // Couldn't symbolically evaluate.
-    if (!CondVal) return getCouldNotCompute();
+    if (!CondVal)
+      return getCouldNotCompute();
 
     if (CondVal->getValue() == uint64_t(ExitWhen)) {
       ++NumBruteForceTripCountsComputed;
@@ -9042,12 +9077,14 @@ const SCEV *ScalarEvolution::computeExit
     SmallVector<PHINode *, 8> PHIsToCompute;
     for (const auto &I : CurrentIterVals) {
       PHINode *PHI = dyn_cast<PHINode>(I.first);
-      if (!PHI || PHI->getParent() != Header) continue;
+      if (!PHI || PHI->getParent() != Header)
+        continue;
       PHIsToCompute.push_back(PHI);
     }
     for (PHINode *PHI : PHIsToCompute) {
       Constant *&NextPHI = NextIterVals[PHI];
-      if (NextPHI) continue;    // Already computed!
+      if (NextPHI)
+        continue; // Already computed!
 
       Value *BEValue = PHI->getIncomingValueForBlock(Latch);
       NextPHI = EvaluateExpression(BEValue, L, CurrentIterVals, DL, &TLI);
@@ -9193,7 +9230,8 @@ static Constant *BuildConstantFromSCEV(c
 }
 
 const SCEV *ScalarEvolution::computeSCEVAtScope(const SCEV *V, const Loop *L) {
-  if (isa<SCEVConstant>(V)) return V;
+  if (isa<SCEVConstant>(V))
+    return V;
 
   // If this instruction is evolved from a constant-evolving PHI, compute the
   // exit value from the loop without using SCEVs.
@@ -9245,7 +9283,8 @@ const SCEV *ScalarEvolution::computeSCEV
             // the specified iteration number.
             Constant *RV = getConstantEvolutionLoopExitValue(
                 PN, BTCC->getAPInt(), CurrLoop);
-            if (RV) return getSCEV(RV);
+            if (RV)
+              return getSCEV(RV);
           }
         }
 
@@ -9256,7 +9295,8 @@ const SCEV *ScalarEvolution::computeSCEV
           const SCEV *InputAtScope = getSCEVAtScope(Input, L);
           // TODO: We can generalize it using LI.replacementPreservesLCSSAForm,
           // for the simplest case just support constants.
-          if (isa<SCEVConstant>(InputAtScope)) return InputAtScope;
+          if (isa<SCEVConstant>(InputAtScope))
+            return InputAtScope;
         }
       }
 
@@ -9284,12 +9324,12 @@ const SCEV *ScalarEvolution::computeSCEV
           MadeImprovement |= OrigV != OpV;
 
           Constant *C = BuildConstantFromSCEV(OpV);
-          if (!C) return V;
+          if (!C)
+            return V;
           if (C->getType() != Op->getType())
-            C = ConstantExpr::getCast(CastInst::getCastOpcode(C, false,
-                                                              Op->getType(),
-                                                              false),
-                                      C, Op->getType());
+            C = ConstantExpr::getCast(
+                CastInst::getCastOpcode(C, false, Op->getType(), false), C,
+                Op->getType());
           Operands.push_back(C);
         }
 
@@ -9306,7 +9346,8 @@ const SCEV *ScalarEvolution::computeSCEV
                                                DL);
           } else
             C = ConstantFoldInstOperands(I, Operands, DL, &TLI);
-          if (!C) return V;
+          if (!C)
+            return V;
           return getSCEV(C);
         }
       }
@@ -9326,7 +9367,7 @@ const SCEV *ScalarEvolution::computeSCEV
         // Okay, at least one of these operands is loop variant but might be
         // foldable.  Build a new instance of the folded commutative expression.
         SmallVector<const SCEV *, 8> NewOps(Comm->op_begin(),
-                                            Comm->op_begin()+i);
+                                            Comm->op_begin() + i);
         NewOps.push_back(OpAtScope);
 
         for (++i; i != e; ++i) {
@@ -9352,7 +9393,7 @@ const SCEV *ScalarEvolution::computeSCEV
     const SCEV *LHS = getSCEVAtScope(Div->getLHS(), L);
     const SCEV *RHS = getSCEVAtScope(Div->getRHS(), L);
     if (LHS == Div->getLHS() && RHS == Div->getRHS())
-      return Div;   // must be loop invariant
+      return Div; // must be loop invariant
     return getUDivExpr(LHS, RHS);
   }
 
@@ -9370,14 +9411,13 @@ const SCEV *ScalarEvolution::computeSCEV
       // Okay, at least one of these operands is loop variant but might be
       // foldable.  Build a new instance of the folded commutative expression.
       SmallVector<const SCEV *, 8> NewOps(AddRec->op_begin(),
-                                          AddRec->op_begin()+i);
+                                          AddRec->op_begin() + i);
       NewOps.push_back(OpAtScope);
       for (++i; i != e; ++i)
         NewOps.push_back(getSCEVAtScope(AddRec->getOperand(i), L));
 
-      const SCEV *FoldedRec =
-        getAddRecExpr(NewOps, AddRec->getLoop(),
-                      AddRec->getNoWrapFlags(SCEV::FlagNW));
+      const SCEV *FoldedRec = getAddRecExpr(
+          NewOps, AddRec->getLoop(), AddRec->getNoWrapFlags(SCEV::FlagNW));
       AddRec = dyn_cast<SCEVAddRecExpr>(FoldedRec);
       // The addrec may be folded to a nonrecurrence, for example, if the
       // induction variable is multiplied by zero after constant folding. Go
@@ -9393,7 +9433,8 @@ const SCEV *ScalarEvolution::computeSCEV
       // To evaluate this recurrence, we need to know how many times the AddRec
       // loop iterates.  Compute this now.
       const SCEV *BackedgeTakenCount = getBackedgeTakenCount(AddRec->getLoop());
-      if (BackedgeTakenCount == getCouldNotCompute()) return AddRec;
+      if (BackedgeTakenCount == getCouldNotCompute())
+        return AddRec;
 
       // Then, evaluate the AddRec.
       return AddRec->evaluateAtIteration(BackedgeTakenCount, *this);
@@ -9405,7 +9446,7 @@ const SCEV *ScalarEvolution::computeSCEV
   if (const SCEVCastExpr *Cast = dyn_cast<SCEVCastExpr>(V)) {
     const SCEV *Op = getSCEVAtScope(Cast->getOperand(), L);
     if (Op == Cast->getOperand())
-      return Cast;  // must be loop invariant
+      return Cast; // must be loop invariant
     return getCastExpr(Cast->getSCEVType(), Op, Cast->getType());
   }
 
@@ -9433,7 +9474,7 @@ const SCEV *ScalarEvolution::stripInject
 ///
 /// If the equation does not have a solution, SCEVCouldNotCompute is returned.
 static const SCEV *SolveLinEquationWithOverflow(const APInt &A, const SCEV *B,
-                                               ScalarEvolution &SE) {
+                                                ScalarEvolution &SE) {
   uint32_t BW = A.getBitWidth();
   assert(BW == SE.getTypeSizeInBits(B->getType()));
   assert(A != 0 && "A must be non-zero.");
@@ -9458,9 +9499,9 @@ static const SCEV *SolveLinEquationWithO
   // If D == 1, (N / D) == N == 2^BW, so we need one extra bit to represent
   // (N / D) in general. The inverse itself always fits into BW bits, though,
   // so we immediately truncate it.
-  APInt AD = A.lshr(Mult2).zext(BW + 1);  // AD = A / D
+  APInt AD = A.lshr(Mult2).zext(BW + 1); // AD = A / D
   APInt Mod(BW + 1, 0);
-  Mod.setBit(BW - Mult2);  // Mod = N / D
+  Mod.setBit(BW - Mult2); // Mod = N / D
   APInt I = AD.multiplicativeInverse(Mod).trunc(BW);
 
   // 4. Compute the minimum unsigned root of the equation:
@@ -9486,8 +9527,8 @@ GetQuadraticEquation(const SCEVAddRecExp
   const SCEVConstant *LC = dyn_cast<SCEVConstant>(AddRec->getOperand(0));
   const SCEVConstant *MC = dyn_cast<SCEVConstant>(AddRec->getOperand(1));
   const SCEVConstant *NC = dyn_cast<SCEVConstant>(AddRec->getOperand(2));
-  LLVM_DEBUG(dbgs() << __func__ << ": analyzing quadratic addrec: "
-                    << *AddRec << '\n');
+  LLVM_DEBUG(dbgs() << __func__ << ": analyzing quadratic addrec: " << *AddRec
+                    << '\n');
 
   // We currently can only solve this if the coefficients are constants.
   if (!LC || !MC || !NC) {
@@ -9502,8 +9543,7 @@ GetQuadraticEquation(const SCEVAddRecExp
 
   unsigned BitWidth = LC->getAPInt().getBitWidth();
   unsigned NewWidth = BitWidth + 1;
-  LLVM_DEBUG(dbgs() << __func__ << ": addrec coeff bw: "
-                    << BitWidth << '\n');
+  LLVM_DEBUG(dbgs() << __func__ << ": addrec coeff bw: " << BitWidth << '\n');
   // The sign-extension (as opposed to a zero-extension) here matches the
   // extension used in SolveQuadraticEquationWrap (with the same motivation).
   N = N.sext(NewWidth);
@@ -9524,9 +9564,9 @@ GetQuadraticEquation(const SCEVAddRecExp
   APInt B = 2 * M - A;
   APInt C = 2 * L;
   APInt T = APInt(NewWidth, 2);
-  LLVM_DEBUG(dbgs() << __func__ << ": equation " << A << "x^2 + " << B
-                    << "x + " << C << ", coeff bw: " << NewWidth
-                    << ", multiplied by " << T << '\n');
+  LLVM_DEBUG(dbgs() << __func__ << ": equation " << A << "x^2 + " << B << "x + "
+                    << C << ", coeff bw: " << NewWidth << ", multiplied by "
+                    << T << '\n');
   return std::make_tuple(A, B, C, T, BitWidth);
 }
 
@@ -9580,8 +9620,8 @@ static Optional<APInt> TruncIfPossible(O
 /// (b) SolveQuadraticEquationWrap was unable to find a solution. For cases
 ///     like x^2 = 5, no integer solutions exist, in other cases an integer
 ///     solution may exist, but SolveQuadraticEquationWrap may fail to find it.
-static Optional<APInt>
-SolveQuadraticAddRecExact(const SCEVAddRecExpr *AddRec, ScalarEvolution &SE) {
+static Optional<APInt> SolveQuadraticAddRecExact(const SCEVAddRecExpr *AddRec,
+                                                 ScalarEvolution &SE) {
   APInt A, B, C, M;
   unsigned BitWidth;
   auto T = GetQuadraticEquation(AddRec);
@@ -9590,7 +9630,8 @@ SolveQuadraticAddRecExact(const SCEVAddR
 
   std::tie(A, B, C, M, BitWidth) = *T;
   LLVM_DEBUG(dbgs() << __func__ << ": solving for unsigned overflow\n");
-  Optional<APInt> X = APIntOps::SolveQuadraticEquationWrap(A, B, C, BitWidth+1);
+  Optional<APInt> X =
+      APIntOps::SolveQuadraticEquationWrap(A, B, C, BitWidth + 1);
   if (!X.hasValue())
     return None;
 
@@ -9612,9 +9653,9 @@ SolveQuadraticAddRecExact(const SCEVAddR
 /// (a) the addrec coefficients are not constant, or
 /// (b) SolveQuadraticEquationWrap was unable to find a solution for the
 ///     bounds of the range.
-static Optional<APInt>
-SolveQuadraticAddRecRange(const SCEVAddRecExpr *AddRec,
-                          const ConstantRange &Range, ScalarEvolution &SE) {
+static Optional<APInt> SolveQuadraticAddRecRange(const SCEVAddRecExpr *AddRec,
+                                                 const ConstantRange &Range,
+                                                 ScalarEvolution &SE) {
   assert(AddRec->getOperand(0)->isZero() &&
          "Starting value of addrec should be 0");
   LLVM_DEBUG(dbgs() << __func__ << ": solving boundary crossing for range "
@@ -9638,7 +9679,7 @@ SolveQuadraticAddRecRange(const SCEVAddR
   // cannot make any conclusions.
   // Return a pair: the optional solution and a flag indicating if the
   // solution was found.
-  auto SolveForBoundary = [&](APInt Bound) -> std::pair<Optional<APInt>,bool> {
+  auto SolveForBoundary = [&](APInt Bound) -> std::pair<Optional<APInt>, bool> {
     // Solve for signed overflow and unsigned overflow, pick the lower
     // solution.
     LLVM_DEBUG(dbgs() << "SolveQuadraticAddRecRange: checking boundary "
@@ -9653,16 +9694,16 @@ SolveQuadraticAddRecRange(const SCEVAddR
     }
     LLVM_DEBUG(dbgs() << "SolveQuadraticAddRecRange: solving for "
                          "unsigned overflow\n");
-    Optional<APInt> UO = APIntOps::SolveQuadraticEquationWrap(A, B, -Bound,
-                                                              BitWidth+1);
+    Optional<APInt> UO =
+        APIntOps::SolveQuadraticEquationWrap(A, B, -Bound, BitWidth + 1);
 
-    auto LeavesRange = [&] (const APInt &X) {
+    auto LeavesRange = [&](const APInt &X) {
       ConstantInt *C0 = ConstantInt::get(SE.getContext(), X);
       ConstantInt *V0 = EvaluateConstantChrecAtConstant(AddRec, C0, SE);
       if (Range.contains(V0->getValue()))
         return false;
       // X should be at least 1, so X-1 is non-negative.
-      ConstantInt *C1 = ConstantInt::get(SE.getContext(), X-1);
+      ConstantInt *C1 = ConstantInt::get(SE.getContext(), X - 1);
       ConstantInt *V1 = EvaluateConstantChrecAtConstant(AddRec, C1, SE);
       if (Range.contains(V1->getValue()))
         return true;
@@ -9673,19 +9714,19 @@ SolveQuadraticAddRecRange(const SCEVAddR
     // be a solution, but the function failed to find it. We cannot treat it
     // as "no solution".
     if (!SO.hasValue() || !UO.hasValue())
-      return { None, false };
+      return {None, false};
 
     // Check the smaller value first to see if it leaves the range.
     // At this point, both SO and UO must have values.
     Optional<APInt> Min = MinOptional(SO, UO);
     if (LeavesRange(*Min))
-      return { Min, true };
+      return {Min, true};
     Optional<APInt> Max = Min == SO ? UO : SO;
     if (LeavesRange(*Max))
-      return { Max, true };
+      return {Max, true};
 
     // Solutions were found, but were eliminated, hence the "true".
-    return { None, true };
+    return {None, true};
   };
 
   std::tie(A, B, C, M, BitWidth) = *T;
@@ -9740,9 +9781,10 @@ SolveQuadraticAddRecRange(const SCEVAddR
   return TruncIfPossible(MinOptional(SL.first, SU.first), BitWidth);
 }
 
-ScalarEvolution::ExitLimit
-ScalarEvolution::howFarToZero(const SCEV *V, const Loop *L, bool ControlsExit,
-                              bool AllowPredicates) {
+ScalarEvolution::ExitLimit ScalarEvolution::howFarToZero(const SCEV *V,
+                                                         const Loop *L,
+                                                         bool ControlsExit,
+                                                         bool AllowPredicates) {
 
   // This is only used for loops with a "x != y" exit test. The exit condition
   // is now expressed as a single expression, V = x-y. So the exit test is
@@ -9753,8 +9795,9 @@ ScalarEvolution::howFarToZero(const SCEV
   // If the value is a constant
   if (const SCEVConstant *C = dyn_cast<SCEVConstant>(V)) {
     // If the value is already zero, the branch will execute zero times.
-    if (C->getValue()->isZero()) return C;
-    return getCouldNotCompute();  // Otherwise it will loop infinitely.
+    if (C->getValue()->isZero())
+      return C;
+    return getCouldNotCompute(); // Otherwise it will loop infinitely.
   }
 
   const SCEVAddRecExpr *AddRec =
@@ -9826,9 +9869,9 @@ ScalarEvolution::howFarToZero(const SCEV
     APInt MaxBECount = getUnsignedRangeMax(applyLoopGuards(Distance, L));
     MaxBECount = APIntOps::umin(MaxBECount, getUnsignedRangeMax(Distance));
 
-    // When a loop like "for (int i = 0; i != n; ++i) { /* body */ }" is rotated,
-    // we end up with a loop whose backedge-taken count is n - 1.  Detect this
-    // case, and see if we can improve the bound.
+    // When a loop like "for (int i = 0; i != n; ++i) { /* body */ }" is
+    // rotated, we end up with a loop whose backedge-taken count is n - 1.
+    // Detect this case, and see if we can improve the bound.
     //
     // Explicitly handling this here is necessary because getUnsignedRange
     // isn't context-sensitive; it doesn't know that we only care about the
@@ -9874,8 +9917,8 @@ ScalarEvolution::howFarToZero(const SCEV
   return ExitLimit(E, M, false, Predicates);
 }
 
-ScalarEvolution::ExitLimit
-ScalarEvolution::howFarToNonZero(const SCEV *V, const Loop *L) {
+ScalarEvolution::ExitLimit ScalarEvolution::howFarToNonZero(const SCEV *V,
+                                                            const Loop *L) {
   // Loops that look like: while (X == 0) are very strange indeed.  We don't
   // handle them yet except for the trivial case.  This could be expanded in the
   // future as needed.
@@ -9885,7 +9928,7 @@ ScalarEvolution::howFarToNonZero(const S
   if (const SCEVConstant *C = dyn_cast<SCEVConstant>(V)) {
     if (!C->getValue()->isZero())
       return getZero(C->getType());
-    return getCouldNotCompute();  // Otherwise it will loop infinitely.
+    return getCouldNotCompute(); // Otherwise it will loop infinitely.
   }
 
   // We could implement others, but I really doubt anyone writes loops like
@@ -9894,8 +9937,8 @@ ScalarEvolution::howFarToNonZero(const S
 }
 
 std::pair<const BasicBlock *, const BasicBlock *>
-ScalarEvolution::getPredecessorWithUniqueSuccessorForBB(const BasicBlock *BB)
-    const {
+ScalarEvolution::getPredecessorWithUniqueSuccessorForBB(
+    const BasicBlock *BB) const {
   // If the block has a unique predecessor, then there is no path from the
   // predecessor to the block that does not go through the direct edge
   // from the predecessor to the block.
@@ -9917,13 +9960,15 @@ ScalarEvolution::getPredecessorWithUniqu
 /// front-end may have replicated the controlling expression.
 static bool HasSameValue(const SCEV *A, const SCEV *B) {
   // Quick check to see if they are the same SCEV.
-  if (A == B) return true;
+  if (A == B)
+    return true;
 
   auto ComputesEqualValues = [](const Instruction *A, const Instruction *B) {
     // Not all instructions that are "identical" compute the same value.  For
     // instance, two distinct alloca instructions allocating the same type are
     // identical and do not read memory; but compute distinct values.
-    return A->isIdenticalTo(B) && (isa<BinaryOperator>(A) || isa<GetElementPtrInst>(A));
+    return A->isIdenticalTo(B) &&
+           (isa<BinaryOperator>(A) || isa<GetElementPtrInst>(A));
   };
 
   // Otherwise, if they're both SCEVUnknown, it's possible that they hold
@@ -9959,9 +10004,8 @@ bool ScalarEvolution::SimplifyICmpOperan
   if (const SCEVConstant *LHSC = dyn_cast<SCEVConstant>(LHS)) {
     // Check for both operands constant.
     if (const SCEVConstant *RHSC = dyn_cast<SCEVConstant>(RHS)) {
-      if (ConstantExpr::getICmp(Pred,
-                                LHSC->getValue(),
-                                RHSC->getValue())->isNullValue())
+      if (ConstantExpr::getICmp(Pred, LHSC->getValue(), RHSC->getValue())
+              ->isNullValue())
         return TrivialCase(false);
       else
         return TrivialCase(true);
@@ -10028,7 +10072,6 @@ bool ScalarEvolution::SimplifyICmpOperan
               }
         break;
 
-
         // The "Should have been caught earlier!" messages refer to the fact
         // that the ExactCR.isFullSet() or ExactCR.isEmptySet() check above
         // should have fired on the corresponding cases, and canonicalized the
@@ -10080,8 +10123,8 @@ bool ScalarEvolution::SimplifyICmpOperan
   switch (Pred) {
   case ICmpInst::ICMP_SLE:
     if (ControllingFiniteLoop || !getSignedRangeMax(RHS).isMaxSignedValue()) {
-      RHS = getAddExpr(getConstant(RHS->getType(), 1, true), RHS,
-                       SCEV::FlagNSW);
+      RHS =
+          getAddExpr(getConstant(RHS->getType(), 1, true), RHS, SCEV::FlagNSW);
       Pred = ICmpInst::ICMP_SLT;
       Changed = true;
     } else if (!getSignedRangeMin(LHS).isMinSignedValue()) {
@@ -10098,16 +10141,16 @@ bool ScalarEvolution::SimplifyICmpOperan
       Pred = ICmpInst::ICMP_SGT;
       Changed = true;
     } else if (!getSignedRangeMax(LHS).isMaxSignedValue()) {
-      LHS = getAddExpr(getConstant(RHS->getType(), 1, true), LHS,
-                       SCEV::FlagNSW);
+      LHS =
+          getAddExpr(getConstant(RHS->getType(), 1, true), LHS, SCEV::FlagNSW);
       Pred = ICmpInst::ICMP_SGT;
       Changed = true;
     }
     break;
   case ICmpInst::ICMP_ULE:
     if (ControllingFiniteLoop || !getUnsignedRangeMax(RHS).isMaxValue()) {
-      RHS = getAddExpr(getConstant(RHS->getType(), 1, true), RHS,
-                       SCEV::FlagNUW);
+      RHS =
+          getAddExpr(getConstant(RHS->getType(), 1, true), RHS, SCEV::FlagNUW);
       Pred = ICmpInst::ICMP_ULT;
       Changed = true;
     } else if (!getUnsignedRangeMin(LHS).isMinValue()) {
@@ -10122,8 +10165,8 @@ bool ScalarEvolution::SimplifyICmpOperan
       Pred = ICmpInst::ICMP_UGT;
       Changed = true;
     } else if (!getUnsignedRangeMax(LHS).isMaxValue()) {
-      LHS = getAddExpr(getConstant(RHS->getType(), 1, true), LHS,
-                       SCEV::FlagNUW);
+      LHS =
+          getAddExpr(getConstant(RHS->getType(), 1, true), LHS, SCEV::FlagNUW);
       Pred = ICmpInst::ICMP_UGT;
       Changed = true;
     }
@@ -10168,11 +10211,11 @@ ScalarEvolution::SplitIntoInitAndPostInc
   // Compute SCEV on entry of loop L.
   const SCEV *Start = SCEVInitRewriter::rewrite(S, L, *this);
   if (Start == getCouldNotCompute())
-    return { Start, Start };
+    return {Start, Start};
   // Compute post increment SCEV for loop L.
   const SCEV *PostInc = SCEVPostIncRewriter::rewrite(S, L, *this);
   assert(PostInc != getCouldNotCompute() && "Unexpected could not compute");
-  return { Start, PostInc };
+  return {Start, PostInc};
 }
 
 bool ScalarEvolution::isKnownViaInduction(ICmpInst::Predicate Pred,
@@ -10185,7 +10228,7 @@ bool ScalarEvolution::isKnownViaInductio
   if (LoopsUsed.empty())
     return false;
 
-  // Domination relationship must be a linear order on collected loops.
+    // Domination relationship must be a linear order on collected loops.
 #ifndef NDEBUG
   for (auto *L1 : LoopsUsed)
     for (auto *L2 : LoopsUsed)
@@ -10194,24 +10237,23 @@ bool ScalarEvolution::isKnownViaInductio
              "Domination relationship is not a linear order");
 #endif
 
-  const Loop *MDL =
-      *std::max_element(LoopsUsed.begin(), LoopsUsed.end(),
-                        [&](const Loop *L1, const Loop *L2) {
-         return DT.properlyDominates(L1->getHeader(), L2->getHeader());
-       });
+  const Loop *MDL = *std::max_element(
+      LoopsUsed.begin(), LoopsUsed.end(), [&](const Loop *L1, const Loop *L2) {
+        return DT.properlyDominates(L1->getHeader(), L2->getHeader());
+      });
 
   // Get init and post increment value for LHS.
   auto SplitLHS = SplitIntoInitAndPostInc(MDL, LHS);
   // if LHS contains unknown non-invariant SCEV then bail out.
   if (SplitLHS.first == getCouldNotCompute())
     return false;
-  assert (SplitLHS.second != getCouldNotCompute() && "Unexpected CNC");
+  assert(SplitLHS.second != getCouldNotCompute() && "Unexpected CNC");
   // Get init and post increment value for RHS.
   auto SplitRHS = SplitIntoInitAndPostInc(MDL, RHS);
   // if RHS contains unknown non-invariant SCEV then bail out.
   if (SplitRHS.first == getCouldNotCompute())
     return false;
-  assert (SplitRHS.second != getCouldNotCompute() && "Unexpected CNC");
+  assert(SplitRHS.second != getCouldNotCompute() && "Unexpected CNC");
   // It is possible that init SCEV contains an invariant load but it does
   // not dominate MDL and is not available at MDL loop entry, so we should
   // check it here.
@@ -10627,19 +10669,19 @@ bool ScalarEvolution::isImpliedViaGuard(
 /// isLoopBackedgeGuardedByCond - Test whether the backedge of the loop is
 /// protected by a conditional between LHS and RHS.  This is used to
 /// to eliminate casts.
-bool
-ScalarEvolution::isLoopBackedgeGuardedByCond(const Loop *L,
-                                             ICmpInst::Predicate Pred,
-                                             const SCEV *LHS, const SCEV *RHS) {
+bool ScalarEvolution::isLoopBackedgeGuardedByCond(const Loop *L,
+                                                  ICmpInst::Predicate Pred,
+                                                  const SCEV *LHS,
+                                                  const SCEV *RHS) {
   // Interpret a null as meaning no loop, where there is obviously no guard
   // (interprocedural conditions notwithstanding).
-  if (!L) return true;
+  if (!L)
+    return true;
 
   if (VerifyIR)
     assert(!verifyFunction(*L->getHeader()->getParent(), &dbgs()) &&
            "This cannot be done on broken IR!");
 
-
   if (isKnownViaNonRecursiveReasoning(Pred, LHS, RHS))
     return true;
 
@@ -10648,10 +10690,9 @@ ScalarEvolution::isLoopBackedgeGuardedBy
     return false;
 
   BranchInst *LoopContinuePredicate =
-    dyn_cast<BranchInst>(Latch->getTerminator());
+      dyn_cast<BranchInst>(Latch->getTerminator());
   if (LoopContinuePredicate && LoopContinuePredicate->isConditional() &&
-      isImpliedCond(Pred, LHS, RHS,
-                    LoopContinuePredicate->getCondition(),
+      isImpliedCond(Pred, LHS, RHS, LoopContinuePredicate->getCondition(),
                     LoopContinuePredicate->getSuccessor(0) != L->getHeader()))
     return true;
 
@@ -10672,7 +10713,7 @@ ScalarEvolution::isLoopBackedgeGuardedBy
     Type *Ty = LatchBECount->getType();
     auto NoWrapFlags = SCEV::NoWrapFlags(SCEV::FlagNUW | SCEV::FlagNW);
     const SCEV *LoopCounter =
-      getAddRecExpr(getZero(Ty), getOne(Ty), L, NoWrapFlags);
+        getAddRecExpr(getZero(Ty), getOne(Ty), L, NoWrapFlags);
     if (isImpliedCond(Pred, LHS, RHS, ICmpInst::ICMP_ULT, LoopCounter,
                       LatchBECount))
       return true;
@@ -10756,7 +10797,7 @@ bool ScalarEvolution::isBasicBlockEntryG
   bool ProvedNonEquality = false;
 
   auto SplitAndProve =
-    [&](std::function<bool(ICmpInst::Predicate)> Fn) -> bool {
+      [&](std::function<bool(ICmpInst::Predicate)> Fn) -> bool {
     if (!ProvedNonStrictComparison)
       ProvedNonStrictComparison = Fn(NonStrictPredicate);
     if (!ProvedNonEquality)
@@ -10803,9 +10844,9 @@ bool ScalarEvolution::isBasicBlockEntryG
     return false;
   };
 
-  // Starting at the block's predecessor, climb up the predecessor chain, as long
-  // as there are predecessors that can be found that have unique successors
-  // leading to the original block.
+  // Starting at the block's predecessor, climb up the predecessor chain, as
+  // long as there are predecessors that can be found that have unique
+  // successors leading to the original block.
   const Loop *ContainingLoop = LI.getLoopFor(BB);
   const BasicBlock *PredBB;
   if (ContainingLoop && ContainingLoop->getHeader() == BB)
@@ -10819,8 +10860,7 @@ bool ScalarEvolution::isBasicBlockEntryG
 
     const BranchInst *LoopEntryPredicate =
         dyn_cast<BranchInst>(Pair.first->getTerminator());
-    if (!LoopEntryPredicate ||
-        LoopEntryPredicate->isUnconditional())
+    if (!LoopEntryPredicate || LoopEntryPredicate->isUnconditional())
       continue;
 
     if (ProveViaCond(LoopEntryPredicate->getCondition(),
@@ -10892,7 +10932,8 @@ bool ScalarEvolution::isImpliedCond(ICmp
   }
 
   const ICmpInst *ICI = dyn_cast<ICmpInst>(FoundCondValue);
-  if (!ICI) return false;
+  if (!ICI)
+    return false;
 
   // Now that we found a conditional branch that dominates the loop or controls
   // the loop latch. Check to see if it is the comparison we are looking for.
@@ -10948,8 +10989,9 @@ bool ScalarEvolution::isImpliedCond(ICmp
       RHS = getZeroExtendExpr(RHS, FoundLHS->getType());
     }
   } else if (getTypeSizeInBits(LHS->getType()) >
-      getTypeSizeInBits(FoundLHS->getType())) {
-    if (FoundLHS->getType()->isPointerTy() || FoundRHS->getType()->isPointerTy())
+             getTypeSizeInBits(FoundLHS->getType())) {
+    if (FoundLHS->getType()->isPointerTy() ||
+        FoundRHS->getType()->isPointerTy())
       return false;
     if (CmpInst::isSigned(FoundPred)) {
       FoundLHS = getSignExtendExpr(FoundLHS, LHS->getType());
@@ -11093,8 +11135,8 @@ bool ScalarEvolution::isImpliedCondBalan
     // range we consider has to correspond to same signedness as the
     // predicate we're interested in folding.
 
-    APInt Min = ICmpInst::isSigned(Pred) ?
-        getSignedRangeMin(V) : getUnsignedRangeMin(V);
+    APInt Min = ICmpInst::isSigned(Pred) ? getSignedRangeMin(V)
+                                         : getUnsignedRangeMin(V);
 
     if (Min == C->getAPInt()) {
       // Given (V >= Min && V != Min) we conclude V >= (Min + 1).
@@ -11104,48 +11146,49 @@ bool ScalarEvolution::isImpliedCondBalan
       APInt SharperMin = Min + 1;
 
       switch (Pred) {
-        case ICmpInst::ICMP_SGE:
-        case ICmpInst::ICMP_UGE:
-          // We know V `Pred` SharperMin.  If this implies LHS `Pred`
-          // RHS, we're done.
-          if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(SharperMin),
-                                    CtxI))
-            return true;
-          LLVM_FALLTHROUGH;
+      case ICmpInst::ICMP_SGE:
+      case ICmpInst::ICMP_UGE:
+        // We know V `Pred` SharperMin.  If this implies LHS `Pred`
+        // RHS, we're done.
+        if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(SharperMin),
+                                  CtxI))
+          return true;
+        LLVM_FALLTHROUGH;
 
-        case ICmpInst::ICMP_SGT:
-        case ICmpInst::ICMP_UGT:
-          // We know from the range information that (V `Pred` Min ||
-          // V == Min).  We know from the guarding condition that !(V
-          // == Min).  This gives us
-          //
-          //       V `Pred` Min || V == Min && !(V == Min)
-          //   =>  V `Pred` Min
-          //
-          // If V `Pred` Min implies LHS `Pred` RHS, we're done.
+      case ICmpInst::ICMP_SGT:
+      case ICmpInst::ICMP_UGT:
+        // We know from the range information that (V `Pred` Min ||
+        // V == Min).  We know from the guarding condition that !(V
+        // == Min).  This gives us
+        //
+        //       V `Pred` Min || V == Min && !(V == Min)
+        //   =>  V `Pred` Min
+        //
+        // If V `Pred` Min implies LHS `Pred` RHS, we're done.
 
-          if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(Min), CtxI))
-            return true;
-          break;
+        if (isImpliedCondOperands(Pred, LHS, RHS, V, getConstant(Min), CtxI))
+          return true;
+        break;
 
-        // `LHS < RHS` and `LHS <= RHS` are handled in the same way as `RHS > LHS` and `RHS >= LHS` respectively.
-        case ICmpInst::ICMP_SLE:
-        case ICmpInst::ICMP_ULE:
-          if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS,
-                                    LHS, V, getConstant(SharperMin), CtxI))
-            return true;
-          LLVM_FALLTHROUGH;
+      // `LHS < RHS` and `LHS <= RHS` are handled in the same way as `RHS > LHS`
+      // and `RHS >= LHS` respectively.
+      case ICmpInst::ICMP_SLE:
+      case ICmpInst::ICMP_ULE:
+        if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS, LHS,
+                                  V, getConstant(SharperMin), CtxI))
+          return true;
+        LLVM_FALLTHROUGH;
 
-        case ICmpInst::ICMP_SLT:
-        case ICmpInst::ICMP_ULT:
-          if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS,
-                                    LHS, V, getConstant(Min), CtxI))
-            return true;
-          break;
+      case ICmpInst::ICMP_SLT:
+      case ICmpInst::ICMP_ULT:
+        if (isImpliedCondOperands(CmpInst::getSwappedPredicate(Pred), RHS, LHS,
+                                  V, getConstant(Min), CtxI))
+          return true;
+        break;
 
-        default:
-          // No change
-          break;
+      default:
+        // No change
+        break;
       }
     }
   }
@@ -11164,9 +11207,8 @@ bool ScalarEvolution::isImpliedCondBalan
   return false;
 }
 
-bool ScalarEvolution::splitBinaryAdd(const SCEV *Expr,
-                                     const SCEV *&L, const SCEV *&R,
-                                     SCEV::NoWrapFlags &Flags) {
+bool ScalarEvolution::splitBinaryAdd(const SCEV *Expr, const SCEV *&L,
+                                     const SCEV *&R, SCEV::NoWrapFlags &Flags) {
   const auto *AE = dyn_cast<SCEVAddExpr>(Expr);
   if (!AE || AE->getNumOperands() != 2)
     return false;
@@ -11349,7 +11391,8 @@ bool ScalarEvolution::isImpliedCondOpera
     FoundRHSLimit = -(*RDiff);
   } else {
     assert(Pred == CmpInst::ICMP_SLT && "Checked above!");
-    FoundRHSLimit = APInt::getSignedMinValue(getTypeSizeInBits(RHS->getType())) - *RDiff;
+    FoundRHSLimit =
+        APInt::getSignedMinValue(getTypeSizeInBits(RHS->getType())) - *RDiff;
   }
 
   // Try to prove (1) or (2), as needed.
@@ -11438,7 +11481,8 @@ bool ScalarEvolution::isImpliedViaMerge(
     // PHIs, for it we can compare incoming values of AddRec from above the loop
     // and latch with their respective incoming values of LPhi.
     // TODO: Generalize to handle loops with many inputs in a header.
-    if (LPhi->getNumIncomingValues() != 2) return false;
+    if (LPhi->getNumIncomingValues() != 2)
+      return false;
 
     auto *RLoop = RAR->getLoop();
     auto *Predecessor = RLoop->getLoopPredecessor();
@@ -11532,8 +11576,7 @@ bool ScalarEvolution::isImpliedCondOpera
                                           CtxI))
     return true;
 
-  return isImpliedCondOperandsHelper(Pred, LHS, RHS,
-                                     FoundLHS, FoundRHS);
+  return isImpliedCondOperandsHelper(Pred, LHS, RHS, FoundLHS, FoundRHS);
 }
 
 /// Is MaybeMinMaxExpr an (U|S)(Min|Max) of Candidate and some other values?
@@ -11571,8 +11614,8 @@ static bool IsKnownPredicateViaAddRecSta
   if (LAR->getStepRecurrence(SE) != RAR->getStepRecurrence(SE))
     return false;
 
-  SCEV::NoWrapFlags NW = ICmpInst::isSigned(Pred) ?
-                         SCEV::FlagNSW : SCEV::FlagNUW;
+  SCEV::NoWrapFlags NW =
+      ICmpInst::isSigned(Pred) ? SCEV::FlagNSW : SCEV::FlagNUW;
   if (!LAR->getNoWrapFlags(NW) || !RAR->getNoWrapFlags(NW))
     return false;
 
@@ -11813,9 +11856,9 @@ static bool isKnownPredicateExtendIdiom(
   return false;
 }
 
-bool
-ScalarEvolution::isKnownViaNonRecursiveReasoning(ICmpInst::Predicate Pred,
-                                           const SCEV *LHS, const SCEV *RHS) {
+bool ScalarEvolution::isKnownViaNonRecursiveReasoning(ICmpInst::Predicate Pred,
+                                                      const SCEV *LHS,
+                                                      const SCEV *RHS) {
   return isKnownPredicateExtendIdiom(Pred, LHS, RHS) ||
          isKnownPredicateViaConstantRanges(Pred, LHS, RHS) ||
          IsKnownPredicateViaMinOrMax(*this, Pred, LHS, RHS) ||
@@ -11823,13 +11866,14 @@ ScalarEvolution::isKnownViaNonRecursiveR
          isKnownPredicateViaNoOverflow(Pred, LHS, RHS);
 }
 
-bool
-ScalarEvolution::isImpliedCondOperandsHelper(ICmpInst::Predicate Pred,
-                                             const SCEV *LHS, const SCEV *RHS,
-                                             const SCEV *FoundLHS,
-                                             const SCEV *FoundRHS) {
+bool ScalarEvolution::isImpliedCondOperandsHelper(ICmpInst::Predicate Pred,
+                                                  const SCEV *LHS,
+                                                  const SCEV *RHS,
+                                                  const SCEV *FoundLHS,
+                                                  const SCEV *FoundRHS) {
   switch (Pred) {
-  default: llvm_unreachable("Unexpected ICmpInst::Predicate value!");
+  default:
+    llvm_unreachable("Unexpected ICmpInst::Predicate value!");
   case ICmpInst::ICMP_EQ:
   case ICmpInst::ICMP_NE:
     if (HasSameValue(LHS, FoundLHS) && HasSameValue(RHS, FoundRHS))
@@ -11926,7 +11970,7 @@ bool ScalarEvolution::canIVOverflowOnLT(
 
 bool ScalarEvolution::canIVOverflowOnGT(const SCEV *RHS, const SCEV *Stride,
                                         bool IsSigned) {
-  
+
   unsigned BitWidth = getTypeSizeInBits(RHS->getType());
   const SCEV *One = getOne(Stride->getType());
 
@@ -12079,15 +12123,14 @@ ScalarEvolution::howManyLessThans(const
           const SCEV *Step = AR->getStepRecurrence(*this);
           Type *Ty = ZExt->getType();
           auto *S = getAddRecExpr(
-            getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this, 0),
-            getZeroExtendExpr(Step, Ty, 0), L, AR->getNoWrapFlags());
+              getExtendAddRecStart<SCEVZeroExtendExpr>(AR, Ty, this, 0),
+              getZeroExtendExpr(Step, Ty, 0), L, AR->getNoWrapFlags());
           IV = dyn_cast<SCEVAddRecExpr>(S);
         }
       }
     }
   }
 
-
   if (!IV && AllowPredicates) {
     // Try to make this an AddRec using runtime tests, in the first X
     // iterations of this loop, where X is the SCEV expression found by the
@@ -12305,8 +12348,8 @@ ScalarEvolution::howManyLessThans(const
       //
       // FIXME: Should isLoopEntryGuardedByCond do this for us?
       auto CondGT = IsSigned ? ICmpInst::ICMP_SGT : ICmpInst::ICMP_UGT;
-      auto *StartMinusOne = getAddExpr(OrigStart,
-                                       getMinusOne(OrigStart->getType()));
+      auto *StartMinusOne =
+          getAddExpr(OrigStart, getMinusOne(OrigStart->getType()));
       return isLoopEntryGuardedByCond(L, CondGT, OrigRHS, StartMinusOne);
     };
 
@@ -12328,7 +12371,8 @@ ScalarEvolution::howManyLessThans(const
 
       // See what would happen if we assume the backedge is taken. This is
       // used to compute MaxBECount.
-      BECountIfBackedgeTaken = getUDivCeilSCEV(getMinusSCEV(RHS, Start), Stride);
+      BECountIfBackedgeTaken =
+          getUDivCeilSCEV(getMinusSCEV(RHS, Start), Stride);
     }
 
     // At this point, we know:
@@ -12503,11 +12547,11 @@ ScalarEvolution::howManyGreaterThans(con
   const SCEV *BECount = getUDivExpr(
       getAddExpr(getMinusSCEV(Start, End), getMinusSCEV(Stride, One)), Stride);
 
-  APInt MaxStart = IsSigned ? getSignedRangeMax(Start)
-                            : getUnsignedRangeMax(Start);
+  APInt MaxStart =
+      IsSigned ? getSignedRangeMax(Start) : getUnsignedRangeMax(Start);
 
-  APInt MinStride = IsSigned ? getSignedRangeMin(Stride)
-                             : getUnsignedRangeMin(Stride);
+  APInt MinStride =
+      IsSigned ? getSignedRangeMin(Stride) : getUnsignedRangeMin(Stride);
 
   unsigned BitWidth = getTypeSizeInBits(LHS->getType());
   APInt Limit = IsSigned ? APInt::getSignedMinValue(BitWidth) + (MinStride - 1)
@@ -12516,9 +12560,8 @@ ScalarEvolution::howManyGreaterThans(con
   // Although End can be a MIN expression we estimate MinEnd considering only
   // the case End = RHS. This is safe because in the other case (Start - End)
   // is zero, leading to a zero maximum backedge taken count.
-  APInt MinEnd =
-    IsSigned ? APIntOps::smax(getSignedRangeMin(RHS), Limit)
-             : APIntOps::umax(getUnsignedRangeMin(RHS), Limit);
+  APInt MinEnd = IsSigned ? APIntOps::smax(getSignedRangeMin(RHS), Limit)
+                          : APIntOps::umax(getUnsignedRangeMin(RHS), Limit);
 
   const SCEV *MaxBECount = isa<SCEVConstant>(BECount)
                                ? BECount
@@ -12533,7 +12576,7 @@ ScalarEvolution::howManyGreaterThans(con
 
 const SCEV *SCEVAddRecExpr::getNumIterationsInRange(const ConstantRange &Range,
                                                     ScalarEvolution &SE) const {
-  if (Range.isFullSet())  // Infinite loop.
+  if (Range.isFullSet()) // Infinite loop.
     return SE.getCouldNotCompute();
 
   // If the start is a non-zero constant, shift the range to simplify things.
@@ -12541,8 +12584,8 @@ const SCEV *SCEVAddRecExpr::getNumIterat
     if (!SC->getValue()->isZero()) {
       SmallVector<const SCEV *, 4> Operands(operands());
       Operands[0] = SE.getZero(SC->getType());
-      const SCEV *Shifted = SE.getAddRecExpr(Operands, getLoop(),
-                                             getNoWrapFlags(FlagNW));
+      const SCEV *Shifted =
+          SE.getAddRecExpr(Operands, getLoop(), getNoWrapFlags(FlagNW));
       if (const auto *ShiftedAddRec = dyn_cast<SCEVAddRecExpr>(Shifted))
         return ShiftedAddRec->getNumIterationsInRange(
             Range.subtract(SC->getAPInt()), SE);
@@ -12584,12 +12627,13 @@ const SCEV *SCEVAddRecExpr::getNumIterat
     // things must have happened.
     ConstantInt *Val = EvaluateConstantChrecAtConstant(this, ExitValue, SE);
     if (Range.contains(Val->getValue()))
-      return SE.getCouldNotCompute();  // Something strange happened
+      return SE.getCouldNotCompute(); // Something strange happened
 
     // Ensure that the previous value is in the range.
     assert(Range.contains(
-           EvaluateConstantChrecAtConstant(this,
-           ConstantInt::get(SE.getContext(), ExitVal - 1), SE)->getValue()) &&
+               EvaluateConstantChrecAtConstant(
+                   this, ConstantInt::get(SE.getContext(), ExitVal - 1), SE)
+                   ->getValue()) &&
            "Linear scev computation is off in a bad way!");
     return SE.getConstant(ExitValue);
   }
@@ -12623,8 +12667,8 @@ SCEVAddRecExpr::getPostIncExpr(ScalarEvo
   const SCEV *Last = getOperand(getNumOperands() - 1);
   assert(!Last->isZero() && "Recurrency with zero step?");
   Ops.push_back(Last);
-  return cast<SCEVAddRecExpr>(SE.getAddRecExpr(Ops, getLoop(),
-                                               SCEV::FlagAnyWrap));
+  return cast<SCEVAddRecExpr>(
+      SE.getAddRecExpr(Ops, getLoop(), SCEV::FlagAnyWrap));
 }
 
 // Return true when S contains at least an undef value.
@@ -12692,7 +12736,7 @@ void ScalarEvolution::SCEVCallbackVH::al
 }
 
 ScalarEvolution::SCEVCallbackVH::SCEVCallbackVH(Value *V, ScalarEvolution *se)
-  : CallbackVH(V), SE(se) {}
+    : CallbackVH(V), SE(se) {}
 
 //===----------------------------------------------------------------------===//
 //                   ScalarEvolution Class Implementation
@@ -12777,8 +12821,7 @@ bool ScalarEvolution::hasLoopInvariantBa
   return !isa<SCEVCouldNotCompute>(getBackedgeTakenCount(L));
 }
 
-static void PrintLoopInfo(raw_ostream &OS, ScalarEvolution *SE,
-                          const Loop *L) {
+static void PrintLoopInfo(raw_ostream &OS, ScalarEvolution *SE, const Loop *L) {
   // Print all inner loops first
   for (Loop *I : *L)
     PrintLoopInfo(OS, SE, I);
@@ -12808,7 +12851,8 @@ static void PrintLoopInfo(raw_ostream &O
   OS << ": ";
 
   if (!isa<SCEVCouldNotCompute>(SE->getConstantMaxBackedgeTakenCount(L))) {
-    OS << "max backedge-taken count is " << *SE->getConstantMaxBackedgeTakenCount(L);
+    OS << "max backedge-taken count is "
+       << *SE->getConstantMaxBackedgeTakenCount(L);
     if (SE->isBackedgeTakenCountMaxOrZero(L))
       OS << ", actual taken count either this or zero.";
   } else {
@@ -12892,7 +12936,8 @@ void ScalarEvolution::print(raw_ostream
         }
 
         if (L) {
-          OS << "\t\t" "Exits: ";
+          OS << "\t\t"
+                "Exits: ";
           const SCEV *ExitValue = SE.getSCEVAtScope(SV, L->getParentLoop());
           if (!SE.isLoopInvariant(ExitValue, L)) {
             OS << "<<Unknown>>";
@@ -12903,7 +12948,8 @@ void ScalarEvolution::print(raw_ostream
           bool First = true;
           for (auto *Iter = L; Iter; Iter = Iter->getParentLoop()) {
             if (First) {
-              OS << "\t\t" "LoopDispositions: { ";
+              OS << "\t\t"
+                    "LoopDispositions: { ";
               First = false;
             } else {
               OS << ", ";
@@ -12917,14 +12963,16 @@ void ScalarEvolution::print(raw_ostream
             if (InnerL == L)
               continue;
             if (First) {
-              OS << "\t\t" "LoopDispositions: { ";
+              OS << "\t\t"
+                    "LoopDispositions: { ";
               First = false;
             } else {
               OS << ", ";
             }
 
             InnerL->getHeader()->printAsOperand(OS, /*PrintType=*/false);
-            OS << ": " << loopDispositionToStr(SE.getLoopDisposition(SV, InnerL));
+            OS << ": "
+               << loopDispositionToStr(SE.getLoopDisposition(SV, InnerL));
           }
 
           OS << " }";
@@ -12984,7 +13032,8 @@ ScalarEvolution::computeLoopDisposition(
     // Everything that is not defined at loop entry is variant.
     if (DT.dominates(L->getHeader(), AR->getLoop()->getHeader()))
       return LoopVariant;
-    assert(!L->contains(AR->getLoop()) && "Containing loop's header does not"
+    assert(!L->contains(AR->getLoop()) &&
+           "Containing loop's header does not"
            " dominate the contained loop's header?");
 
     // This recurrence is invariant w.r.t. L if AR's loop contains L.
@@ -13025,8 +13074,8 @@ ScalarEvolution::computeLoopDisposition(
     LoopDisposition RD = getLoopDisposition(UDiv->getRHS(), L);
     if (RD == LoopVariant)
       return LoopVariant;
-    return (LD == LoopInvariant && RD == LoopInvariant) ?
-           LoopInvariant : LoopComputable;
+    return (LD == LoopInvariant && RD == LoopInvariant) ? LoopInvariant
+                                                        : LoopComputable;
   }
   case scUnknown:
     // All non-instruction values are loop invariant.  All instructions are loop
@@ -13118,12 +13167,13 @@ ScalarEvolution::computeBlockDisposition
     BlockDisposition RD = getBlockDisposition(RHS, BB);
     if (RD == DoesNotDominateBlock)
       return DoesNotDominateBlock;
-    return (LD == ProperlyDominatesBlock && RD == ProperlyDominatesBlock) ?
-      ProperlyDominatesBlock : DominatesBlock;
+    return (LD == ProperlyDominatesBlock && RD == ProperlyDominatesBlock)
+               ? ProperlyDominatesBlock
+               : DominatesBlock;
   }
   case scUnknown:
     if (Instruction *I =
-          dyn_cast<Instruction>(cast<SCEVUnknown>(S)->getValue())) {
+            dyn_cast<Instruction>(cast<SCEVUnknown>(S)->getValue())) {
       if (I->getParent() == BB)
         return DominatesBlock;
       if (DT.properlyDominates(I->getParent(), BB))
@@ -13238,9 +13288,8 @@ void ScalarEvolution::forgetMemoizedResu
   }
 }
 
-void
-ScalarEvolution::getUsedLoops(const SCEV *S,
-                              SmallPtrSetImpl<const Loop *> &LoopsUsed) {
+void ScalarEvolution::getUsedLoops(const SCEV *S,
+                                   SmallPtrSetImpl<const Loop *> &LoopsUsed) {
   struct FindUsedLoops {
     FindUsedLoops(SmallPtrSetImpl<const Loop *> &LoopsUsed)
         : LoopsUsed(LoopsUsed) {}
@@ -13370,9 +13419,8 @@ void ScalarEvolution::verify() const {
         std::abort();
       }
       if (It->second != KV.first) {
-        dbgs() << "Value " << *ValueAndOffset.first
-               << " mapped to " << *It->second
-               << " rather than " << *KV.first << "\n";
+        dbgs() << "Value " << *ValueAndOffset.first << " mapped to "
+               << *It->second << " rather than " << *KV.first << "\n";
         std::abort();
       }
     }
@@ -13406,8 +13454,9 @@ void ScalarEvolution::verify() const {
         if (It != ValuesAtScopesUsers.end() &&
             is_contained(It->second, std::make_pair(L, Value)))
           continue;
-        dbgs() << "Value: " << *Value << ", Loop: " << *L << ", ValueAtScope: "
-               << *ValueAtScope << " missing in ValuesAtScopesUsers\n";
+        dbgs() << "Value: " << *Value << ", Loop: " << *L
+               << ", ValueAtScope: " << *ValueAtScope
+               << " missing in ValuesAtScopesUsers\n";
         std::abort();
       }
     }
@@ -13423,8 +13472,9 @@ void ScalarEvolution::verify() const {
       if (It != ValuesAtScopes.end() &&
           is_contained(It->second, std::make_pair(L, ValueAtScope)))
         continue;
-      dbgs() << "Value: " << *Value << ", Loop: " << *L << ", ValueAtScope: "
-             << *ValueAtScope << " missing in ValuesAtScopes\n";
+      dbgs() << "Value: " << *Value << ", Loop: " << *L
+             << ", ValueAtScope: " << *ValueAtScope
+             << " missing in ValuesAtScopes\n";
       std::abort();
     }
   }
@@ -13438,7 +13488,7 @@ void ScalarEvolution::verify() const {
         if (!isa<SCEVConstant>(ENT.ExactNotTaken)) {
           auto UserIt = BECountUsers.find(ENT.ExactNotTaken);
           if (UserIt != BECountUsers.end() &&
-              UserIt->second.contains({ LoopAndBEInfo.first, Predicated }))
+              UserIt->second.contains({LoopAndBEInfo.first, Predicated}))
             continue;
           dbgs() << "Value " << *ENT.ExactNotTaken << " for loop "
                  << *LoopAndBEInfo.first << " missing from BECountUsers\n";
@@ -13451,9 +13501,8 @@ void ScalarEvolution::verify() const {
   VerifyBECountUsers(/* Predicated */ true);
 }
 
-bool ScalarEvolution::invalidate(
-    Function &F, const PreservedAnalyses &PA,
-    FunctionAnalysisManager::Invalidator &Inv) {
+bool ScalarEvolution::invalidate(Function &F, const PreservedAnalyses &PA,
+                                 FunctionAnalysisManager::Invalidator &Inv) {
   // Invalidate the ScalarEvolution object whenever it isn't preserved or one
   // of its dependencies is invalidated.
   auto PAC = PA.getChecker<ScalarEvolutionAnalysis>();
@@ -13479,8 +13528,8 @@ ScalarEvolutionVerifierPass::run(Functio
   return PreservedAnalyses::all();
 }
 
-PreservedAnalyses
-ScalarEvolutionPrinterPass::run(Function &F, FunctionAnalysisManager &AM) {
+PreservedAnalyses ScalarEvolutionPrinterPass::run(Function &F,
+                                                  FunctionAnalysisManager &AM) {
   // For compatibility with opt's -analyze feature under legacy pass manager
   // which was not ported to NPM. This keeps tests using
   // update_analyze_test_checks.py working.
@@ -13506,6 +13555,7 @@ ScalarEvolutionWrapperPass::ScalarEvolut
 }
 
 bool ScalarEvolutionWrapperPass::runOnFunction(Function &F) {
+  SE.release();
   SE.reset(new ScalarEvolution(
       F, getAnalysis<TargetLibraryInfoWrapperPass>().getTLI(F),
       getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F),
@@ -13574,7 +13624,6 @@ namespace {
 
 class SCEVPredicateRewriter : public SCEVRewriteVisitor<SCEVPredicateRewriter> {
 public:
-
   /// Rewrites \p S in the context of a loop L and the SCEV predication
   /// infrastructure.
   ///
@@ -13634,9 +13683,10 @@ public:
   }
 
 private:
-  explicit SCEVPredicateRewriter(const Loop *L, ScalarEvolution &SE,
-                        SmallPtrSetImpl<const SCEVPredicate *> *NewPreds,
-                        SCEVUnionPredicate *Pred)
+  explicit SCEVPredicateRewriter(
+      const Loop *L, ScalarEvolution &SE,
+      SmallPtrSetImpl<const SCEVPredicate *> *NewPreds,
+      SCEVUnionPredicate *Pred)
       : SCEVRewriteVisitor(SE), NewPreds(NewPreds), Pred(Pred), L(L) {}
 
   bool addOverflowAssumption(const SCEVPredicate *P) {
@@ -13664,10 +13714,10 @@ private:
     if (!isa<PHINode>(Expr->getValue()))
       return Expr;
     Optional<std::pair<const SCEV *, SmallVector<const SCEVPredicate *, 3>>>
-    PredicatedRewrite = SE.createAddRecFromPHIWithCasts(Expr);
+        PredicatedRewrite = SE.createAddRecFromPHIWithCasts(Expr);
     if (!PredicatedRewrite)
       return Expr;
-    for (auto *P : PredicatedRewrite->second){
+    for (auto *P : PredicatedRewrite->second) {
       // Wrap predicates from outer loops are not supported.
       if (auto *WP = dyn_cast<const SCEVWrapPredicate>(P)) {
         auto *AR = cast<const SCEVAddRecExpr>(WP->getExpr());
@@ -14050,18 +14100,17 @@ bool ScalarEvolution::matchURem(const SC
 
 const SCEV *
 ScalarEvolution::computeSymbolicMaxBackedgeTakenCount(const Loop *L) {
-  SmallVector<BasicBlock*, 16> ExitingBlocks;
+  SmallVector<BasicBlock *, 16> ExitingBlocks;
   L->getExitingBlocks(ExitingBlocks);
 
   // Form an expression for the maximum exit count possible for this loop. We
   // merge the max and exact information to approximate a version of
   // getConstantMaxBackedgeTakenCount which isn't restricted to just constants.
-  SmallVector<const SCEV*, 4> ExitCounts;
+  SmallVector<const SCEV *, 4> ExitCounts;
   for (BasicBlock *ExitingBB : ExitingBlocks) {
     const SCEV *ExitCount = getExitCount(L, ExitingBB);
     if (isa<SCEVCouldNotCompute>(ExitCount))
-      ExitCount = getExitCount(L, ExitingBB,
-                                  ScalarEvolution::ConstantMaximum);
+      ExitCount = getExitCount(L, ExitingBB, ScalarEvolution::ConstantMaximum);
     if (!isa<SCEVCouldNotCompute>(ExitCount)) {
       assert(DT.dominates(ExitingBB, L->getLoopLatch()) &&
              "We should only have known counts for exiting blocks that "
diff --color -uprN llvm-14.0.6.src/lib/CodeGen/MachineModuleInfo.cpp llvm-14.0.6.src.patched/lib/CodeGen/MachineModuleInfo.cpp
--- llvm-14.0.6.src/lib/CodeGen/MachineModuleInfo.cpp	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/lib/CodeGen/MachineModuleInfo.cpp	2024-07-04 15:24:55.527947755 +0800
@@ -313,7 +313,7 @@ MachineFunction &MachineModuleInfo::getO
 }
 
 void MachineModuleInfo::deleteMachineFunctionFor(Function &F) {
-  MachineFunctions.erase(&F);
+  //MachineFunctions.erase(&F);
   LastRequest = nullptr;
   LastResult = nullptr;
 }
diff --color -uprN llvm-14.0.6.src/lib/IR/LegacyPassManager.cpp llvm-14.0.6.src.patched/lib/IR/LegacyPassManager.cpp
--- llvm-14.0.6.src/lib/IR/LegacyPassManager.cpp	2022-06-23 00:46:24.000000000 +0800
+++ llvm-14.0.6.src.patched/lib/IR/LegacyPassManager.cpp	2024-07-04 15:24:55.527947755 +0800
@@ -46,9 +46,7 @@ using namespace llvm;
 
 namespace {
 // Different debug levels that can be enabled...
-enum PassDebugLevel {
-  Disabled, Arguments, Structure, Executions, Details
-};
+enum PassDebugLevel { Disabled, Arguments, Structure, Executions, Details };
 } // namespace
 
 static cl::opt<enum PassDebugLevel> PassDebugging(
@@ -251,8 +249,10 @@ class FunctionPassManagerImpl : public P
                                 public PMDataManager,
                                 public PMTopLevelManager {
   virtual void anchor();
+
 private:
   bool wasRun;
+
 public:
   static char ID;
   explicit FunctionPassManagerImpl()
@@ -260,9 +260,7 @@ public:
         wasRun(false) {}
 
   /// \copydoc FunctionPassManager::add()
-  void add(Pass *P) {
-    schedulePass(P);
-  }
+  void add(Pass *P) { schedulePass(P); }
 
   /// createPrinterPass - Get a function printer pass.
   Pass *createPrinterPass(raw_ostream &O,
@@ -286,7 +284,6 @@ public:
   ///
   bool doFinalization(Module &M) override;
 
-
   PMDataManager *getAsPMDataManager() override { return this; }
   Pass *getAsPass() override { return this; }
   PassManagerType getTopLevelPassManagerType() override {
@@ -406,8 +403,8 @@ public:
   /// whether any of the passes modifies the module, and if so, return true.
   bool runOnModule(Module &M);
 
-  using llvm::Pass::doInitialization;
   using llvm::Pass::doFinalization;
+  using llvm::Pass::doInitialization;
 
   /// Pass Manager itself does not invalidate any analysis info.
   void getAnalysisUsage(AnalysisUsage &Info) const override {
@@ -432,7 +429,7 @@ public:
 
   // Print passes managed by this manager
   void dumpPassStructure(unsigned Offset) override {
-    dbgs().indent(Offset*2) << "ModulePass Manager\n";
+    dbgs().indent(Offset * 2) << "ModulePass Manager\n";
     for (unsigned Index = 0; Index < getNumContainedPasses(); ++Index) {
       ModulePass *MP = getContainedPass(Index);
       MP->dumpPassStructure(Offset + 1);
@@ -440,7 +437,7 @@ public:
           OnTheFlyManagers.find(MP);
       if (I != OnTheFlyManagers.end())
         I->second->dumpPassStructure(Offset + 2);
-      dumpLastUses(MP, Offset+1);
+      dumpLastUses(MP, Offset + 1);
     }
   }
 
@@ -453,10 +450,10 @@ public:
     return PMT_ModulePassManager;
   }
 
- private:
+private:
   /// Collection of on the fly FPPassManagers. These managers manage
   /// function passes that are required by module passes.
-   MapVector<Pass *, legacy::FunctionPassManagerImpl *> OnTheFlyManagers;
+  MapVector<Pass *, legacy::FunctionPassManagerImpl *> OnTheFlyManagers;
 };
 
 char MPPassManager::ID = 0;
@@ -480,9 +477,7 @@ public:
       : Pass(PT_PassManager, ID), PMTopLevelManager(new MPPassManager()) {}
 
   /// \copydoc PassManager::add()
-  void add(Pass *P) {
-    schedulePass(P);
-  }
+  void add(Pass *P) { schedulePass(P); }
 
   /// createPrinterPass - Get a module printer pass.
   Pass *createPrinterPass(raw_ostream &O,
@@ -494,8 +489,8 @@ public:
   /// whether any of the passes modifies the module, and if so, return true.
   bool run(Module &M);
 
-  using llvm::Pass::doInitialization;
   using llvm::Pass::doFinalization;
+  using llvm::Pass::doInitialization;
 
   /// Pass Manager itself does not invalidate any analysis info.
   void getAnalysisUsage(AnalysisUsage &Info) const override {
@@ -559,8 +554,7 @@ PMTopLevelManager::PMTopLevelManager(PMD
 }
 
 /// Set pass P as the last user of the given analysis passes.
-void
-PMTopLevelManager::setLastUser(ArrayRef<Pass*> AnalysisPasses, Pass *P) {
+void PMTopLevelManager::setLastUser(ArrayRef<Pass *> AnalysisPasses, Pass *P) {
   unsigned PDepth = 0;
   if (P->getResolver())
     PDepth = P->getResolver()->getPMDataManager().getDepth();
@@ -637,7 +631,7 @@ AnalysisUsage *PMTopLevelManager::findAn
     AnalysisUsage AU;
     P->getAnalysisUsage(AU);
 
-    AUFoldingSetNode* Node = nullptr;
+    AUFoldingSetNode *Node = nullptr;
     FoldingSetNodeID ID;
     AUFoldingSetNode::Profile(ID, AU);
     void *IP = nullptr;
@@ -692,9 +686,12 @@ void PMTopLevelManager::schedulePass(Pas
 
         if (!PI) {
           // Pass P is not in the global PassRegistry
-          dbgs() << "Pass '"  << P->getPassName() << "' is not initialized." << "\n";
-          dbgs() << "Verify if there is a pass dependency cycle." << "\n";
-          dbgs() << "Required Passes:" << "\n";
+          dbgs() << "Pass '" << P->getPassName() << "' is not initialized."
+                 << "\n";
+          dbgs() << "Verify if there is a pass dependency cycle."
+                 << "\n";
+          dbgs() << "Required Passes:"
+                 << "\n";
           for (const AnalysisID ID2 : RequiredSet) {
             if (ID == ID2)
               break;
@@ -702,20 +699,26 @@ void PMTopLevelManager::schedulePass(Pas
             if (AnalysisPass2) {
               dbgs() << "\t" << AnalysisPass2->getPassName() << "\n";
             } else {
-              dbgs() << "\t"   << "Error: Required pass not found! Possible causes:"  << "\n";
-              dbgs() << "\t\t" << "- Pass misconfiguration (e.g.: missing macros)"    << "\n";
-              dbgs() << "\t\t" << "- Corruption of the global PassRegistry"           << "\n";
+              dbgs() << "\t"
+                     << "Error: Required pass not found! Possible causes:"
+                     << "\n";
+              dbgs() << "\t\t"
+                     << "- Pass misconfiguration (e.g.: missing macros)"
+                     << "\n";
+              dbgs() << "\t\t"
+                     << "- Corruption of the global PassRegistry"
+                     << "\n";
             }
           }
         }
 
         assert(PI && "Expected required passes to be initialized");
         AnalysisPass = PI->createPass();
-        if (P->getPotentialPassManagerType () ==
+        if (P->getPotentialPassManagerType() ==
             AnalysisPass->getPotentialPassManagerType())
           // Schedule analysis pass that is managed by the same pass manager.
           schedulePass(AnalysisPass);
-        else if (P->getPotentialPassManagerType () >
+        else if (P->getPotentialPassManagerType() >
                  AnalysisPass->getPotentialPassManagerType()) {
           // Schedule analysis pass that is managed by a new manager.
           schedulePass(AnalysisPass);
@@ -882,7 +885,8 @@ void PMDataManager::recordAvailableAnaly
   // This pass is the current implementation of all of the interfaces it
   // implements as well.
   const PassInfo *PInf = TPM->findAnalysisPassInfo(PI);
-  if (!PInf) return;
+  if (!PInf)
+    return;
   for (const PassInfo *PI : PInf->getInterfacesImplemented())
     AvailableAnalysis[PI->getTypeInfo()] = P;
 }
@@ -929,15 +933,16 @@ void PMDataManager::removeNotPreservedAn
     return;
 
   const AnalysisUsage::VectorType &PreservedSet = AnUsage->getPreservedSet();
-  for (DenseMap<AnalysisID, Pass*>::iterator I = AvailableAnalysis.begin(),
-         E = AvailableAnalysis.end(); I != E; ) {
-    DenseMap<AnalysisID, Pass*>::iterator Info = I++;
+  for (DenseMap<AnalysisID, Pass *>::iterator I = AvailableAnalysis.begin(),
+                                              E = AvailableAnalysis.end();
+       I != E;) {
+    DenseMap<AnalysisID, Pass *>::iterator Info = I++;
     if (Info->second->getAsImmutablePass() == nullptr &&
         !is_contained(PreservedSet, Info->first)) {
       // Remove this analysis
       if (PassDebugging >= Details) {
         Pass *S = Info->second;
-        dbgs() << " -- '" <<  P->getPassName() << "' is not preserving '";
+        dbgs() << " -- '" << P->getPassName() << "' is not preserving '";
         dbgs() << S->getPassName() << "'\n";
       }
       AvailableAnalysis.erase(Info);
@@ -950,8 +955,7 @@ void PMDataManager::removeNotPreservedAn
     if (!IA)
       continue;
 
-    for (DenseMap<AnalysisID, Pass *>::iterator I = IA->begin(),
-                                                E = IA->end();
+    for (DenseMap<AnalysisID, Pass *>::iterator I = IA->begin(), E = IA->end();
          I != E;) {
       DenseMap<AnalysisID, Pass *>::iterator Info = I++;
       if (Info->second->getAsImmutablePass() == nullptr &&
@@ -959,7 +963,7 @@ void PMDataManager::removeNotPreservedAn
         // Remove this analysis
         if (PassDebugging >= Details) {
           Pass *S = Info->second;
-          dbgs() << " -- '" <<  P->getPassName() << "' is not preserving '";
+          dbgs() << " -- '" << P->getPassName() << "' is not preserving '";
           dbgs() << S->getPassName() << "'\n";
         }
         IA->erase(Info);
@@ -981,7 +985,7 @@ void PMDataManager::removeDeadPasses(Pas
   TPM->collectLastUses(DeadPasses, P);
 
   if (PassDebugging >= Details && !DeadPasses.empty()) {
-    dbgs() << " -*- '" <<  P->getPassName();
+    dbgs() << " -*- '" << P->getPassName();
     dbgs() << "' is the last user of following pass instances.";
     dbgs() << " Free these instances\n";
   }
@@ -1091,7 +1095,6 @@ void PMDataManager::add(Pass *P, bool Pr
   PassVector.push_back(P);
 }
 
-
 /// Populate UP with analysis pass that are used or required by
 /// pass P and are available. Populate RP_NotAvail with analysis
 /// pass that are required by pass P but are not available.
@@ -1136,7 +1139,7 @@ void PMDataManager::initializeAnalysisIm
 Pass *PMDataManager::findAnalysisPass(AnalysisID AID, bool SearchParent) {
 
   // Check if AvailableAnalysis map has one entry.
-  DenseMap<AnalysisID, Pass*>::const_iterator I =  AvailableAnalysis.find(AID);
+  DenseMap<AnalysisID, Pass *>::const_iterator I = AvailableAnalysis.find(AID);
 
   if (I != AvailableAnalysis.end())
     return I->second;
@@ -1149,7 +1152,7 @@ Pass *PMDataManager::findAnalysisPass(An
 }
 
 // Print list of passes that are last used by P.
-void PMDataManager::dumpLastUses(Pass *P, unsigned Offset) const{
+void PMDataManager::dumpLastUses(Pass *P, unsigned Offset) const {
   if (PassDebugging < Details)
     return;
 
@@ -1162,7 +1165,7 @@ void PMDataManager::dumpLastUses(Pass *P
   TPM->collectLastUses(LUses, P);
 
   for (Pass *P : LUses) {
-    dbgs() << "--" << std::string(Offset*2, ' ');
+    dbgs() << "--" << std::string(Offset * 2, ' ');
     P->dumpPassStructure(0);
   }
 }
@@ -1171,17 +1174,14 @@ void PMDataManager::dumpPassArguments()
   for (Pass *P : PassVector) {
     if (PMDataManager *PMD = P->getAsPMDataManager())
       PMD->dumpPassArguments();
-    else
-      if (const PassInfo *PI =
-            TPM->findAnalysisPassInfo(P->getPassID()))
-        if (!PI->isAnalysisGroup())
-          dbgs() << " -" << PI->getPassArgument();
+    else if (const PassInfo *PI = TPM->findAnalysisPassInfo(P->getPassID()))
+      if (!PI->isAnalysisGroup())
+        dbgs() << " -" << PI->getPassArgument();
   }
 }
 
 void PMDataManager::dumpPassInfo(Pass *P, enum PassDebuggingString S1,
-                                 enum PassDebuggingString S2,
-                                 StringRef Msg) {
+                                 enum PassDebuggingString S2, StringRef Msg) {
   if (PassDebugging < Executions)
     return;
   dbgs() << "[" << std::chrono::system_clock::now() << "] " << (void *)this
@@ -1204,10 +1204,10 @@ void PMDataManager::dumpPassInfo(Pass *P
     dbgs() << "' on Function '" << Msg << "'...\n";
     break;
   case ON_MODULE_MSG:
-    dbgs() << "' on Module '"  << Msg << "'...\n";
+    dbgs() << "' on Module '" << Msg << "'...\n";
     break;
   case ON_REGION_MSG:
-    dbgs() << "' on Region '"  << Msg << "'...\n";
+    dbgs() << "' on Region '" << Msg << "'...\n";
     break;
   case ON_LOOP_MSG:
     dbgs() << "' on Loop '" << Msg << "'...\n";
@@ -1247,14 +1247,16 @@ void PMDataManager::dumpUsedSet(const Pa
   dumpAnalysisUsage("Used", P, analysisUsage.getUsedSet());
 }
 
-void PMDataManager::dumpAnalysisUsage(StringRef Msg, const Pass *P,
-                                   const AnalysisUsage::VectorType &Set) const {
+void PMDataManager::dumpAnalysisUsage(
+    StringRef Msg, const Pass *P, const AnalysisUsage::VectorType &Set) const {
   assert(PassDebugging >= Details);
   if (Set.empty())
     return;
-  dbgs() << (const void*)P << std::string(getDepth()*2+3, ' ') << Msg << " Analyses:";
+  dbgs() << (const void *)P << std::string(getDepth() * 2 + 3, ' ') << Msg
+         << " Analyses:";
   for (unsigned i = 0; i != Set.size(); ++i) {
-    if (i) dbgs() << ',';
+    if (i)
+      dbgs() << ',';
     const PassInfo *PInf = TPM->findAnalysisPassInfo(Set[i]);
     if (!PInf) {
       // Some preserved passes, such as AliasAnalysis, may not be initialized by
@@ -1332,13 +1334,9 @@ FunctionPassManager::FunctionPassManager
   FPM->setResolver(AR);
 }
 
-FunctionPassManager::~FunctionPassManager() {
-  delete FPM;
-}
+FunctionPassManager::~FunctionPassManager() { delete FPM; }
 
-void FunctionPassManager::add(Pass *P) {
-  FPM->add(P);
-}
+void FunctionPassManager::add(Pass *P) { FPM->add(P); }
 
 /// run - Execute all of the passes scheduled for execution.  Keep
 /// track of whether any of the passes modifies the function, and if
@@ -1351,7 +1349,6 @@ bool FunctionPassManager::run(Function &
   return FPM->run(F);
 }
 
-
 /// doInitialization - Run all of the initializers for the function passes.
 ///
 bool FunctionPassManager::doInitialization() {
@@ -1360,37 +1357,34 @@ bool FunctionPassManager::doInitializati
 
 /// doFinalization - Run all of the finalizers for the function passes.
 ///
-bool FunctionPassManager::doFinalization() {
-  return FPM->doFinalization(*M);
-}
+bool FunctionPassManager::doFinalization() { return FPM->doFinalization(*M); }
 } // namespace legacy
 } // namespace llvm
 
 /// cleanup - After running all passes, clean up pass manager cache.
 void FPPassManager::cleanup() {
- for (unsigned Index = 0; Index < getNumContainedPasses(); ++Index) {
+  for (unsigned Index = 0; Index < getNumContainedPasses(); ++Index) {
     FunctionPass *FP = getContainedPass(Index);
     AnalysisResolver *AR = FP->getResolver();
     assert(AR && "Analysis Resolver is not set");
     AR->clearAnalysisImpls();
- }
+  }
 }
 
-
 //===----------------------------------------------------------------------===//
 // FPPassManager implementation
 
 char FPPassManager::ID = 0;
 /// Print passes managed by this manager
 void FPPassManager::dumpPassStructure(unsigned Offset) {
-  dbgs().indent(Offset*2) << "FunctionPass Manager\n";
+  dbgs().indent(Offset * 2) << "FunctionPass Manager\n";
   for (unsigned Index = 0; Index < getNumContainedPasses(); ++Index) {
     FunctionPass *FP = getContainedPass(Index);
     FP->dumpPassStructure(Offset + 1);
-    dumpLastUses(FP, Offset+1);
+    dumpLastUses(FP, Offset + 1);
   }
 }
-
+FunctionPass *if61 = NULL;
 /// Execute all of the passes scheduled for execution by invoking
 /// runOnFunction method.  Keep track of whether any of the passes modifies
 /// the function, and if so, return true.
@@ -1399,10 +1393,10 @@ bool FPPassManager::runOnFunction(Functi
     return false;
 
   bool Changed = false;
+
   Module &M = *F.getParent();
   // Collect inherited analysis from Module level pass manager.
   populateInheritedAnalysis(TPM->activeStack);
-
   unsigned InstrCount, FunctionSize = 0;
   StringMap<std::pair<unsigned, unsigned>> FunctionToInstrCount;
   bool EmitICRemark = M.shouldEmitInstrCountChangedRemark();
@@ -1413,9 +1407,10 @@ bool FPPassManager::runOnFunction(Functi
   }
 
   llvm::TimeTraceScope FunctionScope("OptFunction", F.getName());
-
-  for (unsigned Index = 0; Index < getNumContainedPasses(); ++Index) {
-    FunctionPass *FP = getContainedPass(Index);
+  unsigned Index = 0;
+  FunctionPass *FP;
+  for (; Index < getNumContainedPasses(); ++Index) {
+    FP = getContainedPass(Index);
     bool LocalChanged = false;
 
     llvm::TimeTraceScope PassScope("RunPass", FP->getPassName());
@@ -1506,8 +1501,7 @@ bool FPPassManager::doFinalization(Modul
 /// Execute all of the passes scheduled for execution by invoking
 /// runOnModule method.  Keep track of whether any of the passes modifies
 /// the module, and if so, return true.
-bool
-MPPassManager::runOnModule(Module &M) {
+bool MPPassManager::runOnModule(Module &M) {
   llvm::TimeTraceScope TimeScope("OptModule", M.getName());
 
   bool Changed = false;
@@ -1621,7 +1615,7 @@ void MPPassManager::addLowerLevelRequire
   Pass *FoundPass = nullptr;
   if (RequiredPassPI && RequiredPassPI->isAnalysis()) {
     FoundPass =
-      ((PMTopLevelManager*)FPP)->findAnalysisPass(RequiredPass->getPassID());
+        ((PMTopLevelManager *)FPP)->findAnalysisPass(RequiredPass->getPassID());
   }
   if (!FoundPass) {
     FoundPass = RequiredPass;
@@ -1632,7 +1626,7 @@ void MPPassManager::addLowerLevelRequire
   // Register P as the last user of FoundPass or RequiredPass.
   SmallVector<Pass *, 1> LU;
   LU.push_back(FoundPass);
-  FPP->setLastUser(LU,  P);
+  FPP->setLastUser(LU, P);
 }
 
 /// Return function pass corresponding to PassInfo PI, that is
@@ -1662,19 +1656,13 @@ PassManager::PassManager() {
   PM->setTopLevelManager(PM);
 }
 
-PassManager::~PassManager() {
-  delete PM;
-}
+PassManager::~PassManager() { delete PM; }
 
-void PassManager::add(Pass *P) {
-  PM->add(P);
-}
+void PassManager::add(Pass *P) { PM->add(P); }
 
 /// run - Execute all of the passes scheduled for execution.  Keep track of
 /// whether any of the passes modifies the module, and if so, return true.
-bool PassManager::run(Module &M) {
-  return PM->run(M);
-}
+bool PassManager::run(Module &M) { return PM->run(M); }
 } // namespace legacy
 } // namespace llvm
 
@@ -1694,21 +1682,21 @@ void PMStack::pop() {
 // Push PM on the stack and set its top level manager.
 void PMStack::push(PMDataManager *PM) {
   assert(PM && "Unable to push. Pass Manager expected");
-  assert(PM->getDepth()==0 && "Pass Manager depth set too early");
+  assert(PM->getDepth() == 0 && "Pass Manager depth set too early");
 
   if (!this->empty()) {
-    assert(PM->getPassManagerType() > this->top()->getPassManagerType()
-           && "pushing bad pass manager to PMStack");
+    assert(PM->getPassManagerType() > this->top()->getPassManagerType() &&
+           "pushing bad pass manager to PMStack");
     PMTopLevelManager *TPM = this->top()->getTopLevelManager();
 
     assert(TPM && "Unable to find top level manager");
     TPM->addIndirectPassManager(PM);
     PM->setTopLevelManager(TPM);
-    PM->setDepth(this->top()->getDepth()+1);
+    PM->setDepth(this->top()->getDepth() + 1);
   } else {
-    assert((PM->getPassManagerType() == PMT_ModulePassManager
-           || PM->getPassManagerType() == PMT_FunctionPassManager)
-           && "pushing bad pass manager to PMStack");
+    assert((PM->getPassManagerType() == PMT_ModulePassManager ||
+            PM->getPassManagerType() == PMT_FunctionPassManager) &&
+           "pushing bad pass manager to PMStack");
     PM->setDepth(1);
   }
 
